% STEP 1: Choose oneside or twoside. Use the 'draft' option a lot when writing.
\documentclass[english, oneside]{HYgradu}

\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp}
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
%\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage[sort,colon]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions

\renewcommand{\topfraction}{.75}
\renewcommand{\floatpagefraction}{.75}

\fussy % Probably not needed but you never know...

% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Simulating the dynamics of supermassive black holes}
\author{Vili Oskari Oja}
\date{\today}
\level{Master's thesis}
\faculty{Faculty of Science}
\department{Department of Physics}
\address{PL 64 (Gustaf Hällströmin katu 2)\\00014 Helsingin yliopisto\\Finland}
\subject{Astronomy}
\prof{Professor Peter Johansson}{Dr. Pauli Pihajoki}
\censors{Professor Peter Johansson}{Dr. Pauli Pihajoki}{}
\depositeplace{}
\additionalinformation{}
\numberofpagesinformation{\numberofpages \ pages}
\classification{}
\keywords{Your keywords here}
\quoting{}

\begin{document}
\setlength{\parindent}{1cm}
\setlength{\parskip}{0cm}
% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.
\begin{abstract}
Abstract goes here.
\end{abstract}

% Place ToC
\mytableofcontents


% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\setlength{\parindent}{.75cm}
\setlength{\parskip}{.6cm}
\chapter{Introduction}


\section{Discovery of black holes}

\section{History of black hole observations}
%miten eri aukkoja havaitaan, binääreissä pieniä aukkoja, keskustoissa suuria ja voidaan nähdä niiden vaikutus ympäröiviin tähtiin

\section{Aim of this thesis}
%Selitystä et miks halutaan tutkia mustia aukkoja, kertoo paljon galasien evoluutiosta ja siten maailmankaikkeuden historiasta. Ketjupaperin sivulta 2 selitystä miten tree koodit toimii vs n-body koodi

\chapter{Dynamical modelling of black holes}

\section{Black hole properties}

A black hole (BH) is defined as a region of spacetime in which the gravitational field is so strong that no objects or signals that carry information can escape from it. Black holes have the interesting property that they are completely determined by the their mass M, angular momentum J, and electric charge Q. This is known as the black hole uniqueness theorem or no-hair theorem, which states that all physical black hole solutions are completely characterized by the above-mentioned three parameters, and must satisfy the condition 
\begin{equation} \label{equ:bhunique}
M^2 - \left( \frac{J}{M} \right)^2 - Q^2 \geq 0 \ ,
\end{equation}
where we set $G = c = 1$ \citep{mazur:2001}.
Thus only by changing these variables can the properties of the black hole change. The physical reasoning for this uniqueness theorem is that the matter beyond the event horizon of a black hole cannot directly affect anything outside of it. Thus only the globally conserved characteristics, such as mass and angular momentum, survive and can be measured from the outside. The electric charge of black holes that appear in nature is most likely very near zero, since having a non-zero charge would require a very large imbalance in the amounts of protons and electrons that enter into the black hole, which is not a realistic scenario with normal matter.

Black holes can be divided into four distinct types based on these properties. Every black hole has mass, but the angular momentum and electric charge are not necessary.
\begin{table}[htb]
\centering
\caption{Different types of black holes}
\begin{tabular}{|c|c|c|}
\hline
 & Non-rotating ($J = 0$) & Rotating ($J \neq 0$) \\ \hline
Uncharged ($Q = 0$) & Schwarzschild & Kerr \\ \hline
Charged ($Q \neq 0$) & Reissner–Nordström & Kerr–Newman \\ \hline
\end{tabular}
\end{table}

The simplest type of black hole is created when an object of mass M becomes smaller than the radius
\begin{equation}
r_S = \frac{2GM}{c^2} \ .
\end{equation}
A black hole can be born in nature when massive a star dies and explodes in a supernova. As will be discussed in more detail in section \ref{sect:stellarholes}, massive stars can fuse matter up to iron, and an iron core is formed in the center of the star near the end of its lifetime. Once this core is massive enough that the degeneracy pressure of electrons can no longer support its gravity, it will collapse into a neutron star. If the neutron star grows until the degeneracy pressure of neutrons cannot support it, it will collapse until it reaches the aforementioned radius, where it will form a black hole. The radius is called the Schwarzschild radius, so named after the German astronomer Karl Schwarzschild, who found an exact solution for the Einstein field equations \citep{schwarzschild:1916}. Einstein field equations are a set of 10 equations in Albert Einstein's general theory of relativity that describe the fundamental interaction between spacetime curvature and mass and energy \citep{einstein:1915}. The surface at this radius is called the event horizon. Event horizons are mathematical surfaces, and they do not form any sort of physical barrier. An observer can fall inside the event horizon without any problem. The event horizon simply marks the limit at which not even light can escape the gravitational pull of the black hole.

Solutions that do not satisfy the condition given by equation \ref{equ:bhunique} do exist, but these solutions are not stationary, i.e.\ they evolve with time to a Kerr solution. In a stationary field the geometry does not change over time, but it can rotate, e.g. the Kerr solution. As will be discussed soon, these non-stationary solutions are not really physical. This gives an upper limit to the angular momentum that a physical uncharged black hole can have. This limit can be expressed as the specific angular momentum 
\begin{equation} \label{equ:angularmomentum}
a = \frac{J}{Mc}
\end{equation}
or as the dimensionless spin parameter
\begin{equation}
a_* = \frac{Jc}{GM^2} \ .
\end{equation}
The parameter can range from 0, meaning that the hole does not spin, to almost 1, meaning that it spins as fast as possible for a given mass \citep{middleton:2016}.

If a black hole would have a larger angular momentum than allowed by these constraints, i.e. $a > 1$, it would mean that it actually would not be a black hole at all, since the event horizon would disappear due to the extreme rotation. 
%This is made clear by looking at equation \ref{equ:evenhorizons}, which describes the surfaces that event horizons occur at. If $\mu^2 < a^2$, the solutions for this equation are complex, which is said to mean that the actual event horizons disappear.
This would cause what is known as a naked singularity, meaning that a singularity that is normally contained within a black hole would be visible to an outside observer. The possible existence of such singularities in nature is uncertain but extremely unlikely. If they were to exist, they might cause fundamental problems for physics as we know it. We would be able to see matter condensed to infinite density, and we have no theories that can predict how spacetime works near such abnormalities. Normally this is not a problem, since they cannot be observed inside event horizons. However the cosmic censorship hypothesis suggests that naked singularities cannot be formed in nature from realistic initial conditions, which would avoid the problem altogether \citep{wald:1997}. An extremely rapid spin could prevent the collapse into a black hole thanks to the centrifugal forces. The object would have to lose enough of its angular momentum before it could collapse.

Astrophysical black holes that appear in nature are assumed to be most likely Kerr black holes. The normal matter that creates and feeds black holes contains roughly equal amounts of protons and electrons, so the overall electric charge is mostly neutral. But stars do spin around themselves, thus possessing angular momentum. Because of the conservation of angular momentum, the stellar remnant will spin around itself as well. Black holes can also gain angular momentum through accreting matter falling into them. Gas coming close to a black hole will form an accretion disk around it. For the matter to be able to reach the black hole and fall into it, it must somehow lose some of its angular momentum. The total angular momentum of the disk must be conserved however, so the momentum has to move outwards in the disk.
The main source for this is believed to be magneto-rotational instability. The matter in an accretion disk can be considered a magnetized, rotating, and perfectly conducting fluid. In this kind of environment magnetic forces can cause torques on the matter, decreasing the angular momentum of matter closer to the black hole and increasing it in the matter farther away. These instabilities will effectively drive angular momentum outwards in the disk.
Eventually the matter will fall within the event horizon, conferring its mass and remaining angular momentum to the black hole, speeding up the rotation of the black hole \citep{bhphysics}. Black holes are also capable of losing their angular momentum. One way for this to happen is through the emission of jets. Jets are beams of ionized matter that are emitted from near black holes and they can be accelerated close to the speed of light. One suggested method for the formation of jets is through magnetic fields in the accretion disk being dragged and twisted by the spinning black hole.

\subsection{The structure of a rotating black hole}

While Schwarzschild black holes have only the one event horizon and a point-like singularity in the middle, spinning black holes are far more complicated. In general relativity gravity is a consequence of the curvature of spacetime, and this curvature causes an effect that is known as de Sitter precession or the geodetic effect. The curvature of spacetime causes the orbits of objects around a mass to precess slightly. This is true for all masses. In addition to this, spinning masses cause an additional effect called Lense-Thirring precession or frame-dragging. The massive object drags the surrounding spacetime with it as it spins, causing nearby particles and photons to rotate around the massive object as well, even if they did not have any angular momentum of their own to begin with. Both of these effects have been experimentally confirmed, for example by the Gravity Probe B mission. It measured the slight changes in the spin of gyroscopes aboard a satellite orbiting Earth, and confirmed that precession does indeed occur from both sources. The geodetic effect near Earth is around 170 times larger, so it can be measured more accurately \citep{everitt:2009}.

Thus the frame-dragging effect is significant only around very massive objects, like black holes that are rotating rapidly. At certain point this effect becomes so strong that any object, including light, \textit{must} spin along the rotation of the black hole. This limit is known as the stationary limit surface, and it is described in equation \ref{equ:statlimsurf}. The area inside this limit is called the ergosphere, so named after the Greek word for work. The ergosphere differs from the insides of an event horizon because particles that fall into it can still escape from this region. If a particle escapes from the ergosphere, it converts some of the black hole's rotational energy into its own momentum \citep{grintro}. This lowers the angular momentum of the black hole, and the process could theoretically over time cause a rotating black hole to spin arbitrarily slow.

%This is called the Penrose process, and it is a possible explanation for some highly energetic astrophysical phenomena, such as gamma ray bursts. 
%TODO tarkenna tätä kappaletta

The Kerr metric is often presented in Boyer-Lindquist coordinates $(t, r, \theta, \phi)$ which are similar to spherical polar coordinates and are related to the Cartesian x,y,z-coordinates via the following transformation:
\begin{align*}
x &= \sqrt{r^2 + a^2} \mathrm{sin}\theta \mathrm{cos}\phi \\
y &= \sqrt{r^2 + a^2} \mathrm{sin}\theta \mathrm{sin}\phi \\
z &= r \mathrm{cos}\theta
\end{align*}
The variable $a$ is the same specific angular momentum as in the equation \ref{equ:angularmomentum}. The only true singularity (i.e. a singularity that exists no matter what coordinate system we choose) in the Kerr metric occurs when $r=0$ and $\theta = \pi/2$ \citep{grintro}. It is easy to see that in Cartesian coordinates this corresponds to a flat ring in the equatorial plane with the radius of $a$. So rotating black holes do not have a point singularity, but a ring singularity instead.

In addition there exists coordinate singularities (i.e. singularities we can get rid of if we choose our coordinates differently) as well. These singularities occur on the surfaces
\begin{equation} \label{equ:evenhorizons}
r_\pm = \mu \pm \sqrt{\mu^2 - a^2}
\end{equation}
which describe the event horizons of the black hole, and on the surfaces
\begin{equation} \label{equ:statlimsurf}
r_{S \pm} = \mu \pm \sqrt{\mu^2 - a^2 \mathrm{cos}^2 \theta}
\end{equation}
which describe the stationary limit surfaces of the black hole. Here $\mu = \frac{GM}{c^2}$. 

In figure \ref{fig:KerrHole} one can see the different surfaces of a spinning black hole and their approximate shapes as seen in the Boyer-Lindquist coordinates. The surfaces mostly resemble axisymmetric ellipsoids, flattened along the rotation axis. Although the outer limit of the ergosphere changes its shape as the spinning gets faster, resembling a pumpkin-shape at nearly maximal spins. It however always touches the outer event horizon at the poles. The inner horizon and stationary limit surface also touch at the poles. On the equatorial plane the inner stationary limit surface also coincides with the ring singularity.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/kerrhole.pdf}
\caption{Diagram showing the structure of a Kerr black hole. It shows the inner and outer ergospheres and event horizons, and the ring singularity in the middle. This kind of black hole has non-zero spin. If the spin was 0 we would have a Schwarzschild black hole instead.
(Figure adapted from \citealt{grintro})}
\label{fig:KerrHole}
\end{figure}

%TODO

%Kahdenlaista frame draggingia Lense-Thirring efekti pyörivillä

%mainitse Eddingtonin luminositeetti yms jutut? Muuttaa valtavasti massaa energiaksi.

%mainitse miten voi menettää massaa?
\newpage
\section{Different types of black holes}

Mass is the only quantity that all black holes have for certain, so it makes sense to classify them by their mass. The black holes are usually put into three different categories. The reason for this kind of distinction is that the different mass black holes are found in very different kinds of environments and have different formation mechanisms. The black holes are also detected through the different kinds of phenomena that they cause.
The mass limits for the categories are not exact, but typically stellar mass black holes (SBH) are below $10^2 \ M_\odot$, intermediate mass black holes (IMBH) are between $10^2$ and $10^5 \ M_\odot$, and supermassive black holes (SMBH) are above $10^5 \ M_\odot$ \citep{bhphysics}.

%kerro eri kohissa et miten voi havaita, mitä eri ilmiöitä nää ehkä synnyttää
%kerro miten syntyvät, massarajoja, tähden kokonaismassa on eri kuin ytimen massa, joka kertoo voiko syntyä musta aukko

\subsection{Stellar mass black holes} \label{sect:stellarholes}

Stellar mass black holes are born from collapsing massive stars. During their active life, stars are in hydrostatic equilibrium due to the gravitational pull of the matter making up the star, and the outward thermal pressure caused by the nuclear reactions in the star's core. At the end of a massive star's lifespan, the hydrogen in the core will have fused into helium. This leads to hydrogen forming a shell around the helium core, and hydrogen fusion will continue there, which is known as shell burning. The helium formed in the shell will accrete into the core, causing the core to grow hotter and denser. Once the core is has grown enough, the helium will start to fuse as well. This same process will happen for heavier elements as well, until the star will have several shells of different elements, as illustrated in figure \ref{fig:FusionShells}.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/FusionShells.pdf}
\caption{A simplified cross-section of a massive, evolved star. As the star starts fusing heavier and heavier elements, new shells will form around the previous ones, until the core becomes iron, which cannot go through fusion without losing energy.
(\copyright \ User:Rursus / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:FusionShells}
\end{figure}

Iron and nickel have the highest binding energies of all the elements, meaning that lighter elements release energy through fusion, but reactions producing heavier elements require additional energy. After forming an iron core, the star cannot fuse any more matter in its core, and thus the supporting radiation pressure drops, causing the star to implode under its own gravity. Iron being the final product in stars causes what is known as the iron peak in the abundance of chemical elements. Iron is one of the most common metals in the universe, as seen in figure \ref{fig:IronPeak}. One can also see that certain other elements such as carbon, oxygen, neon, etc. are more abundant than other elements near their mass. This too is the result of stellar nucleosynthesis, where these elements are created through fusion with helium nuclei, which are known as alpha particles. Due to this, all of these so called alpha elements have a mass number that is a multiple of four.

What happens next depends on the mass of the star. For the star to be able to even reach the iron core stage, it needs to be initially over around 8 solar masses. Otherwise it does not have enough mass to fuse the heavier elements, and it will gradually expand its atmosphere into a planetary nebula, leaving behind a white dwarf, composed of electron-degenerate matter. In more massive stars, the iron core will exceed the Chandrasekhar limit of around 1.4 solar masses, which describes the upper limit for the mass of a stable white dwarf. Stars above this $\sim$8 solar mass threshold but below $\sim$20 solar masses explode in supernovae and leave behind neutron stars. If the initial mass of the star is higher than $\sim$20 solar masses, the stellar core is too massive to not collapse into a black hole \citep{woosley:2002}. The actual mass limit is the Tolman–Oppenheimer–Volkoff limit, which gives the upper bound for a cold, non-rotating neutron star, and is about 2.2 solar masses \citep{margalit:2017}. These limits have to do with the properties of the degenerate matter that dense stellar remnants are made of. While main sequence stars stay in hydrostatic equilibrium thanks to thermal pressure, stellar remnants resist collapse through degeneracy pressure. White dwarfs and neutrons stars can be modelled as a Fermi gas, which is the quantum mechanical equivalent of an ideal gas. The Pauli exclusion principle states that identical fermions cannot occupy the same quantum state within a quantum system simultaneously. Thus there exists a sort of interaction or pressure between the particles in a Fermi gas that keeps them separate, even at absolute zero temperature. This manifests as a degeneracy pressure that resists further gravitational collapse. In white dwarfs the electron degeneracy pressure is what keeps the object in an equilibrium. Past the Chandrasekhar limit the electrons will combine with protons to form neutrons, forming a remnant made of degenerate neutron matter. Neutron stars are then supported by the degenerate neutron pressure. For stellar remnants more massive than the Tolman–Oppenheimer–Volkoff limit, the gravitational pull will exceed even the degenerate pressure, causing the remnant to collapse into a black hole. This also gives a lower limit for the mass of new black holes that form through stellar collapse.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/SolarSystemAbundances.pdf}
\caption{Diagram illustrating the abundancies of different elements in our solar system. Iron is the final product of fusion in stars, causing it to be very abundant in the universe thanks to its production in stars before they explode. The abundant alpha elements are also clearly noticeable.
(\copyright \ User:MHz\textasciigrave as / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:IronPeak}
\end{figure}

One way stellar mass black holes have been observed is in X-ray binaries. These kinds of binaries were discovered through the detection of highly energetic X-ray sources, having luminosities with the same magnitude as the theoretical maximum luminosity from matter accreting onto a black hole, assuming simple spherical accretion. This is called the Eddington limit, and it will be discussed more later on. This combined with their low timescale variability (down to milliseconds) lent support for a model where the X-rays are produced by matter from a star accreting onto a compact object. A way that black holes can be observed without accretion is through the dynamics of nearby stars. If we can detect a star on a short orbit around some invisible point in space, there must be some compact mass that the star is orbiting around.

The mass of a compact object $M_x$ can be estimated by measuring the orbital period $P$, radial velocity amplitude $K$, and the mass of a companion star $M_c$, and the inclination angle $i$ of the star's orbit. Inclination is important because of the mass-inclination degeneracy. Measuring the true orbital velocity of a star depends on its inclination, and thus measuring the mass of the star also depends on it. The inclination can be constrained for example by tracking and modelling how the brightness of the star varies along the orbit \citep{kerkwijk:2011}. These are used in the mass function equation 
\begin{equation}
f(M_x) = \frac{K^3 P}{2 \pi G} = \frac{M_x^3 \mathrm{sin}^3 i}{(M_x + M_c)^2}
\end{equation}
which tells the lower limit for $M_x$ \citep{casares:2007}. The key factor in the equation is $M_c$ which can have a lot of uncertainty. The reason that the mass of the central object is interesting is that both neutron stars and black holes are very compact massive objects and they both can produce X-ray radiation. Matter accreting onto the objects heats up in the accretion disc due to the particles colliding with each other. The matter can reach temperatures of millions of degrees and radiates away its potential energy as X-rays. With neutron stars, infalling matter can also actually hit the surface of the object, heating up upon impact to emit X-rays. If the mass can be measured to be well above the aforementioned Tolman–Oppenheimer–Volkoff limit, other objects than black holes can be quite safely ruled out.

%puhu muista tavoista havaita, värihommaa?
%neutronitähdissä voi olla flareja

%fuusiokartta, raudasta ei pääse eteenpäin, iron peak

%observable in x-ray compact binary systems

\subsection{Intermediate mass black holes}

Intermediate mass black holes are much more elusive compared to both their more massive and lighter companions. We have only very few good candidates and their formation is not fully understood. Learning more about both the birth and evolution of intermediate mass black holes is an important topic, as it allows us to learn more about how lighter black holes gain mass and form supermassive black holes, which in turn is a key element in understanding the formation and evolution of galaxies.

One way for black holes to grow in mass is through the accretion of gas. Matter that accretes onto a black hole forms a disk and will eventually fall into the black hole. The matter particles can collide with each other, creating heat. The heat is radiated away, dominantly in the range of X-rays. This process creates radiation pressure that resists more matter falling into the black hole. The radiation increases as more matter accretes onto the hole, and the limit where the outward radiation force and the inward gravitational force are balanced is called the Eddington luminosity or Eddington limit. This limit, assuming spherical accretion, is given by
\begin{equation}
L_{Edd} = \frac{4 \pi c G M m_p}{\sigma_T} \simeq 3.2 \times 10^4 \left( \frac{M}{M_\odot} \right) L_\odot \ ,
\end{equation}
where $c$ is the speed of light, $G$ is the gravitational constant, $M$ is the mass of the black hole, $L_\odot$ is the luminosity of the Sun, $m_p$ is the proton mass, and $\sigma_T$ is the Thomson scattering cross-section for an electron. The Thomson cross-section describes the effective area for photon interactions for a free charged particle. Assuming there are no other effects which affect the accretion rate, this limit gives the maximum rate at which black holes can grow solely through spherical accretion. 

A so called ``seed'' black hole with a mass around $10^6 M_\odot$ would require over 0.5 Gyr to grow into a $10^9 M_\odot$ supermassive black hole, even if it constantly grows at Eddington rate. Due to this fact, the observed existence of supermassive black holes of $10^9 M_\odot$ when the universe was around 1 Gyr old implies that these seed black holes must have formed at redshifts larger than 10. In such a young universe there are several ways that intermediate mass black holes could form \citep{mezcua:2017}. These methods are illustrated in figure \ref{fig:imbhs}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/imbhs.pdf}
\caption{Different methods of birth for seed intermediate mass black holes. These include collapse of Pop III stars, mergers in dense stellar clusters, and direct collapse of gas in protogalaxies. Some black holes will grow to be supermassive, but some will stay in their intermediate state to this day.
(Figure adapted from \citealt{mezcua:2017})}
\label{fig:imbhs}
\end{figure}

One way for these kinds of black holes to form could be through massive Population III stars. These stars formed early in the universe and thus they had very low metallicities since heavy elements had not yet formed. Low metallicity allowed these stars to grow very large, which can be explained with how metallicity affects the cooling of gas clouds. 
The most relevant cooling processes in this case are two-body radiative cooling processes. At temperatures of over $10^6$ Kelvin where the matter is almost completely ionized, the most significant process is bremsstrahlung or breaking radiation, where emissions happen due to acceleration of electrons when they encounter atomic nuclei. At lower temperatures other processes come into play as well. In recombination an electron recombines with an ion to emit a photon. In collisional ionization an electron collides with an atom or ion and ionizes it, removing from the gas an amount of kinetic energy equal to the ionization threshold. And finally in collisional excitation atoms are excited via collisions, and they emit photons as they decay back to the ground state. Metals have more possible energy states, so more metals means that these processes can happen more often, thus allowing for more efficient cooling. Conversely if there are little to no metals, these processes happen more slowly.

As stars form from collapsing gas clouds, the mass of the cloud also determines the maximum mass of the star. Jeans mass gives the limit for the size of a symmetrical, non-rotating gas cloud before it will collapse under its own gravity. The limit comes from when the cloud's thermal energy and gravitational potential are in equilibrium. The mass can be calculated by
\begin{equation}
M_J = \left( \frac{5 k T}{G \mu m} \right)^{3/2} \left( \frac{3}{4 \pi \rho} \right)^{1/2} \ ,
\end{equation}
where $k$ is the Boltzmann's constant, $G$ the gravitational constant, $T$ the temperature of the gas, $\rho$ the density of the gas, $\mu$ the mean molecular weight, and $m$ the mass of a particle comprising the gas. So the hotter and thinner the gas cloud is, the more massive it has to be for it to collapse. 

At the start of the contraction, the cloud undergoes isothermal collapse. Meaning that the density increases but the temperature can stay approximately constant thanks to cooling. This can allow the cloud to fragment, as isothermal collapse causes the Jeans mass to decrease, allowing the cloud to start collapsing around small internal density inhomogeneities. As the density increases, the cloud becomes more optically thick and the temperature will start to rise as well. This adiabatic collapse will cause the cloud to start heating up into a protostar. If cooling is inefficient to begin with, like in metal-poor clouds made of molecular hydrogen, the isothermal phase can be quite short, decreasing fragmentation, allowing the stars to become very big, and Pop III stars can exceed 200 solar masses.

These stars can also retain their mass as they exhibit weaker stellar winds compared to more metal-rich stars. This is because radiation pressure on many spectral lines is the main mechanism behind winds in massive stars. Hydrogen and helium have only few relevant lines, so metal lines are the major reason for strong stellar winds \citep{vink:2001}. When these stars collapse in a similar manner to lower mass stars, they could then form black holes with masses of several hundred $M_\odot$. These kinds of black holes would still be relatively light, requiring super-Eddington accretion rates to reach the observed masses at around redshift 6.

To circumvent this problem, the seed black holes would need to be more massive from the start. Black holes could also form inside the first protogalaxies by direct collapse of gas. The formation of these black holes would require a rapid inflow of dense gas. The gas must have minimal angular momentum so that it does not form a disk structure to support itself. In addition, to prevent fragmentation, atomic hydrogen, not molecular hydrogen, must dominate the cooling process, making it extremely inefficient. In this kind of environment the gas could form supermassive stars with masses of $10^5 M_\odot$, which could collapse into black holes of $10^4 - 10^5 M_\odot$. Another prediction for the formation of intermediate mass black holes is through multiple mergers in dense stellar clusters. In a dense region of stars, multiple stellar mergers can cause the formation of supermassive stars that can collapse into black holes of $10^2 - 10^4 M_\odot$ \citep{mezcua:2017}.

To figure out whether intermediate mass black holes exist or not, we need to measure the mass of candidate black holes. The most accurate way to probe the mass is through stellar or gas dynamics, i.e. observing the movements of objects around the black hole, and measuring the mass of the central object based on their orbits. Although with our current instruments this method only works for relatively nearby objects. A black hole of $10^5 M_\odot$ has a sphere of influence of about 0.5 pc, and cannot be resolved beyond a megaparsec. Sphere of influence describes the area around a black hole where its gravitational potential dominates the potential of the surrounding galaxy. It is defined with the equation 
\begin{equation}
r_h = \frac{G M_{BH}}{\sigma^2} \ ,
\end{equation}
where $G$ is the gravitational constant, $M_{BH}$ the mass of the black hole, and $\sigma$ the velocity dispersion of the stars in the surrounding bulge.
In addition to kinematics, radiation signatures such as X-ray and radio emissions can be used to ascertain whether an object is a black hole and what its mass is, since the mass and type of the compact object determines how matter accretes onto it, which in turn affects what kind of radiation the accreting matter emits. One of the best current candidates for an intermediate mass black hole is the central object in the globular cluster G1. Both X-ray and radio emissions have been detected there, and photometric and kinematic observations have given an estimate for a black hole with a mass of around $1.8 \times 10^4 M_\odot$ \citep{gebhardt:2005}. One can obtain the mass through these measurements by calculating the full line-of-sight velocity distribution (LOSVD), and using this velocity to estimate the central mass.

\subsection{Supermassive black holes}

Supermassive black holes are the sources of some of the most energetic phenomena in the known universe, thus we have been able to detect their presence despite their relative scarcity and distance. They are found in the centers of galaxies, and it is nowadays generally accepted that supermassive black holes are present in practically every massive galaxy with a bulge component \citep{kormendy:2013}. What would eventually become the first candidates for supermassive black holes were discovered when very luminous radio sources were observed in the 1960s \citep{greenstein:1964}. First thought to be stars, their luminosities, redshifts, and short timescale variabilities made it quite clear that they were something else, hence they were dubbed quasi-stellar objects, or quasars. It was later proposed that matter falling onto a massive compact object could explain the phenomenon.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/MilkyWaySMBH.pdf}
\caption{Illustration of the well-determined orbits of 6 stars around Sagittarius A* in the galactic centre of our Milky Way. The star S2 has an orbital period of about 16 years, and has given us the most accurate estimate for the black hole mass to date. The data is based on \cite{eisenhauer:2005}.
(\copyright \ User:Cmglee / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:MilkyWayBH}
\end{figure}

On top of spectroscopic measurements, the high resolution of the Hubble Space Telescope (HST) has enabled us to do dynamical measurements of galaxies and their massive black holes, meaning that we can track the movements of stars in these distant galaxies. In addition to HST, thanks to advances in adaptive optics, ground based observations have given us the best proof for a supermassive black hole at the center of our own galaxy. The galactic center is only around 8.1 kpc away \citep{eisenhauer:2018}, so individual stars can be resolved and their orbits determined. In figure \ref{fig:MilkyWayBH} orbits of 6 stars orbiting around the galactic center are visualized. Based on the orbit of the star S2, there must be a mass of around $4 \times 10^6 M_\odot$ at the center of the galaxy, and it must be located inside the radius of around 120 AU, otherwise the star would collide with the central mass \citep{kormendy:2013}. This rules out the possibility of the central mass being a large group of smaller masses, and makes a singular, massive central object the most likely scenario.

Supermassive black holes are believed to play an important role in the formation and evolution of galaxies. The mass of the central black hole correlates with the properties of the host galaxy and central bulge that contains it. The first of these correlations to be observed was the $M_{BH} - L_{bulge}$ correlation. The correlation between the black hole mass and the luminosity of the bulge has been tested on many galaxies, and it seems to be very consistent. The luminosity of the bulge is dependent on the mass of the bulge, but the correlation between the explicitly measured mass of the bulge and the black hole mass has also been tested and observed. This $M_{BH} - M_{bulge}$ relation is expressed by
\begin{equation}
\mathrm{log} (M_{BH}/M_{\odot}) = 1.12 \  \mathrm{log} (M_{bulge}/ 10^{11} M_{\odot}) + 8.2 \ ,
\end{equation}
and it has been found to be accurate for many galaxies \citep{haring:2004}.

One way to measure the mass of the bulge is by calculating its virial mass. The virial theorem relates the kinetic energy of a stable to system with its potential energy. In astrophysics the common use of this relation is to relate the gravitational potential energy of a system with its kinetic energy. The virial mass of a bulge can be calculated with the equation
\begin{equation}
M_{bulge} = \frac{k r_e \sigma^2_e}{G} \ ,
\end{equation}
where $k$ is a scaling factor, $r_e$ is the effective radius, $\sigma_e$ is the velocity dispersion inside the effective radius, and $G$ is the gravitational constant \citep{marconi:2003}. The velocity dispersion of the galaxy has been used in these previous correlations, but there seems to be a correlation between the black hole mass and the velocity dispersion itself. This $M_{BH} - \sigma$ relation is given by
\begin{equation}
\mathrm{log} (M_{BH}/M_{\odot}) = 4.24 \ \mathrm{log} ( \sigma / 200 \ \mathrm{km \, s^{-1}} ) + 8.12 \ , 
\end{equation}
and it has been found to be highly accurate for many galaxies \citep{gultekin:2009}.
A tight $M_{BH} - \sigma$ correlation is important on a practical level since it allows for good estimates of the black hole mass from relatively easy measurements, and it also implies a link between the growth of the black hole and formation of the bulge \citep{kormendy:2013}.

\section{Newtonian dynamics}

\subsection{Two-body problem}

Analytically solving the Newtonian two-body problem, i.e.\ determining the motions of two particles interacting only with each other through gravity, is extremely useful. It allows solving the behaviour of two essentially isolated objects, such as a binary star system. It for example also enables us to calculate the masses of two objects that are observed to be interacting with each other if their orbits are known.

Let us assume we have a mass $m_1$ situated at position vector $\mathbf{r}_1$, and a mass $m_2$ at position vector $\mathbf{r}_2$. If the first mass exerts a force $\mathbf{f}_{21}$ on the second mass, by Newton's third law the second object exerts an equal and opposite force, $\mathbf{f}_{12} = -\mathbf{f}_{21}$, on the first object. The masses and their positions are illustrated in figure \ref{fig:2BodyProb}. If these two objects are the only objects in the system, their equations of motion are
\begin{align}
m_1 \mathbf{\ddot{r}}_1 &= -\mathbf{f} \label{equ:eom1} \\
m_2 \mathbf{\ddot{r}}_2 &= \mathbf{f} \label{equ:eom2} \ ,
\end{align}
where $\mathbf{f} = \mathbf{f}_{21}$.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.5]{../images/2bp.pdf}
\caption{The position vectors $\mathbf{r}_1$ and $\mathbf{r}_2$ point to the masses $m_1$ and $m_2$ respectively. $\mbox{$\mathbf{r} = \mathbf{r_2} - \mathbf{r_1}$}$ is their relative displacement vector, and $\mathbf{R}$ is the position vector of the center of mass of the system.}
\label{fig:2BodyProb}
\end{figure}

We can first get an equation describing the motion of the center of mass of the system by adding the equations \ref{equ:eom1} and \ref{equ:eom2} together. The position vector of the center of mass is
\begin{equation}
\mathbf{R} = \frac{m_1 \mathbf{r}_1 + m_2 \mathbf{r}_2}{m_1 + m_2}
\end{equation}
and by performing the addition we get
\begin{equation}
m_1 \mathbf{\ddot{r}}_1 + m_2 \mathbf{\ddot{r}}_2 = (m_1 + m_2) \mathbf{\ddot{R}} = \mathbf{f} - \mathbf{f} = 0 \ .
\end{equation}
This gives us that
\begin{equation}
\mathbf{\ddot{R}} = 0 \ ,
\end{equation}
which shows that the velocity of the center of mass is constant. So the position of the center of mass can be determined at all times from the initial positions and velocities. We can denote the relative displacement vector of the two masses with $\mbox{$\mathbf{r} = \mathbf{r_2} - \mathbf{r_1}$}$. Together with this and the center of mass vector, we can write
\begin{align}
\mathbf{r_1} &= \mathbf{R} - \frac{m_2}{m_1 + m_2}\mathbf{r} \\
\mathbf{r_2} &= \mathbf{R} + \frac{m_1}{m_1 + m_2}\mathbf{r}
\end{align}
Substituting these equations into \ref{equ:eom1} and \ref{equ:eom2}, we can see that both yield 
\begin{equation}
\mu \mathbf{\ddot{r}} = \mathbf{f} \ , \label{equ:eom3}
\end{equation}
where
\begin{equation}
\mu = \frac{m_1 m_2}{m_1 + m_2}
\end{equation}
is called the reduced mass. Thus we have effectively converted the two-body problem of masses $m_1$ and $m_2$ into a one-body problem of the reduced mass $\mu$. The situation is now simpler since we do not have to inspect the movement of two separate particles, but only the movement of one particle with respect to the other particle.

In the case of celestial objects, the force affecting the masses is gravity. The Newtonian gravitational force that the first object exerts on the second object is given by
\begin{equation}
\mathbf{f} = -\frac{G m_1 m_2}{r^3} \mathbf{r}
\end{equation}
where $\mathbf{r}$ is the aforementioned displacement vector and $G$ is the gravitational constant. Substituting this into equation \ref{equ:eom3} gives us
\begin{equation}
\frac{m_1 m_2}{m_1 + m_2} \mathbf{\ddot{r}} = -\frac{G m_1 m_2}{r^3} \mathbf{r}
\end{equation}
which gives us the equation of motion
\begin{equation}
\mathbf{\ddot{r}} = -\frac{G M}{r^3} \mathbf{r} \label{equ:eomgravity}
\end{equation}
where $M = m_1 + m_2$ is the total mass of the system.

\subsection{Equation of the orbit}

The derivation for the Keplerian equation of orbit is done following \cite{bt-galdyn}. We set up spherical coordinates $(r, \theta, \phi)$ as position of $m_1$. The position vector can then be expressed as
\begin{equation}
\mathbf{r} = r \hat{\mathbf{e}}_r \ ,
\end{equation}
where $\hat{\mathbf{e}}_r$ is the radial direction vector. The gravitational field in the equation of motion can be expressed as
\begin{equation}
g(r) = -\frac{G M}{r^2} \ ,
\end{equation}
so the equation of motion becomes
\begin{equation}
\frac{\mathrm{d}^2 \mathbf{r}}{\mathrm{d} t^2} = g(r) \hat{\mathbf{e}}_r \ .
\end{equation}
Since the cross product of a vector with itself is zero, we can see that
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} \left( \mathbf{r} \times \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} \right) = \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} \times \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} + \mathbf{r} \times \frac{\mathrm{d}^2 \mathbf{r}}{\mathrm{d} t^2} = 0 + r \hat{\mathbf{e}}_r \times g(r) \hat{\mathbf{e}}_r = 0 \ ,
\end{equation}
thus $\mathbf{r} \times \mathbf{\dot{r}}$ is some constant vector and always orthogonal to both $\mathbf{r}$ and $\dot{\mathbf{r}}$, which we can denote with for example $\mathbf{L}$. This is simply the angular momentum per unit mass. It is a constant vector, and therefore the movement of the orbiting objects must take place in a plane. Limiting the movement to two dimensions simplifies the situation, and we can use plane polar coordinates $(r, \theta)$ where the center of attraction is at $r = 0$ and $\theta$ is the azimuthal angle in the plane. We can now find the equations of motions for the system in these coordinates by forming the Lagrangian per unit mass, which is
\begin{equation}
\mathcal{L} = K - V = \frac{1}{2} \mathbf{v}^2 - \Phi (r) = \frac{1}{2} \left[ \dot{r}^2 + (r \dot{\theta})^2 \right] - \Phi (r) \ ,
\end{equation}
where $\Phi$ is the gravitational potential and $g(r) = -\mathrm{d} \Phi / \mathrm{d} r$. Writing out the Lagrange equations gives us the equations of motion, which are
\begin{align}
\frac{\mathrm{d} }{\mathrm{d} t} \left( \frac{\partial \mathcal{L}}{\partial \dot{r}} \right) - \frac{\partial \mathcal{L}}{\partial r} &= \ddot{r} - r \dot{\theta}^2 + \frac{\mathrm{d} \Phi}{\mathrm{d} r} = 0 \label{equ:leom1} \\
\frac{\mathrm{d} }{\mathrm{d} t} \left( \frac{\partial \mathcal{L}}{\partial \dot{\theta}} \right) - \frac{\partial \mathcal{L}}{\partial \theta} &= \frac{\mathrm{d}}{\mathrm{d} t} (r^2 \dot{\theta}) = 0 \label{equ:leom2} \ .
\end{align}
Equation \ref{equ:leom2} implies that $r^2 \dot{\theta}$ is a constant, which we can denote with $L$. And this is actually equal to the norm of the $\mathbf{L}$ vector. So this equation is a restatement of the conservation of angular momentum. We can use it to replace time with angle as the independent variable in equation \ref{equ:leom1}. Equation \ref{equ:leom2} implies that
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} = \frac{L}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \ ,
\end{equation}
so equation \ref{equ:leom1} becomes
\begin{equation}
\frac{L^2}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \left( \frac{1}{r^2} \frac{\mathrm{d} r}{\mathrm{d} \theta} \right) - \frac{L^2}{r^3} = -\frac{\mathrm{d} \Phi}{\mathrm{d} r} \label{equ:leom1edit} \ .
\end{equation}
Expanding the first term gives us 
\begin{equation}
\frac{L^2}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \left( \frac{1}{r^2} \frac{\mathrm{d} r}{\mathrm{d} \theta} \right) = -\frac{2 L^2}{r^5} \left( \frac{\mathrm{d} r}{\mathrm{d} \theta} \right)^2 + \frac{L^2}{r^4} \frac{\mathrm{d}^2 r}{\mathrm{d} \theta^2} \ .
\end{equation}
Making the substitution $u \equiv \frac{1}{r}$ allows us to simplify the equation. Since
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} = \frac{2}{r^2} \left( \frac{\mathrm{d} r}{\mathrm{d} \theta} \right)^2 - \frac{1}{r^2} \frac{\mathrm{d}^2 r}{\mathrm{d} \theta^2} \ ,
\end{equation}
we can rearrange the terms so that equation \ref{equ:leom1edit} becomes
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = \frac{1}{L^2 u^2} \frac{\mathrm{d}}{\mathrm{d} r} \Phi (r) \ .
\end{equation}
Further using the relation
\begin{equation}
\frac{\mathrm{d} \Phi}{\mathrm{d} r} = -u^2 \frac{\mathrm{d} \Phi}{\mathrm{d} u}
\end{equation}
this simplifies into
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = -\frac{1}{L^2} \frac{\mathrm{d}}{\mathrm{d} r} \Phi (1/u) \label{equ:leom3} \ .
\end{equation}
Because our gravitational potential is of the form $\Phi = -GMu$, equation \ref{equ:leom3} simply becomes
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = \frac{GM}{L^2} \ .
\end{equation}
We can immediately see that this is the equation for a forced harmonic oscillator. So we know that its general solution is
\begin{equation}
u(\theta) = C \mathrm{cos}(\theta - \theta_0) + \frac{GM}{L^2} \label{equ:eomsolution} \ ,
\end{equation}
where $C > 0$ and $\theta_0$ are arbitrary constants. We set $\theta_0 = 0$. By defining the orbit's eccentricity, i.e. how much the shape of the orbit differs from a perfect circle, by 
\begin{equation}
e \equiv \frac{CL^2}{GM}
\end{equation}
and its semimajor axis, i.e. half of the longest axis of an ellipse, by
\begin{equation}
a \equiv \frac{L^2}{GM(1-e^2)} \ ,
\end{equation}
equation \ref{equ:eomsolution} can be rewritten as 
\begin{equation}
r(\theta) = \frac{a (1-e^2)}{1 + e \ \mathrm{cos} \, \theta} \ .
\end{equation}
This is the equation of the orbit for Keplerian orbits, and it is also the equation for a conic section, since all Keplerian orbits are conic sections. The parameter $\theta$ is called the true anomaly, and it is defined as the angle from the periapsis, as seen from the main focus of the ellipse. Figure \ref{fig:ellipse} illustrates these different parameters.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/ellipse.pdf}
\caption{The different parameters of an elliptic orbit. $C$ is the center of the ellipse, $F_1$ and $F_2$ are the foci of the ellipse, $a$ is the semimajor axis, $b$ is the semiminor axis, $f$ is the distance between the center and a focus of the ellipse, $r$ points to the current position of the orbiting body, and $\theta$ is the true anomaly of the object. Eccentricity $e$ can be expressed as the ratio between $f$ and $a$.}
\label{fig:ellipse}
\end{figure}

The analytic solution of the two-body problem works quite accurately for example in our solar system, even though there are more than just two objects. This is because the mass of the Sun dominates all of the interactions and the planets are far away from each other, so each planet can to the first order be examined on its own. However in more extreme environments, the approximation faces some limitations. The singularity in the equation of motion as $r$ approaches 0 is troublesome when running simulations with close encounters, so it is very useful to rewrite the equations so that the singularity doesn't appear. This is called regularization. The equation of motion also works only for classical Newtonian systems, but fails to give correct results when relativistic effects become prominent. Thus the equation must be altered, for example with post-Newtonian terms, to make it applicable in a relativistic setting. The next two chapters tackle these two problems, respectively.

%TODO
%mainitse lopussa että ei toimi täsmälleen tälleen koska suhteellisuusteoria, pohjusta PN hommaa
%koska yhtälöissä on per r termi, tarvitaan regularisaatiota. Menee ikäväksi kun kappaleet ovat hyvin lähellä toisiaan

\section{Regularization}

Because of the $1/r^2$ term in the equation of motion (equation \ref{equ:eomgravity}), the two-body problem becomes singular when the distance between two objects approaches zero. It is because of these singularities that simulations of the two-body problem become very arduous when collisions or close approaches occur. Integrators may attempt to handle the singularity by taking increasingly smaller time-steps as the separation between the particles decreases, which slows down the entire simulation drastically. The problem can be avoided by transforming to a coordinate system which does not exhibit any singularities, and this procedure is known as regularization \citep{bt-galdyn}. 
%Usually this comes with the cost of having a higher number of variables to keep track of. But regularization shows great advantages in the time-step sizes needed for accurate integration, and the amount of time-steps needed for integrating close encounters is much smaller \citep{diplomarbeit}. 

\subsection{One-dimensional regularization}

The one-dimensional case is simple, but gives a good basis for understanding the methods used in the higher-dimensional cases as well. The Hamiltonian for the relative motion in a one-dimensional two-body problem is
\begin{equation} \label{equ:Hamilton1d}
H(q,p,t) = K + V = \frac{p^2}{2 \mu} - \frac{G \mu M}{\left| q \right|} \ ,
\end{equation}
where $q$ and $p$ are the position and momentum respectively, $\mu$ is the reduced mass, $M$ is the total mass, and $G$ is the gravitational constant. 
To regularize the equation of motion we must do both a coordinate transformation and a time transformation. Starting with the coordinate transformation, we do a transformation to some new coordinate system $(Q, P)$. We can perform a canonical point transformation, i.e. a transformation that preserves the form of the Hamilton's equations, where the old coordinates are changed to new ones through $q = Q^2$. A generating function $S$ is defined as a function whose partial derivatives give us the Hamiltonian in the new coordinate system. We can now get the new momentum through a generating function of the form
\begin{equation}
S(p,Q) = pq(Q) = pQ^2
\end{equation}
by calculating its partial derivatives
\begin{equation}
P = \frac{\partial S}{\partial Q} = 2pQ \ ,
\end{equation}
from which we get that
\begin{equation}
p = \frac{P}{2Q} \ .
\end{equation}
By plugging in the new definitions for $q$ and $p$ into equation \ref{equ:Hamilton1d} the new Hamiltonian is then
\begin{equation}
H(Q,P,t) = \frac{P^2}{8Q^2 \mu} - \frac{G \mu M}{Q^2} \ .
\end{equation}

Next we want to do a time transformation as well. We may choose a function $t = t(s)$, where $s$ is the new time variable, often also called fictitious or regularized time. In a more general sense we may choose a time transformation function $g(q,p)$ so that $\mathrm{d} t = g \, \mathrm{d} s$ \citep{ad}. For the transformation to remain canonical, our new Hamiltonian must be obtained through the Poincaré time transformation \citep{mikkola:2008a}. In the Poincaré transformation one takes time to be a canonical coordinate $(t=q_0)$ and the corresponding momentum of time is the binding energy of the system, $p_0 = -E$. The new Hamiltonian obtained through this transformation is then
\begin{equation}
\Gamma(q,p,s) = g(q,p) (H(q,p,q_0) + p_0) \ . \label{equ:timetransform}
\end{equation}
By choosing $g = r = \left| q \right| = Q^2$, where $r$ is simply the particle separation, the transformation gives us
\begin{align}
\Gamma(Q,P,s) &= Q^2 \left( \frac{P^2}{8Q^2 \mu} - \frac{G \mu M}{Q^2} -E \right) \nonumber \\
&= \frac{P^2}{8 \mu} - E Q^2 - G \mu M \ .
\end{align}
The time evolution of a system is uniquely defined by the normal Hamilton equations
\begin{align}
\frac{\mathrm{d} q}{\mathrm{d} t} &= \frac{\partial H}{\partial p} \\
\frac{\mathrm{d} p}{\mathrm{d} t} &= -\frac{\partial H}{\partial q} \ .
\end{align}
Since the time transformation preserves the form of the Hamilton equations, they are now simply
\begin{align}
\frac{\mathrm{d} Q}{\mathrm{d} s} &= \frac{\partial \Gamma}{\partial P} = \frac{P}{4 \mu} \label{equ:ttHamilton1} \\
\frac{\mathrm{d} P}{\mathrm{d} s} &= -\frac{\partial \Gamma}{\partial Q} = 2EQ \label{equ:ttHamilton2} \ .
\end{align}
Taking a second time derivative of equation \ref{equ:ttHamilton1}, we get 
\begin{equation}
\frac{\mathrm{d}^2 Q}{\mathrm{d} s^2} = \frac{1}{4 \mu} \frac{\mathrm{d} P}{\mathrm{d} s}
\end{equation}
and we can insert the result from equation \ref{equ:ttHamilton2} to get the final equation of motion in these new coordinates
\begin{equation}
\frac{\mathrm{d}^2 Q}{\mathrm{d} s^2} = \frac{E}{2 \mu} Q \label{equ:eomRegu} \ .
\end{equation}
The singularity at $r = 0$ has now been removed. In a bound system $E < 0$, so the solution to the equation of motion is a simple harmonic oscillator.

\subsection{Levi-Civita transformation} \label{sect:2dregu}

Regularization of the two-dimensional case is quite similar to the one-dimensional case, but now our positions and momentums are two dimensional vectors. It has been named the Levi-Civita transformation, after the Italian mathematician who first used this method \citep{levi-civita:1920}. The Hamiltonian is now
\begin{equation}
H(\mathbf{r},\mathbf{p},t) = \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} \ .
\end{equation}
The transformation to the new coordinates is given via complex numbers. The new coordinates are $Q_1, Q_2$ so that $\mathbf{Q} = Q_1 + iQ_2$ and $r = x + iy = \mathbf{Q}^2 = (Q_1 + iQ_2)^2$. From this we get $x = Q_1^2 - Q_2^2$ and $y = 2Q_1 Q_2$. By again choosing $g = r = \mathbf{Q}^2$ as in the previous section, the Poincaré transform gives us 
\begin{align}
\Gamma &= r \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} - E \right) \nonumber \\ 
&= \mathbf{Q}^2 \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{\mathbf{Q}^2} - E \right) \nonumber \\
&= \frac{2^2 \mathbf{Q}^2 \cdot \mathbf{p}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \nonumber \\
&= \frac{\mathbf{P}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \ ,
\end{align}
where the new momenta are $\mathbf{P} = 2 \mathbf{Q} \cdot \mathbf{p}$. Similarly to the one-dimensional case, this is the Hamiltonian of a simple harmonic oscillator, except in this case our harmonic oscillator is two-dimensional. The singularity has thus once again been removed.

The transformation $r = \mathbf{Q}^2$ can also be expressed in vector notation as
\begin{equation}
\mathbf{r} = \mathcal{L}(\mathbf{Q}) \mathbf{Q} \ ,
\end{equation}
where $\mathcal{L}(\mathbf{Q})$ is the Levi-Civita matrix
\begin{equation}
\mathcal{L}(\mathbf{Q}) =
\begin{pmatrix}
Q_1 & -Q_2 \\
Q_2 & Q_1
\end{pmatrix} \ .
\end{equation}
This sort of matrix has properties that are useful when formulating transformations also in higher dimensional cases.

\subsection{Kustaanheimo-Stiefel method}
While the two-body problem is limited to two dimensions in theory, in practice there are usually perturbations which perturb the orbit into the third dimension, thus a three-dimensional generalization is desirable. A problem with this arises from the fact that it is not possible to do the generalization directly in three dimensions. The Levi-Civita matrix is a good starting point, and it has several useful properties. All of its elements are linear homogeneous functions of $Q_i$, and the mapping defined by the matrix is conformal or angle-preserving. A matrix with such properties cannot exist in three dimensions, and the next suitable step from a two-dimensional matrix is a four-dimensional matrix \citep{diplomarbeit}. The Kustaanheimo-Stiefel (KS) method involves doing the transformation into three-dimensional space through four dimensions.

The old and new coordinates and momenta are defined as
\begin{equation}
\begin{aligned}
\mathbf{r} &= (r_1, \ r_2, \ r_3, \ r_4)^\mathrm{T} \\
\mathbf{p} &= (p_1, \ p_2, \ p_3, \ p_4)^\mathrm{T} \\
\mathbf{Q} &= (Q_1, \ Q_2, \ Q_3, \ Q_4)^\mathrm{T} \\
\mathbf{P} &= (P_1, \ P_2, \ P_3, \ P_4)^\mathrm{T} \ ,
\end{aligned}
\end{equation}
where $T$ simply means the transpose of the matrix, and the KS matrix is defined as 
\begin{equation}
\mathcal{L}(\mathbf{Q}) =
\begin{pmatrix}
Q_1 & -Q_2 & -Q_3 & Q_4 \\
Q_2 & Q_1 & -Q_4 & -Q_3 \\
Q_3 & Q_4 & Q_1 & Q_2 \\
Q_4 & -Q_3 & Q_2 & -Q_1
\end{pmatrix} \ .
\end{equation}
The transformation is given by the following relations \citep{ad}: 
\begin{equation}
\mathbf{r} = \mathcal{L} \mathbf{Q} \label{equ:KStransform}
\end{equation}
and
%\begin{equation}
%\mathbf{P} = 2 \mathcal{L}^\mathrm{T} \mathbf{p} \ \label{equ:KStransform2} .
%\end{equation}
%Multiplying equation \ref{equ:KStransform2} with $\mathcal{L}$ yields
%\begin{equation}
%\mathcal{L} \mathbf{P} = 2 \mathcal{L} \mathcal{L}^\mathrm{T} \mathbf{p} = 2 \mathbf{Q}^2 \mathbf{p} \label{equ:KStransform3} \ ,
%\end{equation}
%since the KS matrix shares the property of the Levi-Civita matrix that $\mathcal{L} \mathcal{L}^\mathrm{T} = \mathcal{L}^\mathrm{T} \mathcal{L} = \mathbf{Q}^2 \mathbf{I}$, where $\mathbf{I}$ is the identity matrix. From equation \ref{equ:KStransform3} we get that 
%\begin{equation}
%\mathbf{p} = \frac{\mathcal{L} \mathbf{P}}{2 \mathbf{Q}^2} \ ,
%\end{equation}
%and from squaring this we get
\begin{equation}
\mathbf{p}^2 = \frac{\mathbf{P}^2}{4 \mathbf{Q}^2} \ .
\end{equation}
By again using the time transform function $g = r = \left| \mathbf{r} \right| = \mathbf{Q}^2$ we get
\begin{align}
\Gamma = r \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} - E \right) &= \mathbf{Q}^2 \left( \frac{\mathbf{P}^2}{8 \mathbf{Q}^2 \mu} - \frac{G \mu M}{\mathbf{Q}^2} - E \right) \nonumber \\ 
&= \frac{\mathbf{P}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \ ,
\end{align}
which once again is the Hamiltonian of a harmonic oscillator, this time in four dimensions. The coordinate singularity has again disappeared similarly to the previous cases.
However there is still an extra dimension, so we need to find a mapping to convert between the three dimensional physical space and the four dimensional space.

Calculating $\mathbf{r}$ from equation \ref{equ:KStransform} we get
\begin{align} \label{equ:KSsolve}
\mathbf{r} = 
\begin{pmatrix}
r_1 \\
r_2 \\
r_3 \\
r_4
\end{pmatrix}
= \mathcal{L} \mathbf{Q} &=
\begin{pmatrix}
Q_1 & -Q_2 & -Q_3 & Q_4 \\
Q_2 & Q_1 & -Q_4 & -Q_3 \\
Q_3 & Q_4 & Q_1 & Q_2 \\
Q_4 & -Q_3 & Q_2 & -Q_1
\end{pmatrix} \ 
\begin{pmatrix}
Q_1 \\
Q_2 \\
Q_3 \\
Q_4
\end{pmatrix}
\nonumber \\
&= 
\begin{pmatrix}
Q_1^2 - Q_2^2 - Q_3^2 + Q_4^2 \\
2(Q_1 Q_2 - Q_3 Q_4) \\
2(Q_1 Q_3 + Q_2 Q_4) \\
0
\end{pmatrix} \ .
\end{align}
Having the fourth component of $\mathbf{r}$ set to zero makes sense since in physical space we have motion only in three dimensions.
We can still freely choose one of the extra components of $\mathbf{Q}$, so there are many different ways to change between the physical variables and the KS transformed variables. One way is by setting $Q_4 = 0$ and solving the equation \ref{equ:KSsolve} for the $Q_i$, which gives us the following relations
%\begin{align}
%Q_1 &= \left( \frac{1}{2} (Q_1^2 + Q_2^2 + Q_3^2 + Q_1^2 - Q_2^2 - Q_3^2) \right)^{1/2} = \left( \frac{1}{2} ( \left| \mathbf{r} \right| + \left| r_1 \right| ) \right)^{1/2} \nonumber \\
%Q_2 &= \frac{1}{2} \frac{2 Q_1 Q_2}{Q_1} = \frac{1}{2} \frac{r_2}{Q_1} \\
%Q_3 &= \frac{1}{2} \frac{2 Q_1 Q_3}{Q_1} = \frac{1}{2} \frac{r_3}{Q_1} \nonumber \ 
%\end{align}
\begin{align}
Q_1 &= \left( \frac{1}{2} ( \left| \mathbf{r} \right| + \left| r_1 \right| ) \right)^{1/2} \nonumber \\
Q_2 &= \frac{1}{2} \frac{r_2}{Q_1} \\
Q_3 &= \frac{1}{2} \frac{r_3}{Q_1} \nonumber \ 
\end{align}
and the mapping
\begin{equation}
\mathbf{Q} = 
\begin{cases}
\left( Q_1, \ Q_2, \ Q_3, \ 0 \right)^\mathrm{T} \ , \ \mathrm{if} \ r_1 \geq 0 \\
\left( Q_1, \ Q_2, \ 0, \ Q_3 \right)^\mathrm{T} \ , \ \mathrm{if} \ r_1 < 0 \ .
\end{cases}
\end{equation}
With these mappings we can convert the coordinates between the different dimensional spaces \citep{diplomarbeit, ad}.

Regularized computational methods are very precise, but they are typically more computationally expensive and it is difficult to regularize many particles at the same time, so there would be benefits for an accurate method that does not need this kind of regularization. Fortunately there exists another method which involves only time transformation but no coordinate transformation, named algorithmic regularization (AR) \citep{diplomarbeit}. These kinds of regularization methods do not actually remove the Newtonian collision singularities from the equations of motion, but the equations can be evaluated in a way that they behave regularly regardless. Algorithmic regularization will be discussed in more detail in chapter \ref{chap:ar-chain}.


%\noindent (TODO: Syvällisemmin KS:ää? Lisää alakohtia? Jonkunlainen loppukaneetti.)

\section{Post-Newtonian dynamics} \label{sect:pndynam}

\subsection{Approximating general relativity}

While Newtonian dynamics is a perfectly valid approximation in everyday life, it stops giving correct results for very massive compact objects or objects moving at a significant fraction of the speed of light. In these situations we need to use general relativity (GR), which describes gravity as a geometric property of spacetime. This theory is described by the Einstein field equations, which are a set of 10 coupled, non-linear partial differential equations that describe the fundamental interaction of gravitation as a result of spacetime being curved by mass and energy. The field equations are usually expressed as
\begin{equation}
R_{\mu \nu} - \frac{1}{2}R g_{\mu \nu} + \Lambda g_{\mu \nu} = \frac{8 \pi G}{c^4} T_{\mu \nu} \ ,
\end{equation}
where $R_{\mu \nu}$ is the Ricci curvature tensor and $R$ is the scalar curvature which both are used to describe how the geometry of curved space differs from the geometry of flat Euclidean space. They are functions of the variable $g_{\mu \nu}$ which is the metric tensor which describes the geometric and causal structure of spacetime in our universe. The parameter $\Lambda$ is the cosmological constant which describes the energy density of the vacuum, and the variable $T_{\mu \nu}$ is the stress-energy tensor which describes mass and energy as the source of gravity in the universe. Finally $G$ and $c$ are the gravitational constant and the speed of light respectively.

These equations are very hard to solve exactly due to their non-linear nature, and doing fully general relativistic simulations is computationally very expensive, so more wieldy methods are required for large-scale simulations. This is where post-Newtonian (PN) expansions come in. Post-Newtonian expansions in general relativity are used for finding an approximate solution of the Einstein field equations in the weak field approximation.
In other words, the post-Newtonian theory is an approximate version of general relativity that applies when the gravitational field is relatively weak, and the motion of the matter is relatively slow, $v/c \sim 0.01$. The theory successfully describes the gravitational field of our solar system for example, but it can also be applied to situations involving compact bodies, such as neutron stars or black holes.
%
%The post-Newtonian theory is derived from the Landau-Lifshitz formulation of the Einstein field equations. The equations can be written as 
%\begin{equation}
%\partial_{\mu \nu} H^{\alpha \mu \beta \nu} = \frac{16 \pi G}{c^4}(-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL}) \ ,
%\end{equation}
%where $H^{\alpha \mu \beta \nu} \equiv \mathfrak{g}^{\alpha \beta} \mathfrak{g}^{\mu \nu} - \mathfrak{g}^{\alpha \mu} \mathfrak{g}^{\beta \mu}$ is a tensor density which possesses the same symmetries as the Riemann tensor. In the Landau-Lifshitz formulation the main variables are not the components of the metric tensor $g_{\alpha \beta}$, but those of the gothic inverse metric $\mathfrak{g}^{\alpha \beta} \equiv \sqrt{-g} g^{\alpha \beta}$, where $g^{\alpha \beta}$ is the inverse metric, and $g$ the metric determinant. $T^{\alpha \beta}$ is the energy-momentum tensor of the matter source term, and the Landau-Lifshitz pseudotensor $(-g) t^{\alpha \beta}_{LL} \sim \partial \mathfrak{g} \cdot \partial \mathfrak{g}$ can be interpreted as an energy-momentum (pseudo)tensor for the gravitational field.
%
%The antisymmetry of $H^{\alpha \mu \beta \nu}$ implies the conservation equation
%\begin{equation}
%\partial_\beta \left[ (-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL}) \right] = 0 \ ,
%\end{equation}
%which is formally equivalent to $\nabla_\beta T^{\alpha \beta} = 0$, where $\nabla_\beta$ is the covariant derivative operator. The conservation equation allows for the formulation of global conservation laws, for example for energy, linear momentum, and angular momentum. We then introduce the gravitational potentials $h^{\alpha \beta} = \eta^{\alpha \beta} - \mathfrak{g}^{\alpha \beta}$, where $\eta^{\alpha \beta} = diag(-,+,+,+)$ is the Minkowski metric expressed in Lorentzian coordinates, and impose the harmonic coordinate gauge condition $\partial_\beta h^{\alpha \beta} = 0$.
%
%The field equations become a wave equation in flat spacetime
%\begin{equation} \label{equ:waveeq}
%\square h^{\alpha \beta} = -\frac{16 \pi G}{c^4} \tau^{\alpha \beta} \ ,
%\end{equation}
%where $\square = -\frac{1}{c^2} \frac{\partial^2}{\partial t^2} + \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$ is the flat spacetime d'Alembert operator, and $\tau^{\alpha \beta} = (-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL} + t^{\alpha \beta}_H)$ is defined as the effective energy-momentum pseudotensor, composed of a matter contribution, the Landau-Lifshitz contribution, and the harmonic gauge contribution $t^{\alpha \beta}_H \sim \partial h \cdot \partial h + h \partial^2 h$. The conversation equation now reads $\partial_\beta \tau^{\alpha \beta} = 0$.
%
%So far no approximations have been made, and the wave equation combined with the harmonic gauge condition and conservation equation is an exact formulation of the Einstein field equations. The wave equation determines the potential $h^{\alpha \beta}$ for a given distribution of matter. The behaviour of the matter is determined by the conservation equation/gauge condition.
%
%The wave equation can be integrated without enforcing the conservation equation, and this is known as the relaxed Einstein field equations. The integration is achieved by iteration. Assuming that $h^{\alpha \beta}_n$ is known, $h^{\alpha \beta}_{n+1}$ is obtained by solving
%\begin{equation}
%\square h^{\alpha \beta}_{n+1} = -\frac{16 \pi G}{c^4} \tau^{\alpha \beta}[h_n] \ .
%\end{equation}
%The iterations are started with $h^{\alpha \beta}_0 = 0$, and stopped when the desired accuracy is obtained. In principle, the truncation is the only source of approximation. The procedure produces a formal expansion of $h^{\alpha \beta}$ in powers of G. This is known as the post-Minkowskian expansion of the gravitational field. After iterations have been done up to the desired accuracy, we can again impose the gauge condition to get a proper metric.
%
%The difference between post-Minkowskian and post-Newtonian expansion is that in post-Newtonian approximation we assume a slow-motion condition, i.e. all speeds within the matter distribution (such as the speed of sound within a body, or the speed of the body as a whole) are small compared with the speed of light. In astrophysical situations the assumption of slow speeds is accurate in the vast majority of cases, since the virial theorem implies that $U \sim v^2$ for any gravitationally bound system; weak fields are naturally accompanied by slow motion.
The approximations are expanded in terms of a small parameter $\varepsilon \sim v/c$, which expresses orders of deviation from Newton's laws of gravity. A correction of order $(v/c)^n$ to a Newtonian expression is said to be of $(n/2)\mathrm{PN}$ order. So for example the correction 3.5PN would contain terms up to the order $(v/c)^7$ \citep{will:2006}.

The post-Newtonian corrections are known for a general N-body system only up to the first order, 1.0PN. These corrections are described by the Einstein-Infeld-Hoffman (EIH) equations \citep{einstein:1938}, and the equations of motion are given by
\begin{align}
\mathbf{a}_{\mathrm{Newton},i} + \mathbf{a}_{\mathrm{1.0PN}, i} &= \sum_{j \neq i} \frac{G m_j \mathbf{n}_{ji}}{r_{ij}^2} + \frac{1}{c^2} \sum_{j \neq i} \left[ \vphantom{\sum_{k \neq j} \frac{G m_k}{r_{jk}}} v_i^2 + 2 v_j^2 - 4(\mathbf{v}_i \cdot \mathbf{v}_j) \right. \nonumber \\
&- \left. \frac{3}{2}(\mathbf{n}_{ij} \cdot \mathbf{v}_j)^2 - 4 \sum_{k \neq i} \frac{G m_k}{r_{ik}} - \sum_{k \neq j} \frac{G m_k}{r_{jk}} + \frac{1}{2} (\mathbf{r}_{ji} \cdot \mathbf{a}_j) \right] \nonumber \\
&+ \frac{1}{c^2} \sum_{j \neq i} \frac{G m_j}{r_{ij}^2} \left[ \mathbf{n}_{ji} \cdot (4 \mathbf{v}_i - 3 \mathbf{v}_j) \right] (\mathbf{v}_i - \mathbf{v}_j) \nonumber \\
&+ \frac{7}{2c^2} \sum_{j \neq i} \frac{G m_j \mathbf{a}_j}{r_{ij}} + \mathcal{O} \left( \frac{v}{c} \right)^4 \ . \label{equ:PN1Nbody}
\end{align}
Here $\mathbf{n}_{ij} = \mathbf{r}_{ij}/r_{ij}$ is the unit vector pointing from $i$ to $j$, and the big O notation indicates terms of the order $(v/c)^4$ and higher. One can see that the acceleration of a particular body depends on the accelerations of all the other bodies in the system, a new relativistic feature which does not appear in purely Newtonian systems. And since the accelerations that are being calculated also appear on the right hand side of the equation, the equations must be solved iteratively if exact solutions are required. However using the usual Newtonian accelerations on the right hand side of the equation in place of the PN corrected accelerations works as a good first approximation.

\subsection{Gravitational waves}

While the EIH equations (Eq. \ref{equ:PN1Nbody}) give accurate results for example in our solar system, it is important to get more accurate approximations also for more extreme systems. General equations of PN corrections for a three-body system are known up to order 2.0PN \citep{gravity}. However, if we consider only a two-body system, PN corrections are known up to order of 3.5PN due to the relative simplicity of the situation. An extremely interesting effect arises from these later terms. The first three integer terms 1.0PN, 2.0PN, and 3.0PN are conservative, and we can find expressions for the total energy and angular momentum of the system. However the 2.5PN and 3.5PN corrections contain dissipative terms, and when these are taken into account the total energy and angular momentum of the system is not conserved. This is called the radiation reaction effect, and these losses are due to the emission of gravitational waves (GW) \citep{gravwaves}. After 3.5PN corrections, even the integer terms start contributing to the radiation reaction. Gravitational waves are generated by accelerating masses, and can be considered wave like disturbances in the curvature of spacetime. This effect causes the orbit in the two-body problem to shrink and circularize as energy is radiated away from the system. Gravitational waves play a major role for example in the final phases of a black hole binary merger.

The 2.5PN term contains the most dominant radiation reaction terms, and its effects on the system can be calculated by formulas derived by \cite{peters-mathews:1963}, which give the amount of energy and angular momentum the system loses, averaged over a single orbital period:
\begin{align}
\left< \frac{\mathrm{d} E}{\mathrm{d} t} \right> &= - \frac{32}{5} \frac{G^4 \mu^2 M^3}{c^5 a^5} \frac{1 + \frac{73}{24}e^2 + \frac{37}{96}e^4}{(1-e^2)^{7/2}} \\
\left< \frac{\mathrm{d} L_z}{\mathrm{d} t} \right> &= - \frac{32}{5} \frac{G^{7/2} \mu^2 M^{3/2}}{c^5 a^{7/2}} \frac{1 + \frac{7}{8}e^2}{(1-e^2)^2} \ .
\end{align}
Here $a$ and $e$ are simply the Keplerian semimajor axis and orbital eccentricity of the system, and $\mu$ and $M$ are the reduced and total mass of the two bodies. It should be noted that these formulas consider only the effects of the 2.5PN term, and not any of the other terms. \cite{peters:1964} also derived equations for the changes in the semimajor axis and eccentricity of the orbit as a result of these GW losses:
\begin{align}
\left< \frac{\mathrm{d} a}{\mathrm{d} t} \right> &= - \frac{64}{5}\frac{G^3 \mu M^2}{c^5 a^3} \frac{1 + \frac{73}{24}e^2 + \frac{37}{96}e^4}{(1-e^2)^{7/2}} \label{equ:GWsemimajor} \\
\left< \frac{\mathrm{d} e}{\mathrm{d} t} \right> &= - \frac{304}{15} e \frac{G^3 \mu M^2}{c^5 a^4} \frac{1 + \frac{121}{304}e^2}{(1-e^2)^{5/2}} \ .
\end{align}
Since the terms are negative, the emission of gravitational waves shrinks and circularizes the orbit. One can also see that when the eccentricity is near 1 or as the semimajor axis approaches 0, these effects strongly increase. This makes sense, as in both of these cases the orbiting body passes close to the other massive body in deep plunging orbits, entering very deep into the gravitational well and having high orbital velocity.

These correction terms which are higher than 1.0PN cause important effects and allow us to examine more extreme systems without using full GR. Thus it is important to include them in large-scale simulations. One method is to use what is known as the two-body formulation, where we calculate the PN corrections for each particle pair separately, where at least one of the particles is a black hole. PN corrections are not expected to be of major significance for interactions between stellar, gas, or dark matter particles. Another method is the cross-term formulation \citep{will:2014}, which is an approximation of the EIH equations (Eq. \ref{equ:PN1Nbody}) for the case when a single large mass dominates the system. This is computationally expensive, so it only works in situations where there are at most hundreds of particles.

\subsection{The first application of PN corrections}

The first use of a PN expansion (to the first order) was made by Einstein himself in calculating the perihelion precession of the orbit of Mercury.
Mercury's perihelion precesses (rotates) around the Sun. This is mainly due to perturbations caused by the presence of the other planets. Another much less significant factor is the oblateness of the Sun. In the 1800s it was noticed that the orbit deviates from the precession predicted by these Newtonian effects by about 43 arc seconds per one tropical century.
Several different reasons for this deviation were proposed, and perhaps the most prevalent of these was Vulcan. Vulcan was believed to be a small hypothetical planet that was supposed to orbit the Sun inside the orbit of Mercury. The perturbations caused by this planet could have explained the anomalous precession of Mercury. 
%The existence of the planet was hypothesized by the French mathematician Urbain Le Verrier, who also predicted the existence and position of Neptune with only mathematics, so his theory was thought plausible. While many people claimed to have observed the planet, its supposed orbit was so close to the sun that this was extremely challenging to verify.
There were attempts to confirm the existence of the planet, but in 1915 the hypothetical planet was quite firmly put to rest when Einstein explained the excess precession through his theory of relativity. General relativity gives the equation for the advance of the perihelion of a planet in our solar system in radians per a single orbit:
\begin{equation}
\langle \dot{\omega} \rangle = \frac{6 \pi G M}{c^2 a (1-e^2)} \ . \label{equ:pericenterShift}
\end{equation}
Here $M$ is the total mass of the system, $a$ is the semimajor axis of the orbit of the planet, and $e$ is the orbital eccentricity \citep{will:tegp}.

The reason why the same extra precession that affects all of the planets had been observed only in Mercury is that the magnitude of the differences from simple Newtonian theory diminishes rapidly as one gets farther from the Sun. As can be seen from equation \ref{equ:pericenterShift}, the rate of the precession increases as eccentricity approaches 1 and as the semimajor axis gets shorter.  Mercury's fairly eccentric orbit makes it much easier to detect the perihelion shift than is the case for the nearly circular orbits of Venus and Earth. 
This was the first case of solving the general relativistic two-body problem, which is the most common use of the PN expansion nowadays.
%Hyvää matskua: gravity postminkowskian implementation sivu 10, gravity postnewtonian fundamental heti alku, common reader 3.2
%In the case of a two-body system, the post-Newtonian corrections give rise to a perturbed Keplerian orbit. The only secular effect on the orbit is the pericenter advance.
%PN1 terms are the ones responsible for the advance of the pericenter of an eccentric orbit, given by $\dot{\omega} = 6 \pi f_\mathrm{b} m/a (1-e^2)$, where $a$ and $e$ are the semimajor axis and eccentricity of the orbit, and $f_\mathrm{b}$ is the orbital frequency, given by Kepler's third law $2 \pi f_\mathrm{b} = (m/a^3)^{1/2}$ \citep{will:2006}. 
%
%The intrinsic angular momentum (spin) of a body is also a source of gravity that affects the metric and body motions. The spin causes precession in the ascending node. %TODO lisää kaavaa, gravity kalvoista

There are some systems that cannot be properly described by a post-Newtonian approximation because of their extreme conditions. Some examples of such systems include the final phases of a compact object merger, the cores of supernovae, and the structure of rapidly rotating neutron stars. These must be analysed using different methods, for example the full solution of Einstein's equations using numerical methods \citep{will:2006}. 

\section{Black hole mergers}

Galaxy formation is believed to be a hierarchical or ``bottom-up'' process, where smaller systems interact and merge multiple times, forming ever larger systems \citep{bt-galdyn}. This hierarchical model will also logically result in the central supermassive black holes that most galaxies host interacting with other black holes during the merger events. The black holes will first form binaries and then eventually merge with each other.

There are three main phases that the black holes go through before their eventual merging. At first, when the galaxies start merging, the black holes start to sink into the center of the combined galaxy system due to dynamical friction. They will eventually form a gravitationally bound binary system. The semimajor axis of the bound system depends on the masses of the black holes. For black holes with masses of $\sim 10^9 \, M_\odot$, the semimajor axis will be $\sim 100$ pc \citep{rantala:2018}. Next the semimajor axis of the binary will shrink, also known as hardening, due to stars interacting with the binary. In this process stars are ejected, carrying away angular momentum and energy from the system. Finally, once the binary is close enough together, it will start to noticeably lose energy due to the emission of gravitational waves, until their eventual coalescence. This will start happening at a semimajor axis of $\sim 0.1$ pc for the aforementioned masses \citep{rantala:2018}. These different regimes will be discussed in the next sections.

\subsection{Dynamical friction}

When galaxies merge, the interactions between their stars cause the stars to transfer momentum and kinetic energy from their relative orbital motion to random motion in the galaxy, which is known as violent relaxation. This leads to the orbital decay of the stars and causes the stellar matter to sink into the system. This process, known as dynamical friction, applies to large clusters of stars or even a single massive particle, like a black hole, moving through a larger host system \citep{bt-galdyn}. It is important to note that dynamical friction is not caused by actual collisions, only by gravitational interactions between the particles.

An intuitive way of understanding the process is to think of a body of mass $M$ travelling through a population of particles of mass $m_a$, where $m_a \ll M$. We call the mass $M$ the subject body and the smaller particles of mass $m_a$ field stars. We assume that the field stars are members of a large host with a total mass much greater than $M$, so that it can be approximated as a nearly infinite and homogeneous system. Thus when the subject body moves through the sea of field particles, it attracts the smaller particles through gravity, and leaves behind a so called gravitational wake. This higher concentration of mass behind the particle compared to the mass in front of the particle causes a drag force that slows down the subject body. This is illustrated in figure \ref{fig:dynfric}.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.33]{../images/dynfric.pdf}
\caption{A simplified image showing the basic mechanism behind dynamical friction. The subject body $M$ attracts the field particles towards itself as it travels, which creates a concentration of mass behind it. This creates a dynamical friction force $F_\mathrm{df}$ which slows down the subject body.}
\label{fig:dynfric}
\end{figure}

\cite{chandrasekhar:1943} derived a formula for the resulting force on the subject body by this effect. This force is expressed as
\begin{equation}
\mathbf{F}_{\mathrm{df}} = M \frac{\mathrm{d} \mathbf{v}_M}{\mathrm{d} t} = -16 \pi^2 G^2 M^2 m_a \mathrm{ln} \Lambda \left[ \int_0^{v_M} v_m^2 f(v_m) \mathrm{d}v_m \right] \frac{\mathbf{v}_M}{v_M^3} \label{equ:dynamicalfriction}
\end{equation}
and is called Chandrasekhar's dynamical friction formula. Here $f(v_m)$ is the distribution function of the field particles. A distribution function gives the number of particles per phase space volume having approximately the specified velocity near a certain position. Chandrasekhar's formula assumes that the field stars have an isotropic velocity distribution, and thus position does not affect the distribution function. The function is normalized so that integrating it over all the velocities gives the number density of particles near the subject body. The dynamical drag force always opposes the motion of the subject body, similarly to ordinary frictional drag.

The logarithm $\mathrm{ln} \Lambda$ is known as the Coulomb logarithm, and if we approximate that the particles are point masses, it is defined as
\begin{equation}
\mathrm{ln} \Lambda \approx \mathrm{ln} \left( \frac{b_{\mathrm{max}}}{b_{90}} \right) \ . \label{equ:coulomblog}
\end{equation}
Here $b$ means the impact parameter of a two-body encounter, defined as the shortest distance between the two particles if they passed each other without any interaction. The parameter $b_{\mathrm{max}}$ is the maximum impact parameter considered, and $b_{90}$ is the impact parameter at which the direction of the orbit of the passing particle will be deflected by $90^{\circ}$. This latter parameter can be expressed as
\begin{equation}
b_{90} = \frac{G(M+m_a)}{v^2_{\mathrm{typ}}} \ ,
\end{equation}
where $v_{\mathrm{typ}}$ is the typical encounter speed between the particles. Since we assumed that $M \gg m_a$, we can approximate that $M+m_a \approx M$. This gives the Coulomb logarithm in the form of
\begin{equation}
\mathrm{ln} \Lambda \approx \mathrm{ln} \left( \frac{b_{\mathrm{max}} v^2_{\mathrm{typ}}}{GM} \right) \ .
\end{equation}
This logarithm basically describes the cut-off of the friction effect at the maximum impact parameter \citep{galform}.

If the subject mass moves slowly, so that we can assume that $v_M$ is small, the field star distribution function can be approximately replaced with $f(0)$. In this case the integral is easy to evaluate, as we can take the function out of the integral and we are left with $\int_0^{v_M} v_m^2 \mathrm{d} v_m = v_M^3/3$. From this we get the following form for equation \ref{equ:dynamicalfriction}: 
\begin{equation}
\mathbf{F}_{\mathrm{df}} = -\frac{16 \pi^2}{3} G^2 M^2 m_a \, \mathrm{ln} \Lambda \, f(0) \mathbf{v}_M \ . \label{equ:dynfricslow}
\end{equation}
Very interestingly, if instead the subject body moves at a high velocity, so that $v_M \gg v_m$, this behaviour changes. We can assume that the subject body moves at a higher velocity than all the surrounding field particles, so we are effectively integrating over all the velocities of the field particles. If we move $4\pi$ inside the integral, we have $\int_0^{v_M} 4 \pi v_m^2 f(v_m) \mathrm{d}v_m$, where $4 \pi v_m^2$ is a shell in velocity space. Since $v_M$ is large we are integrating over the whole velocity volume, and as it was mentioned earlier the distribution function is normalized to give the number density $n$ of particles near the subject body. So from the equation \ref{equ:dynamicalfriction} we get
\begin{equation}
\mathbf{F}_{\mathrm{df}} = -4 \pi G^2 M^2 m_a n \, \mathrm{ln} \Lambda \, \frac{\mathbf{v}_M}{v_M^3} \ . \label{equ:dynfricfast}
\end{equation}
Here the mass density $m_a n$ can be replaced with $\rho(< v_M)$, the overall density of the field particles moving slower than the subject body \citep{bt-galdyn}.

From equations \ref{equ:dynfricslow} and \ref{equ:dynfricfast} we can see that unlike with hydrodynamic friction, where the drag always increases as the velocity increases, the situation here is more complicated. At sufficiently slow velocities dynamical friction is relative to the velocity of the subject body, $F_{\mathrm{df}} \propto v_M$. This is similar to ordinary drag where the force increases with velocity. But at larger velocities the frictional force begins to fall off, $F_{\mathrm{df}} \propto v_M^{-2}$. And as the mass density of the field stars can be replaced with the overall density, the force is independent of the individual masses of the field particles, so the formula is also valid for a field with distributed particle masses.

Chandrasekhar's formula makes several unrealistic assumptions when applied to a real astrophysical system. It assumes that all of the particles are point masses, it does not take into account the self-gravity of the wake left by the subject body, and it assumes a homogeneous and infinite distribution of the field particles, meaning that the choice of $b_\mathrm{max}$ and $v_\mathrm{typ}$ are a bit arbitrary in real scenarios. Despite these assumptions, Chandrasekhar's formula gives good approximations for the drag effect on a supermassive black hole moving through a field of stars. Taking these effects into account leads to an orbital decay time that is roughly twice as long compared to the estimate from Chandrasekhar's formula \citep{weinberg:1989}.

The point mass approximation can be corrected by using the half-mass radius $r_h$, which is the radius of the subject body that contains half of its total mass, in place of $b_{90}$ in equation \ref{equ:coulomblog} \citep{bt-galdyn}. If the closest approach of a field star to the center of the subject body is $\lesssim r_h$, the orbit of the star will be less affected and thus its contribution to the drag force will be smaller than if the subject body was a point mass. Conversely the total drag force is largely unaffected if $r_h \lesssim b_{90}$. Thus using the half-mass radius gives a good estimate for the dynamical friction in case the subject body is not considered to be a point mass.

The fact that the exact values for the parameters in the Coulomb logarithm are hard to determine does not cause as much error as one might imagine. The ratio between $b_\mathrm{max}$ and $b_{90}$ is typically quite large. However we take the logarithm, so the end result is much smaller and not very sensitive to changes of order unity in the parameters. 

Finally, if we want take self-gravitation of the system into account as well, we need to use a more rigorous method such as linear response theory. Instead of considering only independent two-body interactions, the subject body is considered to be a moving external potential which causes a response density in the host system. In turn, the gravitational effect of this response density causes a drag effect on the original potential. This drag force is dynamical friction \citep{galform}. 


\subsection{Three-body scattering}

As the black holes sink close to the center of the newly merged galaxy system, they will eventually reach a point where they form a gravitationally bound binary. The distance where this happens is dependent on the masses of the black holes, but for black holes of masses $\sim 10^9 \, M_\odot$, the semimajor axis of the system is $\sim 100$ pc \citep{rantala:2018}, and the black hole orbital velocities exceed the local stellar velocity dispersion. At this point the dynamical friction formula is no longer valid, and instead the evolution of the binary is mainly determined by its interactions with the surrounding stellar population. The binary hardens, i.e. its separation shrinks, by losing kinetic energy to stars that pass close to it through complex three-body interactions, similar to the gravitational slingshot mechanisms used to accelerate spacecraft in our solar system. This hardening happens at a nearly constant rate of
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} \left( \frac{1}{a} \right) \propto \frac{G \rho}{\sigma}  \ ,
\end{equation}
if we assume a constant stellar density $\rho$ and velocity dispersion $\sigma$ \citep{quinlan:1996}. Here $a$ is the semimajor axis of the bound system. These stars can be ejected from the system at high velocities, spreading the central stars to wider orbits. The velocity of the ejected bodies is typically comparable to the circular orbital velocity of the binary
\begin{equation}
v_\star \sim V_{\mathrm{bin}} = \sqrt{\frac{2 G M_\bullet}{a}} \ ,
\end{equation}
where $a$ is the semimajor axis of the binary and $M_\bullet$ is the mass of the binary. The range of ejection velocities is broad, and some stars can even be ejected at high enough velocities to escape the potential of the galaxy, over $1000$ $\mathrm{km/s}$ for a $\sim 10^9 M_\odot$ binary \citep{rantala:2018}. These are so-called high velocity stars (HVS).

This analysis fails however if the stellar population surrounding the binary is not actually constant. For a star to be able to get close enough to interact strongly with the binary, it has to have angular momentum of $L \lesssim \sqrt{G(M_1 + M_2)a}$, where $M_1$ and $M_2$ are the masses of the black holes, and $a$ is the semimajor axis \citep{bt-galdyn}. The region of stars with low enough angular momentum is called the loss cone. As the stars that interact with the binary are ejected out of the system, the loss cone is slowly drained of stars. If the loss cone is completely emptied, the binary cannot harden any more, and thus its evolution is halted. This is the so called final-parsec problem, and there must be some mechanisms to refill the loss cone if black hole mergers do happen.

One clear candidate for this is two-body relaxations between stars which cause them to end up inside the loss cone \citep{milosavljevic:2003}. In non-spherical galaxies torques could drive gas to the central region of the galaxy, which would also help the hardening process \citep{mayer:2007}.
Thus the final parsec problem is more problematic in spherical galaxies where the mass inside the loss cone is the smallest due to stars not being able to be on as varied orbits as in non-spherical galaxies, and there are no torques to drive gas inwards. In triaxial and axisymmetric galaxies however the asymmetric perturbations are able to cause stars to end up on centrophilic orbits that would refill the loss cone. If the host of the binary were to undergo another merger while the black holes are in the process of merging, the perturbations caused by this could break the  symmetry of galactic potentials, and efficiently refill the loss cone.

N-body simulations of non-spherical systems suggest that the final parsec problem is not as severe as one might think. The problems has been found to not hamper the hardening of binaries in triaxial \citep{berczik:2006} and axisymmetric galaxies \citep{khan:2013}.
The simulations however are not completely accurate. Approximating a galaxy with $N \gtrsim 10^9$ stars by a model consisting of only around $N \lesssim 10^6$ particles leads to enhanced two-body relaxation timescales. Thus the loss cone is much less depleted than in real galaxies \citep{milosavljevic:2003}. This problem could be alleviated with simulations capable of handling a much larger amount of particles. Simulations with $N \sim 10^7 - 10^8$ particles could reproduce a more realistic, diffusive behaviour of a galaxy.

\subsection{Gravitational waves}

If the black hole binary overcomes the final-parsec problem and reaches a sufficiently small distance, it will lose enough energy through the emission of gravitational waves to eventually coalesce in less than the Hubble time \citep{milosavljevic:2003}. This separation is $\sim 0.1$ pc for a binary with a mass of $\sim 10^9 \, M_\odot$ \citep{rantala:2018}. Hubble time is the time it would take for the universe to expand to its current state if it expanded at a constant Hubble rate. The rate at which the semimajor axis shrinks due to the radiation reaction effect can be estimated with the Peters and Mathews equation \ref{equ:GWsemimajor}. The evolution of the semimajor axis $\dot{a} \propto -a^{-3}$, and if we assume a constant eccentricity we can approximate the coalescence timescale with \citep{bt-galdyn}
\begin{equation}
t_c \sim -\frac{a}{4\dot{a}} \ .
\end{equation}

The longest evolutionary phase for a black hole binary is when the radius of the bound system is below that at which it becomes hard, but still above the radius at which gravitational waves become prominent \citep{begelman:1980}. This is known as the bottleneck radius, and it marks the area where binary black holes are most likely observed. The decay time at this bottleneck radius is uncertain, and depends on both the population of the loss cone and how much gas affects the evolution of the binary \citep{bt-galdyn}. If the coalescence time is very long, over 10 Gyr, we would expect to be able to observe these binaries at the centers of most galaxies. If the timescale is shorter, we could observe the coalescence events happening.

Current observational evidence suggests that long-lived supermassive black hole binaries might be quite rare. Though there are many implied candidates based on periodic variability, no concrete observations of a supermassive black hole binary have been made. If both of the supermassive black holes of a binary are quasars and they are very close to each other, it is difficult to tell them apart from a singular active galactic nuclei \citep{roedig:2014}. In the future observations made by the LIGO and LISA gravitational wave detectors might give concrete proof of the merging and thus existence of supermassive black hole binaries, and we might be able to even make interferometric direct observations of a binary black hole.

\chapter{AR-CHAIN} \label{chap:ar-chain}
%TODO kerronko introssa siitä et miks simulaatiota tehään, modaa sen perusteella mitä tässä sanotaan, kuinka paljon taustaa. 
%Selitä et gadget on muuten hyvä paitsi että, ja miks sit ketju on jebin

One of the widely used software packages for galaxy-scale simulations is the smoothed particle hydrodynamics simulation code GADGET-3 \citep{springel:2005}. It can be used to effectively simulate the global dynamics of galaxies, for simulating galaxy mergers, and even for cosmological simulations. Because the number of particles that is feasible to simulate on these scales is magnitudes smaller than the number of real particles, e.g. stars, there is need to account for the fact that these simulated particles are not real individual objects but rather large clumps that represent the real particles. Because of this these objects are ``softened'' by spreading their gravitational potential, making them non-point masses. One limitation of the software is that due to the use of softening, it cannot resolve very small-scale effects.

To this end \cite{rantala:2017} developed an extension for GADGET-3 called KETJU. The code implements algorithmically regularized regions around supermassive black holes to allow for solving the detailed dynamics of these systems, while still getting the benefits of the galaxy-wide efficiency of GADGET-3. Algorithmic regularization means dealing with the difficult close encounters and collisions of point masses in a dynamical simulation via a time transformation, while not actually removing the collisional singularity from the equations of motion.

KETJU utilizes the algorithmic regularization chain, or AR-CHAIN, method \citep{mikkola:2002, mikkola:2006,mikkola:2008b}, which combines algorithmic regularization with an older Kustaanheimo-Stiefel chain method \citep{mikkola:1993}. In this method computational time is saved by KS-regularizing only certain interparticle vectors, so that the selected vectors and particles form a continuous chain. 

The AR-CHAIN method is comprised of three important parts: algorithmic regularization, the use of a chained coordinate system to reduce roundoff errors, and the Gragg-Bulirsch-Stoer extrapolation method for attaining very high numerical integration accuracy. These three parts will next be discussed separately in more detail. Lastly there will be an overview of the actual implementation of this method.


%Koodi flowchartti jotenki

%Kuva leapfrogista
%Mukaileva kuva antin BS esimerkistä?


\section{Algorithmic regularization}

The main goal of the algorithmic regularization method is to be able to deal with close encounters of particles in dynamical systems without having to resort to more computationally expensive regularization methods, like the Kustaanheimo-Stiefel regularization. This can be achieved via two slightly different time transformations. 

The first approach is called the Logarithmic Hamiltonian (LogH) method, which was discovered by \cite{mikkola:1999} and \cite{preto:1999}. To guarantee simulation accuracy, it is desirable to be able to use a symplectic integration method, i.e. a method which constrains the allowed motions in phase space strongly enough so that the usual tendency of numerical orbit integrations to drift in energy is absent. In practice a symplectic integrator conserves the energy of the system. The leapfrog integrator is one of the most common ones.

\subsection{Leapfrog}
A leapfrog can be used for numerically integrating second order differential equations of the form $d^2x/dt^2 = F(x)$, equivalently $dx/dt = v , \ dv/dt = F(x)$. It gains its name from the way it updates the positions and velocities of the system at interleaved time points, so that they ``leapfrog'' over each other. The integration can be performed in two different ways, depending on whether we first update the position, which is known as drifting, or the velocity, which is known as kicking. If we update the position first it is called the ``Drift-Kick-Drift'' (DKD) version, and conversely if we update the velocity first it is called the ``Kick-Drift-Kick'' (KDK) version. The equations for a DKD version are
\begin{equation}
\begin{aligned}
x_{i+1/2} &= x_{i}+v_{i} \frac{\Delta t}{2} \\
v_{i+1} &= v_{i}+F(x_{i+1/2})\Delta t \\
x_{i+1} &= x_{i+1/2}+v_{i+1}{\frac {\Delta t}{2}} \ ,
\end{aligned}
\end{equation}
where $x_i$ and $v_i$ are the values at step $i$, and $\Delta t$ is the time step. In a dynamical system the function $F(x)$ is simply the acceleration. First the position is updated by half a time step, then the velocity is updated based on this half step value, and finally the position is updated again so that the whole system has advanced by one time step. In addition to its symplectic nature, the leapfrog method is also time reversible, meaning that we can use it to accurately integrate the system backwards in time.

\subsection{Logarithmic Hamiltonian}
To be able to use an explicit symplectic method such as the leapfrog, we need to have a Hamiltonian that is separable, i.e. where the different parts contain only one type of canonical coordinates.
For an N-body system such a Hamiltonian is
\begin{equation}
H = \sum_i^N \frac{1}{2}m_i \boldsymbol{v}_i^2 - \sum_i^N \sum_{j>i}^N \frac{G m_i m_j}{r_{ij}} = T - U \ ,
\end{equation}
where $T$ is the kinetic energy and $U$ is the so-called force function, which is the negative of the potential energy. The binding energy $B$ of the system is thus \mbox{$B = -H = U-T$}. If we apply the time transformation
\begin{equation}
\mathrm{d}s = U \mathrm{d}t \ ,
\end{equation}
where $s$ is the new fictitious time variable, we can get a new Hamiltonian through the Poincaré time transform mentioned in section \ref{sect:2dregu}:
\begin{equation}
\Gamma = g (H+p_0) \ ,
\end{equation}
where $p_0$ is the binding energy and $g$ is the time transform function that is gotten from $\mathrm{d}t = g \mathrm{d}s$. 
Thus the time transformed Hamiltonian is
\begin{equation}
\Gamma = \frac{T-U+B}{U} \ . \label{equ:loghGamma}
\end{equation}
This Hamiltonian is not separable yet. However, because along the correct orbit $\Gamma = 0$ since the Hamiltonian equals the energy of the system, we can use
\begin{equation}
\tilde{\Gamma} = \mathrm{log}(1+\Gamma) \label{equ:loghNewGamma}
\end{equation}
as the new Hamiltonian \citep{mikkola:1999}. This is since if $\Gamma$ equals 0, the logarithm also gives us 0, so along the correct orbit $\tilde{\Gamma}$ equals the time transformed Hamiltonian.
We can turn equation \ref{equ:loghGamma} into the form 
\begin{equation}
1 + \Gamma = \frac{T+B}{U} \ ,
\end{equation}
take the logarithm of both sides, and combine it with \ref{equ:loghNewGamma} which gets us 
\begin{equation}
\tilde{\Gamma} = \mathrm{log}(T+B) - \mathrm{log}(U)
\end{equation}
which is in a separable form. Thus we have managed to form a time transformed Hamiltonian that is separable, and thus usable for an explicit symplectic integrator. 

\subsection{Time Transformed Leapfrog}
The second approach is called Time Transformed Leapfrog (TTL) method, developed by \cite{mikkola:2002}. This method relies on a time transformation
\begin{equation}
\mathrm{d}s = \Omega \mathrm{d}t \ ,
\end{equation}
where $\Omega$ is an arbitrary positive function of the coordinates, i.e. $\Omega = \Omega(\boldsymbol{r})$. We also introduce an auxiliary variable $\omega$ whose value is equal to $\Omega$ at the start, but its new values are obtained from the differential equation
\begin{equation}
\frac{\mathrm{d}\omega}{\mathrm{d}t} = \boldsymbol{v} \cdot \frac{\partial \Omega}{\partial \boldsymbol{r}} \ .
\end{equation}
This allows us to form new time transformed equations of motion with these new variables.
%The variable $\omega$ is considered to be in the same category as the velocities $\boldymbol{v}$.

The use of these variables $\Omega$ and $\omega$ ensures that encounters of the least massive particles of the system have a non-negligible effect on the time transform, even if their contribution to the potential energy might be small. Based on \cite{mikkola:2008b}, one good definition for $\Omega$ in an N-body system is 
\begin{equation}
\begin{aligned}
\Omega &= \sum_i^N \sum_{j>i}^N \frac{G \Omega_{ij}}{r_{ij}} \\
\Omega_{ij} &=
\begin{cases}
\tilde{m}^2, &\mathrm{if} \ m_im_j < \epsilon_{\Omega} \tilde{m} \\
0  &\mathrm{otherwise}
\end{cases} \\
\tilde{m} &= \sum_i^N \sum_{j>i}^N \frac{2m_i m_j}{N(N-1)} \ .
\end{aligned}
\end{equation}
Here $\tilde{m}$ is the mean mass product of the system, and the mean mass epsilon $\epsilon_{\Omega}$ is defined by the user, and a typical value is of the order $\sim 10^{-3}$. Choosing the value for $\Omega$ in this manner guarantees the proper treatment for even the smaller bodies in the system.

These two aforementioned methods can be combined into a single time transform function defined as 
\begin{equation}
ds = [\alpha U + \beta \Omega + \gamma] dt = [\alpha (T + B) + \beta \omega + \gamma] dt \ .
\end{equation}
The two definitions are identical for the exact solution. Here $\alpha$, $\beta$, and $\gamma$ are user-defined parameters which determine which kind of regularization will be used. One can also easily see that if we define $\Omega = U$, the logarithmic Hamiltonian and the time transformed leapfrog methods are mathematically equivalent \citep{mikkola:2008b}. Numerically they are not the same however, due to roundoff errors in updating the value of $\Omega$. 

Choosing $(\alpha, \beta, \gamma) = (1,0,0)$ gives us the logarithmic Hamiltonian method, $(\alpha, \beta, \gamma) = (0,1,0)$ gives the time transformed leapfrog, and $(\alpha, \beta, \gamma) = (0,0,1)$ includes no time transformation at all, and thus it corresponds to the regular leapfrog method. \cite{mikkola:2006} suggest a possible choice of $(\alpha, \beta, \gamma) = (1,\mathrm{small},0)$, which corresponds to a mix of LogH and TTL. This would be desirable for systems with very large particle mass ratios so that the smallest masses are not ignored when choosing the timestep. 

The time transformed equations of motion of an N-body system can now be written for both the coordinates
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha (T + B) + \beta \omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{r}_i}{\mathrm{d}s} &= t' \boldsymbol{v}_i
\end{aligned}
\end{equation}
and the velocities
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha U + \beta \Omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{v}_i}{\mathrm{d}s} &= t' \left[ \sum_{i \neq j}^N \frac{G m_j \boldsymbol{r}_{ij}}{r_{ij}^3} + \boldsymbol{f}_i \right] \\
\frac{\mathrm{d} \omega}{\mathrm{d}s} &= t' \sum_i^N \frac{\partial \Omega}{\partial \boldsymbol{r}_i} \cdot \boldsymbol{v}_i \ .
\end{aligned}
\end{equation}
%\frac{\mathrm{d} B}{\mathrm{d}s} &= -t' \sum_i^N m_i \boldsymbol{v}_i \cdot \boldsymbol{f}_i \\
Here $\boldsymbol{f}_i$ are any possible external perturbing accelerations that might affect the system.

\cite{mikkola:2008b} concluded that using the choice of $(\alpha, \beta, \gamma) = (1,0,0)$, i.e. pure logarithmic Hamiltonian method, gives the most accurate results for the integration. Despite this the time transformed leapfrog method and the variables $\Omega$ and $\omega$ are not useless, since monitoring these variables can help identify encounters of light particles, and provides the integrator a way to forcibly lower the timestep if needed.

By using a time transformation we do not experience any catastrophic errors in the case of close-by two-body encounters since the integration with the leapfrog is exact even on collision orbits, as long as the system is not evaluated during collisions \citep{mikkola:1999}. The algorithm conserves energy down to numerical precision, and the only error is a phase error for time, which scales proportionally to $\mathcal{O}(\Delta t^3)$. This is a spectacular result for a method where the collisional singularity is not actually removed.



\section{Chain coordinate system}

Computers use floating-point arithmetic to represent real numbers, so rounding errors are inevitable when doing calculations with such numbers. Rounding errors are especially prevalent when subtracting two very large numbers whose values are very close to each other, for example a close encounter of a pair of particles if their coordinates are measured from a distant origin. Reducing this roundoff error as much as possible is important in order to ensure that calculations and thus the simulations are correct. Reducing this error can be achieved via introducing a new chained coordinate system, where the position of a particle is defined by its relative position to a previous particle in the chain.

This method is similar to the earlier described Kustaanheimo-Stiefel chain regularization method \citep{mikkola:1993}, where the system is not globally KS regularized. Instead we do the KS transform only for the shortest interparticle vectors in the system. The difference is that in AR-CHAIN we do not do a KS transformation, instead we only link all the particles together in a chain. \cite{mikkola:1999} pointed out that without using this kind of chain structure, the numerical roundoff when calculating the forces is too large for satisfactory results. Thanks to this structure, the separations of particles that are near each other can be derived from the chain, so we do not need to calculate them by doing the error-prone subtraction.
It should be noted that the chain transformation is just a linear mapping, so doing it does not require us to alter the Hamiltonian of the system. This allows the chain structure to be directly used with the transformations shown in the previous section.

Before constructing the chained system we need to have the system in a center of mass (CoM) frame. The position and velocity of the CoM of the dynamical system are defined as
\begin{equation}
\begin{aligned}
\boldsymbol{r}_{cm} &= \frac{\sum_\mathrm{i}^\mathrm{N} m_i \boldsymbol{r}_i}{\sum m_i} \\
\boldsymbol{v}_{cm} &= \frac{\sum_\mathrm{i}^\mathrm{N} m_i \boldsymbol{v}_i}{\sum m_i} \ .
\end{aligned}
\end{equation}

The construction of the chain is quite straightforward, and it starts by first calculating all the interparticle distances in the CoM frame. These are pairwise symmetric, so if we have $N$ particles, we only need to calculate $N(N-1)/2$ vectors instead of $N^2$. We pick the shortest distance, and the two particles are picked as the ``head'' and ``tail'' of the chain. The chain is extended by adding to it the particle that is closest to either the head or tail of the chain, if it is not already a member of the chain. This added particle then becomes the new head or tail of the chain. This procedure is continued until no suitable particles remain outside of the chain. In figure \ref{fig:chain} one can see an example of a chained system consisting of 11 particles.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=1]{../images/chain.pdf}
\caption{An illustration of a chained coordinate system demonstrating the 10 interparticle position vectors between 10 stars and a supermassive black hole. As the closest interparticle distance in this system is between the black hole and the nearby star, the construction of the chain starts from there and then extends from both the head and tail until complete. The dashed line shows the shortest path between the two stars in the CoM frame, which is very different compared to the path when following the chain.}
\label{fig:chain}
\end{figure}

A system of $N$ particles will contain $N-1$ chained interparticle vectors. These chain coordinates $\boldsymbol{X}_k$ can be collected into the vector $\boldsymbol{X}$ so that
\begin{equation}
\begin{aligned}
\boldsymbol{X}_k &= \boldsymbol{r}_{j_k} - \boldsymbol{r}_{i_k} \\
\boldsymbol{X} &= (\boldsymbol{X}_1, \boldsymbol{X}_2, \ldots, \boldsymbol{X}_{N-1}) \ .
\end{aligned}
\end{equation}
Similarly for velocities
\begin{equation}
\begin{aligned}
\boldsymbol{V}_k &= \boldsymbol{v}_{j_k} - \boldsymbol{v}_{i_k} \\
\boldsymbol{V} &= (\boldsymbol{V}_1, \boldsymbol{V}_2, \ldots, \boldsymbol{V}_{N-1})
\end{aligned}
\end{equation}
and accelerations
\begin{equation}
\begin{aligned}
\boldsymbol{A}_k &= \boldsymbol{a}_{j_k} - \boldsymbol{a}_{i_k} \\
\boldsymbol{A} &= (\boldsymbol{A}_1, \boldsymbol{A}_2, \ldots, \boldsymbol{A}_{N-1}) \ .
\end{aligned}
\end{equation}
Here the vector labelling starts from the head of the chain. These chain vectors are used in the force calculations to compute the separations $\boldsymbol{r}_{ij}$ for particles that are near each other in the chain. For particles that are far away from each other in the chain structure, the separation is actually more accurately calculated by using for example the CoM frame, due to the accumulating error of summing many separation vectors. The separation is calculated using the chain if the difference in the chain indexes is not greater than the arbitrarily chosen limit $N_d$. If we have chain indexes $i$ and $j$, and we assume that $j > i$, the separations in the AR-CHAIN algorithm are calculated with
\begin{equation}
\boldsymbol{r}_{ij} =
\begin{cases}
\boldsymbol{r}_j - \boldsymbol{r}_i \ &\mathrm{if} \ j-i > N_d \\
\sum_{k=i}^j \boldsymbol{X}_k \ &\mathrm{if} \ j-i \leq N_d \ .
\end{cases}
\end{equation}
According to \cite{mikkola:2008b} a good value for this is $N_d = 2$. This method of selecting the way the separations of given particles is calculated is one of the most important features of this algorithm.

Finally we can write down the leapfrog suitable equations of motion for the chained system \citep{mikkola:2008b}. For the coordinates the equations are
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha (T + B) + \beta \omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{X}_i}{\mathrm{d}s} &= t' \boldsymbol{V}_i \ ,
\end{aligned}
\end{equation}
and for the velocities they are
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha U + \beta \Omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{V}_i}{\mathrm{d}s} &= t' [ \boldsymbol{A}_i(\{ \boldsymbol{X}_i \}) + \boldsymbol{f}_i ] \\
\frac{\mathrm{d} \omega}{\mathrm{d}s} &= t' \sum_i^N \frac{\partial \Omega}{\partial \boldsymbol{X}_i} \cdot \boldsymbol{V}_i \ .
\end{aligned}
\end{equation}

\section{Gragg–Bulirsch–Stoer algorithm}

\section{AR-CHAIN code overview}

The standalone version of AR-CHAIN code first prepares the test scenario we want to examine. It starts by simply setting values for the parameters of the chain to the defaults, and reserving memory for how many particles we want to simulate and setting the physical properties for all the particles. Here we also set the settings for what kind, if any, PN corrections we want to use. We can set the maximum PN order, whether to use PN cross terms or spin terms, and whether PN corrections should be taken into account only with BH-BH interactions, or also with BH-star interactions. We also determine the desired error tolerance for the BS algorithm, which affects how accurate the integration will be.

After this the chain is initialized. We build the chain structure, and calculate all system dependent variables required for propagation. First we make sure that the initial data is in CoM frame. The CoM mass is a sum of all the masses in the chain. The CoM positions, velocities, and auxiliary velocities are the weighted sums of all the particles' properties. Finally these are normalised by the total CoM mass. Then the CoM properties are substracted from all the particles' properties, ensuring they are in the CoM frame.

Next we build the chain structure, which also updates the chain variables.
All pairwise distances between chain members are calculated. These are pairwise symmetric, so instead of $n^2$ values, we only need $n(n-1)/2$. When the distances have been found, they are sorted by length and we pick the shortest one as the starting point, and add the two points to the chain. The chain is then extended from both ends (``head'' and ``tail''), by adding the particle within the shortest distance to either, unless the particle to be added is already in the chain. A distance added to the chain is removed from the list of distances, and the process is continued until no valid distances remain.
Finally all the members are assigned their index in the chain, and the particles are always kept in the chain index order in memory. The chained positions and velocities for all the particles are calculated from the CoM positions and velocities. 

After building the chain, we calculate the rest of the variables of the chain structure, such as the binding energy. The binding energy requires calculating the total energy of the chain, which is simply the combination of the kinetic energy and the potential energy. Kinetic energy is simply the sum over all the particles, while potential energy is the sum of potential energies between every particle to every other particle.
%A softening length is added to all the distances when calculating.
Finally we calculate the potential energies and accelerations for all the particles in the chain.

After all this preparation is done, we can start propagating the chain. The system will be propagated for a given time, and it will print out all the particle data after every given printing interval, both determined from command line. First we estimate an appropriate initial timestep of the fictious time using only the initial variables, with no external information. This is done with a routine where the smallest timestep between all the pairs of particles is chosen.

Next we get to the main loop of the code. The system is integrated until the target time is met. This starts with printing the data into a file. After that we check for potential mergers between a black hole and other particles in the chain. This is done by looking through all the pairwise distances and merging particles that are too close. If mergers do happen, the chain needs to be initialized again.

The actual propagation is done via the BS algorithm. We start by estimating the optimal number for the maximum amount of substep divisions. The amount of substeps always starts at 2, and is incremented by 2 should the division not be sufficient. After this we start taking BS substeps using leapfrog.
The original chain is copied and this copy is then propagated so that we are able to do the propagations but still finally update the original chain once we have the desired propagation result. The propagation is done as drift-kick-drift. The first drift is with half a timestep, then kicks and drifts are done with full timesteps for number of steps minus one amount, and then in the end kick with a full timestep and drift with a half timestep.

The drift part is quite simple. First the kinetic energy is calculated, which is used to calculate the time transform for adjusting the timestep. The physical time of the the chain is incremented and the new positions for all the particles are calculated. The CoM position is calculated as well. The chain and CoM coordinates are not in sync anymore.

The kick phase is more complicated. It starts with recalculating the time transform variable omega, potential energy, and Keplerian accelerations for all the members. The potential energy and omega are used in calculating the time transform. Due to the post-Newtonian terms the accelerations are affected by the velocities, so the normal formulation of a leapfrog does not work. This can be circumvented using auxiliary velocities $\mathbf{w}$ and chained auxiliary velocities $\mathbf{W}$. When the physical velocities are being perturbed, the auxiliary ones are used in the calculations, and vice versa. 

First we calculate the velocity independent accelerations for all the particles, which includes the effects of faraway perturbing particles. Then we calculate velocity dependant accelerations by doing an auxiliary kick for half a timestep, a physical kick for a full timestep, and another half timestep auxiliary kick. The velocity dependant accelerations are obviously only used if we are using PN corrections, since non-relativistic accelerations do not depend on velocities.

The kicks need the CoM to be in sync with the chain, so they are synced first. 
First we calculate the BH-BH interactions, and then the BH-star interactions.
If sufficient PN terms are used, the system loses energy through gravitational radiation. Binding energy and radiated energy are updated only when updating the physical variables. For both physical and auxiliary cases, the velocities and spins are updated for all the particles and the CoM system. After this the chain and the CoM are again not in sync.
After the leapfrog algorithm is done, the chain and CoM systems are synced once more.

After taking a step with leapfrog we do the extrapolation. Rational or polynomial extrapolation is used.
%TODO puhu rat ja poly jutuist yleisessä selityksessä?
The extrapolation algorithm calculates new values for the positions and velocities of the particles. Next we calculate the maximum error of these newly integrated values, and then the error is scaled relative to the set BS tolerance. If the error is small enough, the integration has been successful. If the error is deemed too large, we increase the number of substeps and repeat the whole process of leapfrog and extrapolation again.

If the maximum number of substeps is reached and the error is still too big, then the timestep itself is reduced, and the same procedure of taking incrementally more timesteps is repeated. This is repeated until the error is small enough to satisfy the tolerance level.

Once the desired error level is reached, we check if the optimal maximum amount of substep divisions was appropriate or if it should be adjusted. If the timestep was reduced during the integration, the optimal division amount will not be adjusted. After all this the new integrated values for all the values of the particles are updated into the chain.

After the integration is done, we check if the chain structure needs to be updated. The chain is stored in the order of indexing, so we want to keep the physically nearby particles close to each other in the chain structure as well. We must check if there are any pairwise distances in the system that are smaller than the smallest chained distance. If that is the case, the chain structure needs to be reconstructed.

Finally, after the system has been integrated for at least the target time, we check if the integration passed the target time by too much. Since the timestep varies, we cannot be certain that the end time for the integration was exactly the target time. If the time is too much past the target, the system is integrated backwards to be closer to the target time. This uses the same BS algorithm as normal.
Once this backwards iteration is done the final values of the chain are printed and the program is finished.


%The code uses post-Newtonian corrections to take into account and approximate the relativistic effects near the black hole particles. The corrections are represented by additional terms in the relative accelerations of the two bodies, so that
%\begin{equation}
%\boldsymbol{a}_\mathrm{2-body} = \boldsymbol{a}_\mathrm{Newtonian} + \sum_{k=2}^7 c^{-k} \boldsymbol{a}_\mathrm{(k/2)PN} + \boldsymbol{a}_S \ ,
%\end{equation}
%where $\boldsymbol{a}_\mathrm{Newtonian}$ is the usual Newtonian two-body acceleration, $c$ is the speed of light, $\boldsymbol{a}_\mathrm{xPN}$ is the PN correction of order $x$, and $\boldsymbol{a}_S$ indicates PN terms depending on the spins of the particles. PN corrections up to order PN3.5 are included in the code.
%
%For spinning bodies, additional PN corrections are required. The PN contribution to the equations of motion for the spins is given by
%\begin{equation}
%\boldsymbol{\dot{S}}_i = \boldsymbol{S}_{\mathrm{PN},i} \times \boldsymbol{S}_i \ ,
%\end{equation}
%where $\boldsymbol{S}_i$ is the spin angular momentum of the particle $i$ and $\boldsymbol{S}_{\mathrm{PN},i}$ gives the effect of the spin-orbit, spin-spin, and quadrupole-monopole interactions. %TODO Selitä näitä pls
%
%These two-body PN corrections are only used when at least one of the bodies is a black hole, since for other particles they're not of any significance. Only when a black hole is involved are the velocities and masses large enough to require relativistic treatment.

%\section{Bulirsch–Stoer algorithm}
%\section{Time-transformed leapfrog}
%\section{Collisions}

\chapter{AR-CHAIN results}
\section{OJ287}

These following runs all had similar initial conditions, apart from the magnitude of the spin. The initial values for the orbital elements were those measured by Valtonen \& Mikkola (cite), with the semimajor axis of the binary being around 11350 AU, the eccentricity being 0.6, and spin being 0.28. Simulations were also run with a spin of 0 and a spin three times the original. The direction of the spin was towards the positive z-axis of the system. The inclination of the orbit was 45 degrees in the xz-plane. So the spin was inclined 45 degrees to the initial orbit. The initial timestep in the simulations was 0.01 gadget times, and the interval for writing the data in a file was 1 gadget time. 

The orbital elements precessed, so average values were used to make smoother and clearer graphs. 

The eccentricity of the orbit goes down as the orbit circularizes due to the GW emissions.

The PN2.5 term is responsible for the GW emissions.

The spin caused the orbit to twist. This can be seen clearly in images \ref{fig:spin0Orbits} and \ref{fig:spinNormalOrbits}. If there was no spin, the orbit stayed neatly in the original plane, but with spin the orbit started turning.

One should keep in mind that this model is very simplistic. There are only the two black holes, and no other stars to interact with, and no accretion disks, which are thought to play a relatively large role in the life of the system.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/spinNormal.pdf}
\caption{The development of the semimajor axis and eccentricity of the smaller of the two black holes in the binary, with it having a spin of 0.28. The merging happened at around 206800 gadget time, corresponding to 32300 years.}
\label{fig:spinNormal}
\end{figure}

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/spin0.pdf}
\caption{The development of the semimajor axis and eccentricity of the smaller of the two black holes in the binary, with it having no spin at all. The merging happened at around 202300 gadget time, corresponding to 31700 years.}
\label{fig:spin0}
\end{figure}

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/spin3.pdf}
\caption{The development of the semimajor axis and eccentricity of the smaller of the two black holes in the binary, with it having a spin of 0.84, three times the real value. The merging happened at around 214400 gadget time, corresponding to 33500 years.}
\label{fig:spin3}
\end{figure}

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/spin0Orbits.png}
\caption{Plot of the the orbits of the binary if there is no spin. The orbiting member doesn't leave the original plane it was in.}
\label{fig:spin0Orbits}
\end{figure}

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/spinNormalOrbits.png}
\caption{Plot of the the orbits of the binary with the measured spin.}
\label{fig:spinNormalOrbits}
\end{figure}

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/noPN.pdf}
\caption{The development of the semimajor axis and eccentricity of the smaller of the two black holes in the binary, if the post-Newtonian terms aren't factored in. The simulation was run for slightly over 1500 years. Since the PN terms were not taken into account the orbit didn't actually decay, just as expected. The results themselves aren't very interesting, but it shows that the corrections are indeed needed for realistic results.}
\label{fig:noPN}
\end{figure}

\subsection{Estimated time of merging}
\subsection{Differences from reality}
\section{Effects of spin}

%\chapter{KETJU and results}
%\section{KETJU overview}
%\section{}
%\section{}

\chapter{Conclusions}

% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\clearpage
\addcontentsline{toc}{chapter}{Bibliography} % This lines adds the bibliography to the ToC
\bibliographystyle{plainnat}
\small
\bibliography{lahteet}


\end{document}

