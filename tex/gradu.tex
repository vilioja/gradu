% STEP 1: Choose oneside or twoside. Use the 'draft' option a lot when writing.
\documentclass[english, oneside]{HYgradu}

\usepackage[utf8]{inputenc} % For UTF8 support. Use UTF8 when saving your file.
\usepackage{lmodern} % Font package
\usepackage{textcomp}
\usepackage[pdftex]{color, graphicx} % For pdf output and jpg/png graphics
\usepackage[pdftex, plainpages=false]{hyperref} % For hyperlinks and pdf metadata
\usepackage{fancyhdr} % For nicer page headers
%\usepackage{tikz} % For making vector graphics (hard to learn but powerful)
%\usepackage{wrapfig} % For nice text-wrapping figures (use at own discretion)
\usepackage{amsmath, amssymb} % For better math
\usepackage[colon]{natbib} % For bibliography
\usepackage[footnotesize,bf]{caption} % For more control over figure captions
\usepackage{multirow, makecell}
\usepackage{refcount}

\makeatletter
\newcommand{\thickhline}{%
    \noalign {\ifnum 0=`}\fi \hrule height 1.5pt
    \futurelet \reserved@a \@xhline
}
\newcolumntype{"}{@{\hskip\tabcolsep\vrule width 1.5pt\hskip\tabcolsep}}
\makeatother

\renewcommand{\topfraction}{.75}
\renewcommand{\floatpagefraction}{.75}

\fussy % Probably not needed but you never know...

% OPTIONAL STEP: Set up properties and metadata for the pdf file that pdfLaTeX makes.
% But you don't really need to do this unless you want to.
\hypersetup{
    bookmarks=true,         % show bookmarks bar first?
    unicode=true,           % to show non-Latin characters in Acrobat’s bookmarks
    pdftoolbar=true,        % show Acrobat’s toolbar?
    pdfmenubar=true,        % show Acrobat’s menu?
    pdffitwindow=false,     % window fit to page when opened
    pdfstartview={FitH},    % fits the width of the page to the window
    pdftitle={},            % title
    pdfauthor={},           % author
    pdfsubject={},          % subject of the document
    pdfcreator={},          % creator of the document
    pdfproducer={pdfLaTeX}, % producer of the document
    pdfkeywords={something} {something else}, % list of keywords for
    pdfnewwindow=true,      % links in new window
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=black,        % color of internal links
    citecolor=black,        % color of links to bibliography
    filecolor=magenta,      % color of file links
    urlcolor=cyan           % color of external links
}

% STEP 2:
% Set up all the information for the title page and the abstract form.
% Replace parameters with your information.
\title{Simulating the dynamics of supermassive black holes}
\author{Vili Oskari Oja}
\date{\today}
\level{Master's thesis}
\faculty{Faculty of Science}
\department{Department of Physics}
\address{PL 64 (Gustaf Hällströmin katu 2)\\00014 Helsingin yliopisto\\Finland}
\subject{Astronomy}
\prof{Professor Peter Johansson}{Dr. Pauli Pihajoki}
\censors{Professor Peter Johansson}{Dr. Pauli Pihajoki}{}
\depositeplace{}
\additionalinformation{}
\numberofpagesinformation{\getpagerefnumber{pagecount} pages}
\classification{}
\keywords{\scriptsize{galaxies: supermassive black holes - galaxies: kinematics and dynamics - galaxies: nuclei - methods: N-body simulations}}
\quoting{}

\begin{document}
\setlength{\parindent}{1cm}
\setlength{\parskip}{0cm}
% Generate title page.
\maketitle

% STEP 3:
% Write your abstract (of course you really do this last).
% You can make several abstract pages (if you want it in different languages),
% but you should also then redefine some of the above parameters in the proper
% language as well, in between the abstract definitions.
\begin{abstract}
Abstract goes here.
\end{abstract}

% Place ToC
\mytableofcontents


% -----------------------------------------------------------------------------------
% STEP 4: Write the thesis.
% Your actual text starts here. You shouldn't mess with the code above the line except
% to change the parameters. Removing the abstract and ToC commands will mess up stuff.
\setlength{\parindent}{.75cm}
\setlength{\parskip}{.6cm}
\chapter{Introduction}


\section{Brief history of black holes}

Back in 1784 the English natural philosopher and clergyman John Michell proposed the idea of a body so massive that even light could not escape from it \citep{michell:1784}. He assumed a body with the same density as the Sun, but 500 times the diameter and a surface escape velocity exceeding the speed of light. He also noted that such a body might be noticeable through its gravitational effects on nearby bodies. However when the wavelike nature of light became known, it was unclear at the time if gravity would have any effect on escaping light waves.

In 1915 Albert \cite{einstein:1915} developed his theory of general relativity, and soon after Karl \cite{schwarzschild:1916} found a solution to the Einstein field equations, which describes the gravitational field of a point mass and a spherical mass. This Schwarzschild solution describes a stationary, non-rotating, uncharged black hole. Another solution was found in the following years, for a black hole that could also have electric charge \citep{reissner:1916, nordstrom:1918}.

So far these were all theories, and it was unsure whether such phenomena could appear in nature. However, in 1931 Subrahmanyan \cite{chandrasekhar:1931} calculated with special relativity that a non-rotating body of electron-degenerate matter, e.g. a white dwarf, above a certain mass limit has no stable solutions, and will collapse under its own gravity. However this will not necessarily form a black hole, as the white dwarf can collapse into a neutron star. In 1939 Robert Oppenheimer and others predicted a maximum mass limit for neutron stars as well, above which they would collapse further, and concluded that no physical law was likely to stop this collapse into a black hole \citep{oppenheimer:1939}.

Neutron stars had never been observed however, so it was not known if they and by extension black holes could exist in nature. In 1967 Jocelyn Bell and the team she was working with discovered the first radio pulsars \citep{hewish-bell:1968}, which were soon shown to be rapidly rotating neutron stars. Since neutron stars were confirmed to exist, black holes gained strong support for their existence as well. Around this time new solutions to the Einstein field equations were also found in addition to the Schwarzschild solution. In 1963 Roy \cite{kerr:1963} discovered an exact solution for an uncharged, rotating black hole, and a few years later it was generalized for a charged black hole as well \citep{newman:1965}.

The origin for the term ``black hole'' is not certain \citep{blackholeorigins}. Initially black holes were referred to simply as gravitationally completely collapsed objects. The new term made a famous and popular appearance during a lecture by the American physicist John Wheeler in 1967. However, the term was already used several years earlier at the beginning of 1964 by a \textit{Science News Letter} reporter Ann Ewing, who wrote a report on a meeting of the American Association for the Advancement of Science. She did not come up with the term however, and the term has been traced even further back to physicist Robert Dicke, who in the early 1960s reportedly compared the astrophysical phenomenon to the Black Hole of Calcutta, an infamous prison where only few people ever left alive.

\section{From theory to reality}

As black holes inherently do not emit any strong optically visible signals, detecting them has always been a challenge. The best way to observe them has been to observe the effects the black holes have on their surroundings. In 1972 scientists studying the X-ray source Cygnus X-1 speculated that the source might be a star accreting matter onto a black hole, but the evidence was not completely clear \citep{bolton:1972, webster:1972}.

Since 1995 astronomers have observed and tracked the motions of stars surrounding the central radio source Sagittarius A* in our Milky Way. By 1998 it was determined that there must be at least 2.6 million solar mass object within a radius of 0.02 light years to explain the orbits of the stars \citep{ghez:1998}. By 2009 this approximation was refined to be around 4.3 million solar masses inside a radius of less than 0.002 light years \citep{gillessen:2009}. This observation is hard to explain with anything else than a black hole.

Another idea to observe black holes was to measure the gravitational waves they emit as predicted by Einstein's theory. The Laser Interferometer Gravitational-Wave Observatory (LIGO) was built for this purpose. In September of 2015 the first ever direct observation of gravitational waves was made, the signal from the merger of around 36 and 29 solar mass black holes \citep{abbott:2016}. This observation provided the most concrete evidence for the existence of black holes to date.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/blackhole.jpg}
\caption{The first ever direct visual evidence of a black hole, the supermassive black hole in the centre of Messier 87 and its shadow. Visible are the emission ring and the central shadow of the black hole. The brighter lower half is due to the black hole's rotation and relativistic beaming. Image by the EHT collaboration.}
\label{fig:BlackHole}
\end{figure}

Though the existence of black holes was confirmed, visually there were only simulated results of what a black hole and its surrounding regions would look like, until this year. The Event Horizon Telescope (EHT) is an ongoing collaboration that uses many telescopes on Earth in the effort to observe the event horizons of supermassive black holes, especially the black holes in the center of the Milky Way and Messier 87 (M87). EHT began observations on the black hole in the center of M87 in 2017, and after 2 years of data processing, in April of 2019, the first ever direct visual evidence of a black hole was released \citep{akiyama:2019}, shown in figure \ref{fig:BlackHole}, proving the predictions made over a hundred years ago to be correct.

\section{Aim of this thesis}

Nowadays it is widely accepted that supermassive black holes play an important role in the evolution of galaxies, especially by affecting their central regions during galaxy mergers. Supermassive black holes are also behind some of the most energetic phenomena in the universe, being the central engines for active galactic nuclei. In addition gravitational waves emitted by merging black holes can give us a unique method of observing the far reaches of the universe. Studying how supermassive black holes form and evolve is crucial for understanding these mechanisms.

In this thesis I will describe the basic properties of black holes and how different types of black holes are formed. I will also go through the methods needed to be able to accurately simulate black holes to small separations, and I will describe some of the simulation tools used for this at the Theoretical Extragalactic Research Group at the University of Helsinki. The aim of this thesis is to study the effects of post-Newtonian corrections in simulating black hole mergers, and to study how three-body interactions with stars affect the evolution of a supermassive black hole binary. 

This thesis is structured as follows. In Chapter 2 I will review the properties of black holes, how black holes of different masses are believed to form, and how a black hole binary evolves during a galaxy merger. I will also go through the regularization and post-Newtonian approximation methods used in the simulation code. In Chapter 3 I will explain the algorithmic regularization chain, or AR-CHAIN, method used for my simulations. In Chapter 4 I will show the simulations I have run and discuss the results. Chapter 5 contains a summary and concluding remarks for this thesis.

\chapter{Dynamical modelling of black holes}

\section{Black hole properties}

A black hole (BH) is defined as a region of spacetime in which the gravitational field is so strong that no objects or signals that carry information can escape from it. Black holes have the interesting property that they are completely determined by the their mass M, angular momentum J, and electric charge Q. This is known as the black hole uniqueness theorem or no-hair theorem, which states that all physical black hole solutions are completely characterized by the above-mentioned three parameters, and must satisfy the condition 
\begin{equation} \label{equ:bhunique}
M^2 - \left( \frac{J}{M} \right)^2 - Q^2 \geq 0 \ ,
\end{equation}
where we set $G = c = 1$ \citep{mazur:2001}.
Thus only by changing these variables can the properties of the black hole change. The physical reasoning for this uniqueness theorem is that the matter beyond the event horizon of a black hole cannot directly affect anything outside of it. Thus only the globally conserved characteristics, such as mass and angular momentum, survive and can be measured from the outside. The electric charge of black holes that appear in nature is most likely very near zero, since having a non-zero charge would require a very large imbalance in the amounts of protons and electrons that enter into the black hole, which is not a realistic scenario with normal matter.

Black holes can be divided into four distinct types based on these properties. Every black hole has mass, but the angular momentum and electric charge are not necessary.
\begin{table}[htb]
\centering
\caption{Different types of black holes}
\begin{tabular}{|c"c|c|}
\hline
 & Non-rotating ($J = 0$) & Rotating ($J \neq 0$) \\ \thickhline
Uncharged ($Q = 0$) & Schwarzschild & Kerr \\ \hline
Charged ($Q \neq 0$) & Reissner–Nordström & Kerr–Newman \\ \hline
\end{tabular}
\end{table}

The simplest type of black hole is created when an object of mass M becomes smaller than the radius
\begin{equation}
r_S = \frac{2GM}{c^2} \ .
\end{equation}
A black hole can be born in nature when massive a star dies and explodes in a supernova. As will be discussed in more detail in section \ref{sect:stellarholes}, massive stars can fuse matter up to iron, and an iron core is formed in the center of the star near the end of its lifetime. Once this core is massive enough that the degeneracy pressure of electrons can no longer support its gravity, it will collapse into a neutron star. If the neutron star grows until the degeneracy pressure of neutrons cannot support it, it will collapse until it reaches the aforementioned radius, where it will form a black hole. The radius is called the Schwarzschild radius, so named after the German astronomer Karl Schwarzschild, who found an exact solution for the Einstein field equations \citep{schwarzschild:1916}. Einstein field equations are a set of 10 equations in Albert Einstein's general theory of relativity that describe the fundamental interaction between spacetime curvature and mass and energy \citep{einstein:1915}. The surface at this radius is called the event horizon. Event horizons are mathematical surfaces, and they do not form any sort of physical barrier. An observer can fall inside the event horizon without any problem. The event horizon simply marks the limit at which not even light can escape the gravitational pull of the black hole.

Solutions that do not satisfy the condition given by equation \eqref{equ:bhunique} do exist, but these solutions are not stationary, i.e.\ they evolve with time to a Kerr solution. In a stationary field the geometry does not change over time, but it can rotate, e.g. the Kerr solution. As will be discussed soon, these non-stationary solutions are not really physical. This gives an upper limit to the angular momentum that a physical uncharged black hole can have. This limit can be expressed as the specific angular momentum 
\begin{equation} \label{equ:angularmomentum}
a = \frac{J}{Mc}
\end{equation}
or as the dimensionless spin parameter
\begin{equation}
a_* = \frac{Jc}{GM^2} \ .
\end{equation}
The parameter can range from 0, meaning that the hole does not spin, to almost 1, meaning that it spins as fast as possible for a given mass \citep{middleton:2016}.

If a black hole would have a larger angular momentum than allowed by these constraints, i.e. $a > 1$, it would mean that it actually would not be a black hole at all, since the event horizon would disappear due to the extreme rotation. 
%This is made clear by looking at equation \eqref{equ:evenhorizons}, which describes the surfaces that event horizons occur at. If $\mu^2 < a^2$, the solutions for this equation are complex, which is said to mean that the actual event horizons disappear.
This would cause what is known as a naked singularity, meaning that a singularity that is normally contained within a black hole would be visible to an outside observer. The possible existence of such singularities in nature is uncertain but extremely unlikely. If they were to exist, they might cause fundamental problems for physics as we know it. We would be able to see matter condensed to infinite density, and we have no theories that can predict how spacetime works near such abnormalities. Normally this is not a problem, since they cannot be observed inside event horizons. However the cosmic censorship hypothesis suggests that naked singularities cannot be formed in nature from realistic initial conditions, which would avoid the problem altogether \citep{wald:1997}. An extremely rapid spin could prevent the collapse into a black hole thanks to the centrifugal forces. The object would have to lose enough of its angular momentum before it could collapse.

Astrophysical black holes that appear in nature are assumed to be most likely Kerr black holes. The normal matter that creates and feeds black holes contains roughly equal amounts of protons and electrons, so the overall electric charge is mostly neutral. But stars do spin around themselves, thus possessing angular momentum. Because of the conservation of angular momentum, the stellar remnant will spin around itself as well. Black holes can also gain angular momentum through accreting matter falling into them. Gas coming close to a black hole will form an accretion disk around it. For the matter to be able to reach the black hole and fall into it, it must somehow lose some of its angular momentum. The total angular momentum of the disk must be conserved however, so the momentum has to move outwards in the disk.
The main source for this is believed to be magneto-rotational instability. The matter in an accretion disk can be considered a magnetized, rotating, and perfectly conducting fluid. In this kind of environment magnetic forces can cause torques on the matter, decreasing the angular momentum of matter closer to the black hole and increasing it in the matter farther away. These instabilities will effectively drive angular momentum outwards in the disk.
Eventually the matter will fall within the event horizon, conferring its mass and remaining angular momentum to the black hole, speeding up the rotation of the black hole \citep{bhphysics}. Black holes are also capable of losing their angular momentum. One way for this to happen is through the emission of jets. Jets are beams of ionized matter that are emitted from near black holes and they can be accelerated close to the speed of light. One suggested method for the formation of jets is through magnetic fields in the accretion disk being dragged and twisted by the spinning black hole.

\subsection{The structure of a rotating black hole}

While Schwarzschild black holes have only the one event horizon and a point-like singularity in the middle, spinning black holes are far more complicated. In general relativity gravity is a consequence of the curvature of spacetime, and this curvature causes an effect that is known as de Sitter precession or the geodetic effect. The curvature of spacetime causes the orbits of objects around a mass to precess slightly. This is true for all masses. In addition to this, spinning masses cause an additional effect called Lense-Thirring precession or frame-dragging. The massive object drags the surrounding spacetime with it as it spins, causing nearby particles and photons to rotate around the massive object as well, even if they did not have any angular momentum of their own to begin with. Both of these effects have been experimentally confirmed, for example by the Gravity Probe B mission. It measured the slight changes in the spin of gyroscopes aboard a satellite orbiting Earth, and confirmed that precession does indeed occur from both sources. The geodetic effect near Earth is around 170 times larger, so it can be measured more accurately \citep{everitt:2009}.

Thus the frame-dragging effect is significant only around very massive objects, like black holes that are rotating rapidly. At certain point this effect becomes so strong that any object, including light, \textit{must} spin along the rotation of the black hole. This limit is known as the stationary limit surface, and it is described in equation \eqref{equ:statlimsurf}. The area inside this limit is called the ergosphere, so named after the Greek word for work. The ergosphere differs from the insides of an event horizon because particles that fall into it can still escape from this region. If a particle escapes from the ergosphere, it converts some of the black hole's rotational energy into its own momentum \citep{grintro}. This lowers the angular momentum of the black hole, and the process could theoretically over time cause a rotating black hole to spin arbitrarily slow.

%This is called the Penrose process, and it is a possible explanation for some highly energetic astrophysical phenomena, such as gamma ray bursts. 
%TODO tarkenna tätä kappaletta

The Kerr metric is often presented in Boyer-Lindquist coordinates $(t, r, \theta, \phi)$ which are similar to spherical polar coordinates and are related to the Cartesian x,y,z-coordinates via the following transformation:
\begin{align*}
x &= \sqrt{r^2 + a^2} \mathrm{sin}\theta \mathrm{cos}\phi \\
y &= \sqrt{r^2 + a^2} \mathrm{sin}\theta \mathrm{sin}\phi \\
z &= r \mathrm{cos}\theta
\end{align*}
The variable $a$ is the same specific angular momentum as in the equation \eqref{equ:angularmomentum}. The only true singularity (i.e. a singularity that exists no matter what coordinate system we choose) in the Kerr metric occurs when $r=0$ and $\theta = \pi/2$ \citep{grintro}. It is easy to see that in Cartesian coordinates this corresponds to a flat ring in the equatorial plane with the radius of $a$. So rotating black holes do not have a point singularity, but a ring singularity instead.

In addition there exists coordinate singularities (i.e. singularities we can get rid of if we choose our coordinates differently) as well. These singularities occur on the surfaces
\begin{equation} \label{equ:evenhorizons}
r_\pm = \mu \pm \sqrt{\mu^2 - a^2}
\end{equation}
which describe the event horizons of the black hole, and on the surfaces
\begin{equation} \label{equ:statlimsurf}
r_{S \pm} = \mu \pm \sqrt{\mu^2 - a^2 \mathrm{cos}^2 \theta}
\end{equation}
which describe the stationary limit surfaces of the black hole. Here $\mu = \frac{GM}{c^2}$. 

In figure \ref{fig:KerrHole} one can see the different surfaces of a spinning black hole and their approximate shapes as seen in the Boyer-Lindquist coordinates. The surfaces mostly resemble axisymmetric ellipsoids, flattened along the rotation axis. Although the outer limit of the ergosphere changes its shape as the spinning gets faster, resembling a pumpkin-shape at nearly maximal spins. It however always touches the outer event horizon at the poles. The inner horizon and stationary limit surface also touch at the poles. On the equatorial plane the inner stationary limit surface also coincides with the ring singularity.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/kerrhole.pdf}
\caption{Diagram showing the structure of a Kerr black hole. It shows the inner and outer ergospheres and event horizons, and the ring singularity in the middle. This kind of black hole has non-zero spin. If the spin was 0 we would have a Schwarzschild black hole instead.
(Figure adapted from \citealt{grintro})}
\label{fig:KerrHole}
\end{figure}

%TODO

%Kahdenlaista frame draggingia Lense-Thirring efekti pyörivillä

%mainitse Eddingtonin luminositeetti yms jutut? Muuttaa valtavasti massaa energiaksi.

%mainitse miten voi menettää massaa?
\newpage
\section{Different types of black holes}

Mass is the only quantity that all black holes have for certain, so it makes sense to classify them by their mass. The black holes are usually put into three different categories. The reason for this kind of distinction is that the different mass black holes are found in very different kinds of environments and have different formation mechanisms. The black holes are also detected through the different kinds of phenomena that they cause.
The mass limits for the categories are not exact, but typically stellar mass black holes (SBH) are below $10^2 \ M_\odot$, intermediate mass black holes (IMBH) are between $10^2$ and $10^5 \ M_\odot$, and supermassive black holes (SMBH) are above $10^5 \ M_\odot$ \citep{bhphysics}.

%kerro eri kohissa et miten voi havaita, mitä eri ilmiöitä nää ehkä synnyttää
%kerro miten syntyvät, massarajoja, tähden kokonaismassa on eri kuin ytimen massa, joka kertoo voiko syntyä musta aukko

\subsection{Stellar mass black holes} \label{sect:stellarholes}

Stellar mass black holes are born from collapsing massive stars. During their active life, stars are in hydrostatic equilibrium due to the gravitational pull of the matter making up the star, and the outward thermal pressure caused by the nuclear reactions in the star's core. At the end of a massive star's lifespan, the hydrogen in the core will have fused into helium. This leads to hydrogen forming a shell around the helium core, and hydrogen fusion will continue there, which is known as shell burning. The helium formed in the shell will accrete into the core, causing the core to grow hotter and denser. Once the core is has grown enough, the helium will start to fuse as well. This same process will happen for heavier elements as well, until the star will have several shells of different elements, as illustrated in figure \ref{fig:FusionShells}.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/FusionShells.pdf}
\caption{A simplified cross-section of a massive, evolved star. As the star starts fusing heavier and heavier elements, new shells will form around the previous ones, until the core becomes iron, which cannot go through fusion without losing energy.
(\copyright \ User:Rursus / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:FusionShells}
\end{figure}

Iron and nickel have the highest binding energies of all the elements, meaning that lighter elements release energy through fusion, but reactions producing heavier elements require additional energy. After forming an iron core, the star cannot fuse any more matter in its core, and thus the supporting radiation pressure drops, causing the star to implode under its own gravity. Iron being the final product in stars causes what is known as the iron peak in the abundance of chemical elements. Iron is one of the most common metals in the universe, as seen in figure \ref{fig:IronPeak}. One can also see that certain other elements such as carbon, oxygen, neon, etc. are more abundant than other elements near their mass. This too is the result of stellar nucleosynthesis, where these elements are created through fusion with helium nuclei, which are known as alpha particles. Due to this, all of these so called alpha elements have a mass number that is a multiple of four.

What happens next depends on the mass of the star. For the star to be able to even reach the iron core stage, it needs to be initially over around 8 solar masses. Otherwise it does not have enough mass to fuse the heavier elements, and it will gradually expand its atmosphere into a planetary nebula, leaving behind a white dwarf, composed of electron-degenerate matter. In more massive stars, the iron core will exceed the Chandrasekhar limit of around 1.4 solar masses, which describes the upper limit for the mass of a stable white dwarf. Stars above this $\sim$8 solar mass threshold but below $\sim$20 solar masses explode in supernovae and leave behind neutron stars. If the initial mass of the star is higher than $\sim$20 solar masses, the stellar core is too massive to not collapse into a black hole \citep{woosley:2002}. The actual mass limit is the Tolman–Oppenheimer–Volkoff limit, which gives the upper bound for a cold, non-rotating neutron star, and is about 2.2 solar masses \citep{margalit:2017}. These limits have to do with the properties of the degenerate matter that dense stellar remnants are made of. While main sequence stars stay in hydrostatic equilibrium thanks to thermal pressure, stellar remnants resist collapse through degeneracy pressure. White dwarfs and neutrons stars can be modelled as a Fermi gas, which is the quantum mechanical equivalent of an ideal gas. The Pauli exclusion principle states that identical fermions cannot occupy the same quantum state within a quantum system simultaneously. Thus there exists a sort of interaction or pressure between the particles in a Fermi gas that keeps them separate, even at absolute zero temperature. This manifests as a degeneracy pressure that resists further gravitational collapse. In white dwarfs the electron degeneracy pressure is what keeps the object in an equilibrium. Past the Chandrasekhar limit the electrons will combine with protons to form neutrons, forming a remnant made of degenerate neutron matter. Neutron stars are then supported by the degenerate neutron pressure. For stellar remnants more massive than the Tolman–Oppenheimer–Volkoff limit, the gravitational pull will exceed even the degenerate pressure, causing the remnant to collapse into a black hole. This also gives a lower limit for the mass of new black holes that form through stellar collapse.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/SolarSystemAbundances.pdf}
\caption{Diagram illustrating the abundancies of different elements in our solar system. Iron is the final product of fusion in stars, causing it to be very abundant in the universe thanks to its production in stars before they explode. The abundant alpha elements are also clearly noticeable.
(\copyright \ User:MHz\textasciigrave as / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:IronPeak}
\end{figure}

One way stellar mass black holes have been observed is in X-ray binaries. These kinds of binaries were discovered through the detection of highly energetic X-ray sources, having luminosities with the same magnitude as the theoretical maximum luminosity from matter accreting onto a black hole, assuming simple spherical accretion. This is called the Eddington limit, and it will be discussed more later on. This combined with their low timescale variability (down to milliseconds) lent support for a model where the X-rays are produced by matter from a star accreting onto a compact object. A way that black holes can be observed without accretion is through the dynamics of nearby stars. If we can detect a star on a short orbit around some invisible point in space, there must be some compact mass that the star is orbiting around.

The mass of a compact object $M_x$ can be estimated by measuring the orbital period $P$, radial velocity amplitude $K$, and the mass of a companion star $M_c$, and the inclination angle $i$ of the star's orbit. Inclination is important because of the mass-inclination degeneracy. Measuring the true orbital velocity of a star depends on its inclination, and thus measuring the mass of the star also depends on it. The inclination can be constrained for example by tracking and modelling how the brightness of the star varies along the orbit \citep{kerkwijk:2011}. These are used in the mass function equation 
\begin{equation}
f(M_x) = \frac{K^3 P}{2 \pi G} = \frac{M_x^3 \mathrm{sin}^3 i}{(M_x + M_c)^2}
\end{equation}
which tells the lower limit for $M_x$ \citep{casares:2007}. The key factor in the equation is $M_c$ which can have a lot of uncertainty. The reason that the mass of the central object is interesting is that both neutron stars and black holes are very compact massive objects and they both can produce X-ray radiation. Matter accreting onto the objects heats up in the accretion disc due to the particles colliding with each other. The matter can reach temperatures of millions of degrees and radiates away its potential energy as X-rays. With neutron stars, infalling matter can also actually hit the surface of the object, heating up upon impact to emit X-rays. If the mass can be measured to be well above the aforementioned Tolman–Oppenheimer–Volkoff limit, other objects than black holes can be quite safely ruled out.

%puhu muista tavoista havaita, värihommaa?
%neutronitähdissä voi olla flareja

%fuusiokartta, raudasta ei pääse eteenpäin, iron peak

%observable in x-ray compact binary systems

\subsection{Intermediate mass black holes}

Intermediate mass black holes are much more elusive compared to both their more massive and lighter companions. We have only very few good candidates and their formation is not fully understood. Learning more about both the birth and evolution of intermediate mass black holes is an important topic, as it allows us to learn more about how lighter black holes gain mass and form supermassive black holes, which in turn is a key element in understanding the formation and evolution of galaxies.

One way for black holes to grow in mass is through the accretion of gas. Matter that accretes onto a black hole forms a disk and will eventually fall into the black hole. The matter particles can collide with each other, creating heat. The heat is radiated away, dominantly in the range of X-rays. This process creates radiation pressure that resists more matter falling into the black hole. The radiation increases as more matter accretes onto the hole, and the limit where the outward radiation force and the inward gravitational force are balanced is called the Eddington luminosity or Eddington limit. This limit, assuming spherical accretion, is given by
\begin{equation}
L_{Edd} = \frac{4 \pi c G M m_p}{\sigma_T} \simeq 3.2 \times 10^4 \left( \frac{M}{M_\odot} \right) L_\odot \ ,
\end{equation}
where $c$ is the speed of light, $G$ is the gravitational constant, $M$ is the mass of the black hole, $L_\odot$ is the luminosity of the Sun, $m_p$ is the proton mass, and $\sigma_T$ is the Thomson scattering cross-section for an electron. The Thomson cross-section describes the effective area for photon interactions for a free charged particle. Assuming there are no other effects which affect the accretion rate, this limit gives the maximum rate at which black holes can grow solely through spherical accretion. 

A so called ``seed'' black hole with a mass around $10^6 M_\odot$ would require over 0.5 Gyr to grow into a $10^9 M_\odot$ supermassive black hole, even if it constantly grows at Eddington rate. Due to this fact, the observed existence of supermassive black holes of $10^9 M_\odot$ when the universe was around 1 Gyr old implies that these seed black holes must have formed at redshifts larger than 10. In such a young universe there are several ways that intermediate mass black holes could form \citep{mezcua:2017}. These methods are illustrated in figure \ref{fig:imbhs}.

\begin{figure}
\centering
\includegraphics[width=\textwidth]{../images/imbhs.pdf}
\caption{Different methods of birth for seed intermediate mass black holes. These include collapse of Pop III stars, mergers in dense stellar clusters, and direct collapse of gas in protogalaxies. Some black holes will grow to be supermassive, but some will stay in their intermediate state to this day.
(Figure adapted from \citealt{mezcua:2017})}
\label{fig:imbhs}
\end{figure}

One way for these kinds of black holes to form could be through massive Population III stars. These stars formed early in the universe and thus they had very low metallicities since heavy elements had not yet formed. Low metallicity allowed these stars to grow very large, which can be explained with how metallicity affects the cooling of gas clouds. 
The most relevant cooling processes in this case are two-body radiative cooling processes. At temperatures of over $10^6$ Kelvin where the matter is almost completely ionized, the most significant process is bremsstrahlung or breaking radiation, where emissions happen due to acceleration of electrons when they encounter atomic nuclei. At lower temperatures other processes come into play as well. In recombination an electron recombines with an ion to emit a photon. In collisional ionization an electron collides with an atom or ion and ionizes it, removing from the gas an amount of kinetic energy equal to the ionization threshold. And finally in collisional excitation atoms are excited via collisions, and they emit photons as they decay back to the ground state. Metals have more possible energy states, so more metals means that these processes can happen more often, thus allowing for more efficient cooling. Conversely if there are little to no metals, these processes happen more slowly.

As stars form from collapsing gas clouds, the mass of the cloud also determines the maximum mass of the star. Jeans mass gives the limit for the size of a symmetrical, non-rotating gas cloud before it will collapse under its own gravity. The limit comes from when the cloud's thermal energy and gravitational potential are in equilibrium. The mass can be calculated by
\begin{equation}
M_J = \left( \frac{5 k T}{G \mu m} \right)^{3/2} \left( \frac{3}{4 \pi \rho} \right)^{1/2} \ ,
\end{equation}
where $k$ is the Boltzmann's constant, $G$ the gravitational constant, $T$ the temperature of the gas, $\rho$ the density of the gas, $\mu$ the mean molecular weight, and $m$ the mass of a particle comprising the gas. So the hotter and thinner the gas cloud is, the more massive it has to be for it to collapse. 

At the start of the contraction, the cloud undergoes isothermal collapse. Meaning that the density increases but the temperature can stay approximately constant thanks to cooling. This can allow the cloud to fragment, as isothermal collapse causes the Jeans mass to decrease, allowing the cloud to start collapsing around small internal density inhomogeneities. As the density increases, the cloud becomes more optically thick and the temperature will start to rise as well. This adiabatic collapse will cause the cloud to start heating up into a protostar. If cooling is inefficient to begin with, like in metal-poor clouds made of molecular hydrogen, the isothermal phase can be quite short, decreasing fragmentation, allowing the stars to become very big, and Pop III stars can exceed 200 solar masses.

These stars can also retain their mass as they exhibit weaker stellar winds compared to more metal-rich stars. This is because radiation pressure on many spectral lines is the main mechanism behind winds in massive stars. Hydrogen and helium have only few relevant lines, so metal lines are the major reason for strong stellar winds \citep{vink:2001}. When these stars collapse in a similar manner to lower mass stars, they could then form black holes with masses of several hundred $M_\odot$. These kinds of black holes would still be relatively light, requiring super-Eddington accretion rates to reach the observed masses at around redshift 6.

To circumvent this problem, the seed black holes would need to be more massive from the start. Black holes could also form inside the first protogalaxies by direct collapse of gas. The formation of these black holes would require a rapid inflow of dense gas. The gas must have minimal angular momentum so that it does not form a disk structure to support itself. In addition, to prevent fragmentation, atomic hydrogen, not molecular hydrogen, must dominate the cooling process, making it extremely inefficient. In this kind of environment the gas could form supermassive stars with masses of $10^5 M_\odot$, which could collapse into black holes of $10^4 - 10^5 M_\odot$. Another prediction for the formation of intermediate mass black holes is through multiple mergers in dense stellar clusters. In a dense region of stars, multiple stellar mergers can cause the formation of supermassive stars that can collapse into black holes of $10^2 - 10^4 M_\odot$ \citep{mezcua:2017}.

To figure out whether intermediate mass black holes exist or not, we need to measure the mass of candidate black holes. The most accurate way to probe the mass is through stellar or gas dynamics, i.e. observing the movements of objects around the black hole, and measuring the mass of the central object based on their orbits. Although with our current instruments this method only works for relatively nearby objects. A black hole of $10^5 M_\odot$ has a sphere of influence of about 0.5 pc, and cannot be resolved beyond a megaparsec. Sphere of influence describes the area around a black hole where its gravitational potential dominates the potential of the surrounding galaxy. It is defined with the equation 
\begin{equation}
r_h = \frac{G M_{BH}}{\sigma^2} \ ,
\end{equation}
where $G$ is the gravitational constant, $M_{BH}$ the mass of the black hole, and $\sigma$ the velocity dispersion of the stars in the surrounding bulge.
In addition to kinematics, radiation signatures such as X-ray and radio emissions can be used to ascertain whether an object is a black hole and what its mass is, since the mass and type of the compact object determines how matter accretes onto it, which in turn affects what kind of radiation the accreting matter emits. One of the best current candidates for an intermediate mass black hole is the central object in the globular cluster G1. Both X-ray and radio emissions have been detected there, and photometric and kinematic observations have given an estimate for a black hole with a mass of around $1.8 \times 10^4 M_\odot$ \citep{gebhardt:2005}. One can obtain the mass through these measurements by calculating the full line-of-sight velocity distribution (LOSVD), and using this velocity to estimate the central mass.

\subsection{Supermassive black holes}

Supermassive black holes are the sources of some of the most energetic phenomena in the known universe, thus we have been able to detect their presence despite their relative scarcity and distance. They are found in the centers of galaxies, and it is nowadays generally accepted that supermassive black holes are present in practically every massive galaxy with a bulge component \citep{kormendy:2013}. What would eventually become the first candidates for supermassive black holes were discovered when very luminous radio sources were observed in the 1960s \citep{greenstein:1964}. First thought to be stars, their luminosities, redshifts, and short timescale variabilities made it quite clear that they were something else, hence they were dubbed quasi-stellar objects, or quasars. It was later proposed that matter falling onto a massive compact object could explain the phenomenon.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/MilkyWaySMBH.pdf}
\caption{Illustration of the well-determined orbits of 6 stars around Sagittarius A* in the galactic centre of our Milky Way. The star S2 has an orbital period of about 16 years, and has given us the most accurate estimate for the black hole mass to date. The data is based on \cite{eisenhauer:2005}.
(\copyright \ User:Cmglee / Wikimedia Commons / CC-BY-SA-3.0)}
\label{fig:MilkyWayBH}
\end{figure}

On top of spectroscopic measurements, the high resolution of the Hubble Space Telescope (HST) has enabled us to do dynamical measurements of galaxies and their massive black holes, meaning that we can track the movements of stars in these distant galaxies. In addition to HST, thanks to advances in adaptive optics, ground based observations have given us the best proof for a supermassive black hole at the center of our own galaxy. The galactic center is only around 8.1 kpc away \citep{eisenhauer:2018}, so individual stars can be resolved and their orbits determined. In figure \ref{fig:MilkyWayBH} orbits of 6 stars orbiting around the galactic center are visualized. Based on the orbit of the star S2, there must be a mass of around $4 \times 10^6 M_\odot$ at the center of the galaxy, and it must be located inside the radius of around 120 AU, otherwise the star would collide with the central mass \citep{kormendy:2013}. This rules out the possibility of the central mass being a large group of smaller masses, and makes a singular, massive central object the most likely scenario.

Supermassive black holes are believed to play an important role in the formation and evolution of galaxies. The mass of the central black hole correlates with the properties of the host galaxy and central bulge that contains it. The first of these correlations to be observed was the $M_{BH} - L_{bulge}$ correlation. The correlation between the black hole mass and the luminosity of the bulge has been tested on many galaxies, and it seems to be very consistent. The luminosity of the bulge is dependent on the mass of the bulge, but the correlation between the explicitly measured mass of the bulge and the black hole mass has also been tested and observed. This $M_{BH} - M_{bulge}$ relation is expressed by
\begin{equation}
\mathrm{log} (M_{BH}/M_{\odot}) = 1.12 \  \mathrm{log} (M_{bulge}/ 10^{11} M_{\odot}) + 8.2 \ ,
\end{equation}
and it has been found to be accurate for many galaxies \citep{haring:2004}.

One way to measure the mass of the bulge is by calculating its virial mass. The virial theorem relates the kinetic energy of a stable to system with its potential energy. In astrophysics the common use of this relation is to relate the gravitational potential energy of a system with its kinetic energy. The virial mass of a bulge can be calculated with the equation
\begin{equation}
M_{bulge} = \frac{k r_e \sigma^2_e}{G} \ ,
\end{equation}
where $k$ is a scaling factor, $r_e$ is the effective radius, $\sigma_e$ is the velocity dispersion inside the effective radius, and $G$ is the gravitational constant \citep{marconi:2003}. The velocity dispersion of the galaxy has been used in these previous correlations, but there seems to be a correlation between the black hole mass and the velocity dispersion itself. This $M_{BH} - \sigma$ relation is given by
\begin{equation}
\mathrm{log} (M_{BH}/M_{\odot}) = 4.24 \ \mathrm{log} ( \sigma / 200 \ \mathrm{km \, s^{-1}} ) + 8.12 \ , 
\end{equation}
and it has been found to be highly accurate for many galaxies \citep{gultekin:2009}.
A tight $M_{BH} - \sigma$ correlation is important on a practical level since it allows for good estimates of the black hole mass from relatively easy measurements, and it also implies a link between the growth of the black hole and formation of the bulge \citep{kormendy:2013}.

\section{Newtonian dynamics}

\subsection{Two-body problem}

Analytically solving the Newtonian two-body problem, i.e.\ determining the motions of two particles interacting only with each other through gravity, is extremely useful. It allows solving the behaviour of two essentially isolated objects, such as a binary star system. It for example also enables us to calculate the masses of two objects that are observed to be interacting with each other if their orbits are known.

Let us assume we have a mass $m_1$ situated at position vector $\mathbf{r}_1$, and a mass $m_2$ at position vector $\mathbf{r}_2$. If the first mass exerts a force $\mathbf{f}_{21}$ on the second mass, by Newton's third law the second object exerts an equal and opposite force, $\mathbf{f}_{12} = -\mathbf{f}_{21}$, on the first object. The masses and their positions are illustrated in figure \ref{fig:2BodyProb}. If these two objects are the only objects in the system, their equations of motion are
\begin{align}
m_1 \mathbf{\ddot{r}}_1 &= -\mathbf{f} \label{equ:eom1} \\
m_2 \mathbf{\ddot{r}}_2 &= \mathbf{f} \label{equ:eom2} \ ,
\end{align}
where $\mathbf{f} = \mathbf{f}_{21}$.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.5]{../images/2bp.pdf}
\caption{The position vectors $\mathbf{r}_1$ and $\mathbf{r}_2$ point to the masses $m_1$ and $m_2$ respectively. $\mbox{$\mathbf{r} = \mathbf{r_2} - \mathbf{r_1}$}$ is their relative displacement vector, and $\mathbf{R}$ is the position vector of the center of mass of the system.}
\label{fig:2BodyProb}
\end{figure}

We can first get an equation describing the motion of the center of mass of the system by adding the equations \eqref{equ:eom1} and \eqref{equ:eom2} together. The position vector of the center of mass is
\begin{equation}
\mathbf{R} = \frac{m_1 \mathbf{r}_1 + m_2 \mathbf{r}_2}{m_1 + m_2}
\end{equation}
and by performing the addition we get
\begin{equation}
m_1 \mathbf{\ddot{r}}_1 + m_2 \mathbf{\ddot{r}}_2 = (m_1 + m_2) \mathbf{\ddot{R}} = \mathbf{f} - \mathbf{f} = 0 \ .
\end{equation}
This gives us that
\begin{equation}
\mathbf{\ddot{R}} = 0 \ ,
\end{equation}
which shows that the velocity of the center of mass is constant. So the position of the center of mass can be determined at all times from the initial positions and velocities. We can denote the relative displacement vector of the two masses with $\mbox{$\mathbf{r} = \mathbf{r_2} - \mathbf{r_1}$}$. Together with this and the center of mass vector, we can write
\begin{align}
\mathbf{r_1} &= \mathbf{R} - \frac{m_2}{m_1 + m_2}\mathbf{r} \\
\mathbf{r_2} &= \mathbf{R} + \frac{m_1}{m_1 + m_2}\mathbf{r}
\end{align}
Substituting these equations into \eqref{equ:eom1} and \eqref{equ:eom2}, we can see that both yield 
\begin{equation}
\mu \mathbf{\ddot{r}} = \mathbf{f} \ , \label{equ:eom3}
\end{equation}
where
\begin{equation}
\mu = \frac{m_1 m_2}{m_1 + m_2}
\end{equation}
is called the reduced mass. Thus we have effectively converted the two-body problem of masses $m_1$ and $m_2$ into a one-body problem of the reduced mass $\mu$. The situation is now simpler since we do not have to inspect the movement of two separate particles, but only the movement of one particle with respect to the other particle.

In the case of celestial objects, the force affecting the masses is gravity. The Newtonian gravitational force that the first object exerts on the second object is given by
\begin{equation}
\mathbf{f} = -\frac{G m_1 m_2}{r^3} \mathbf{r}
\end{equation}
where $\mathbf{r}$ is the aforementioned displacement vector and $G$ is the gravitational constant. Substituting this into equation \eqref{equ:eom3} gives us
\begin{equation}
\frac{m_1 m_2}{m_1 + m_2} \mathbf{\ddot{r}} = -\frac{G m_1 m_2}{r^3} \mathbf{r}
\end{equation}
which gives us the equation of motion
\begin{equation}
\mathbf{\ddot{r}} = -\frac{G M}{r^3} \mathbf{r} \label{equ:eomgravity}
\end{equation}
where $M = m_1 + m_2$ is the total mass of the system.

\subsection{Equation of the orbit}

The derivation for the Keplerian equation of orbit is done following \cite{bt-galdyn}. We set up spherical coordinates $(r, \theta, \phi)$ as position of $m_1$. The position vector can then be expressed as
\begin{equation}
\mathbf{r} = r \hat{\mathbf{e}}_r \ ,
\end{equation}
where $\hat{\mathbf{e}}_r$ is the radial direction vector. The gravitational field in the equation of motion can be expressed as
\begin{equation}
g(r) = -\frac{G M}{r^2} \ ,
\end{equation}
so the equation of motion becomes
\begin{equation}
\frac{\mathrm{d}^2 \mathbf{r}}{\mathrm{d} t^2} = g(r) \hat{\mathbf{e}}_r \ .
\end{equation}
Since the cross product of a vector with itself is zero, we can see that
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} \left( \mathbf{r} \times \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} \right) = \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} \times \frac{\mathrm{d} \mathbf{r}}{\mathrm{d} t} + \mathbf{r} \times \frac{\mathrm{d}^2 \mathbf{r}}{\mathrm{d} t^2} = 0 + r \hat{\mathbf{e}}_r \times g(r) \hat{\mathbf{e}}_r = 0 \ ,
\end{equation}
thus $\mathbf{r} \times \mathbf{\dot{r}}$ is some constant vector and always orthogonal to both $\mathbf{r}$ and $\dot{\mathbf{r}}$, which we can denote with for example $\mathbf{L}$. This is simply the angular momentum per unit mass. It is a constant vector, and therefore the movement of the orbiting objects must take place in a plane. Limiting the movement to two dimensions simplifies the situation, and we can use plane polar coordinates $(r, \theta)$ where the center of attraction is at $r = 0$ and $\theta$ is the azimuthal angle in the plane. We can now find the equations of motions for the system in these coordinates by forming the Lagrangian per unit mass, which is
\begin{equation}
\mathcal{L} = K - V = \frac{1}{2} \mathbf{v}^2 - \Phi (r) = \frac{1}{2} \left[ \dot{r}^2 + (r \dot{\theta})^2 \right] - \Phi (r) \ ,
\end{equation}
where $\Phi$ is the gravitational potential and $g(r) = -\mathrm{d} \Phi / \mathrm{d} r$. Writing out the Lagrange equations gives us the equations of motion, which are
\begin{align}
\frac{\mathrm{d} }{\mathrm{d} t} \left( \frac{\partial \mathcal{L}}{\partial \dot{r}} \right) - \frac{\partial \mathcal{L}}{\partial r} &= \ddot{r} - r \dot{\theta}^2 + \frac{\mathrm{d} \Phi}{\mathrm{d} r} = 0 \label{equ:leom1} \\
\frac{\mathrm{d} }{\mathrm{d} t} \left( \frac{\partial \mathcal{L}}{\partial \dot{\theta}} \right) - \frac{\partial \mathcal{L}}{\partial \theta} &= \frac{\mathrm{d}}{\mathrm{d} t} (r^2 \dot{\theta}) = 0 \label{equ:leom2} \ .
\end{align}
Equation \eqref{equ:leom2} implies that $r^2 \dot{\theta}$ is a constant, which we can denote with $L$. And this is actually equal to the norm of the $\mathbf{L}$ vector. So this equation is a restatement of the conservation of angular momentum. We can use it to replace time with angle as the independent variable in equation \eqref{equ:leom1}. Equation \eqref{equ:leom2} implies that
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} = \frac{L}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \ ,
\end{equation}
so equation \eqref{equ:leom1} becomes
\begin{equation}
\frac{L^2}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \left( \frac{1}{r^2} \frac{\mathrm{d} r}{\mathrm{d} \theta} \right) - \frac{L^2}{r^3} = -\frac{\mathrm{d} \Phi}{\mathrm{d} r} \label{equ:leom1edit} \ .
\end{equation}
Expanding the first term gives us 
\begin{equation}
\frac{L^2}{r^2} \frac{\mathrm{d}}{\mathrm{d} \theta} \left( \frac{1}{r^2} \frac{\mathrm{d} r}{\mathrm{d} \theta} \right) = -\frac{2 L^2}{r^5} \left( \frac{\mathrm{d} r}{\mathrm{d} \theta} \right)^2 + \frac{L^2}{r^4} \frac{\mathrm{d}^2 r}{\mathrm{d} \theta^2} \ .
\end{equation}
Making the substitution $u \equiv \frac{1}{r}$ allows us to simplify the equation. Since
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} = \frac{2}{r^2} \left( \frac{\mathrm{d} r}{\mathrm{d} \theta} \right)^2 - \frac{1}{r^2} \frac{\mathrm{d}^2 r}{\mathrm{d} \theta^2} \ ,
\end{equation}
we can rearrange the terms so that equation \eqref{equ:leom1edit} becomes
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = \frac{1}{L^2 u^2} \frac{\mathrm{d}}{\mathrm{d} r} \Phi (r) \ .
\end{equation}
Further using the relation
\begin{equation}
\frac{\mathrm{d} \Phi}{\mathrm{d} r} = -u^2 \frac{\mathrm{d} \Phi}{\mathrm{d} u}
\end{equation}
this simplifies into
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = -\frac{1}{L^2} \frac{\mathrm{d}}{\mathrm{d} r} \Phi (1/u) \label{equ:leom3} \ .
\end{equation}
Because our gravitational potential is of the form $\Phi = -GMu$, equation \eqref{equ:leom3} simply becomes
\begin{equation}
\frac{\mathrm{d}^2 u}{\mathrm{d} \theta^2} + u = \frac{GM}{L^2} \ .
\end{equation}
We can immediately see that this is the equation for a forced harmonic oscillator. So we know that its general solution is
\begin{equation}
u(\theta) = C \mathrm{cos}(\theta - \theta_0) + \frac{GM}{L^2} \label{equ:eomsolution} \ ,
\end{equation}
where $C > 0$ and $\theta_0$ are arbitrary constants. We set $\theta_0 = 0$. By defining the orbit's eccentricity, i.e. how much the shape of the orbit differs from a perfect circle, by 
\begin{equation}
e \equiv \frac{CL^2}{GM}
\end{equation}
and its semimajor axis, i.e. half of the longest axis of an ellipse, by
\begin{equation}
a \equiv \frac{L^2}{GM(1-e^2)} \ ,
\end{equation}
equation \eqref{equ:eomsolution} can be rewritten as 
\begin{equation}
r(\theta) = \frac{a (1-e^2)}{1 + e \ \mathrm{cos} \, \theta} \ .
\end{equation}
This is the equation of the orbit for Keplerian orbits, and it is also the equation for a conic section, since all Keplerian orbits are conic sections. The parameter $\theta$ is called the true anomaly, and it is defined as the angle from the periapsis, as seen from the main focus of the ellipse. Figure \ref{fig:ellipse} illustrates these different parameters.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/ellipse.pdf}
\caption{The different parameters of an elliptic orbit. $C$ is the center of the ellipse, $F_1$ and $F_2$ are the foci of the ellipse, $a$ is the semimajor axis, $b$ is the semiminor axis, $f$ is the distance between the center and a focus of the ellipse, $r$ points to the current position of the orbiting body, and $\theta$ is the true anomaly of the object. Eccentricity $e$ can be expressed as the ratio between $f$ and $a$.}
\label{fig:ellipse}
\end{figure}

The analytic solution of the two-body problem works quite accurately for example in our solar system, even though there are more than just two objects. This is because the mass of the Sun dominates all of the interactions and the planets are far away from each other, so each planet can to the first order be examined on its own. However in more extreme environments, the approximation faces some limitations. The singularity in the equation of motion as $r$ approaches 0 is troublesome when running simulations with close encounters, so it is very useful to rewrite the equations so that the singularity does not appear. This is called regularization. The equation of motion also works only for classical Newtonian systems, but fails to give correct results when relativistic effects become prominent. Thus the equation must be altered, for example with post-Newtonian terms, to make it applicable in a relativistic setting. The next two chapters tackle these two problems, respectively.

%TODO
%mainitse lopussa että ei toimi täsmälleen tälleen koska suhteellisuusteoria, pohjusta PN hommaa
%koska yhtälöissä on per r termi, tarvitaan regularisaatiota. Menee ikäväksi kun kappaleet ovat hyvin lähellä toisiaan

\section{Regularization}

Because of the $1/r^2$ term in the equation of motion (equation \eqref{equ:eomgravity}), the two-body problem becomes singular when the distance between two objects approaches zero. It is because of these singularities that simulations of the two-body problem become very arduous when collisions or close approaches occur. Integrators may attempt to handle the singularity by taking increasingly smaller time-steps as the separation between the particles decreases, which slows down the entire simulation drastically. The problem can be avoided by transforming to a coordinate system which does not exhibit any singularities, and this procedure is known as regularization \citep{bt-galdyn}. 
%Usually this comes with the cost of having a higher number of variables to keep track of. But regularization shows great advantages in the time-step sizes needed for accurate integration, and the amount of time-steps needed for integrating close encounters is much smaller \citep{diplomarbeit}. 

\subsection{One-dimensional regularization}

The one-dimensional case is simple, but gives a good basis for understanding the methods used in the higher-dimensional cases as well. The Hamiltonian for the relative motion in a one-dimensional two-body problem is
\begin{equation} \label{equ:Hamilton1d}
H(q,p,t) = K + V = \frac{p^2}{2 \mu} - \frac{G \mu M}{\left| q \right|} \ ,
\end{equation}
where $q$ and $p$ are the position and momentum respectively, $\mu$ is the reduced mass, $M$ is the total mass, and $G$ is the gravitational constant. 
To regularize the equation of motion we must do both a coordinate transformation and a time transformation. Starting with the coordinate transformation, we do a transformation to some new coordinate system $(Q, P)$. We can perform a canonical point transformation, i.e. a transformation that preserves the form of the Hamilton's equations, where the old coordinates are changed to new ones through $q = Q^2$. A generating function $S$ is defined as a function whose partial derivatives give us the Hamiltonian in the new coordinate system. We can now get the new momentum through a generating function of the form
\begin{equation}
S(p,Q) = pq(Q) = pQ^2
\end{equation}
by calculating its partial derivatives
\begin{equation}
P = \frac{\partial S}{\partial Q} = 2pQ \ ,
\end{equation}
from which we get that
\begin{equation}
p = \frac{P}{2Q} \ .
\end{equation}
By plugging in the new definitions for $q$ and $p$ into equation \eqref{equ:Hamilton1d} the new Hamiltonian is then
\begin{equation}
H(Q,P,t) = \frac{P^2}{8Q^2 \mu} - \frac{G \mu M}{Q^2} \ .
\end{equation}

Next we want to do a time transformation as well. We may choose a function $t = t(s)$, where $s$ is the new time variable, often also called fictitious or regularized time. In a more general sense we may choose a time transformation function $g(q,p)$ so that $\mathrm{d} t = g \, \mathrm{d} s$ \citep{ad}. For the transformation to remain canonical, our new Hamiltonian must be obtained through the Poincaré time transformation \citep{mikkola:2008a}. In the Poincaré transformation one takes time to be a canonical coordinate $(t=q_0)$ and the corresponding momentum of time is the binding energy of the system, $p_0 = -E$. The new Hamiltonian obtained through this transformation is then
\begin{equation}
\Gamma(q,p,s) = g(q,p) (H(q,p,q_0) + p_0) \ . \label{equ:timetransform}
\end{equation}
By choosing $g = r = \left| q \right| = Q^2$, where $r$ is simply the particle separation, the transformation gives us
\begin{align}
\Gamma(Q,P,s) &= Q^2 \left( \frac{P^2}{8Q^2 \mu} - \frac{G \mu M}{Q^2} -E \right) \nonumber \\
&= \frac{P^2}{8 \mu} - E Q^2 - G \mu M \ .
\end{align}
The time evolution of a system is uniquely defined by the normal Hamilton equations
\begin{align}
\frac{\mathrm{d} q}{\mathrm{d} t} &= \frac{\partial H}{\partial p} \\
\frac{\mathrm{d} p}{\mathrm{d} t} &= -\frac{\partial H}{\partial q} \ .
\end{align}
Since the time transformation preserves the form of the Hamilton equations, they are now simply
\begin{align}
\frac{\mathrm{d} Q}{\mathrm{d} s} &= \frac{\partial \Gamma}{\partial P} = \frac{P}{4 \mu} \label{equ:ttHamilton1} \\
\frac{\mathrm{d} P}{\mathrm{d} s} &= -\frac{\partial \Gamma}{\partial Q} = 2EQ \label{equ:ttHamilton2} \ .
\end{align}
Taking a second time derivative of equation \eqref{equ:ttHamilton1}, we get 
\begin{equation}
\frac{\mathrm{d}^2 Q}{\mathrm{d} s^2} = \frac{1}{4 \mu} \frac{\mathrm{d} P}{\mathrm{d} s}
\end{equation}
and we can insert the result from equation \eqref{equ:ttHamilton2} to get the final equation of motion in these new coordinates
\begin{equation}
\frac{\mathrm{d}^2 Q}{\mathrm{d} s^2} = \frac{E}{2 \mu} Q \label{equ:eomRegu} \ .
\end{equation}
The singularity at $r = 0$ has now been removed. In a bound system $E < 0$, so the solution to the equation of motion is a simple harmonic oscillator.

\subsection{Levi-Civita transformation} \label{sect:2dregu}

Regularization of the two-dimensional case is quite similar to the one-dimensional case, but now our positions and momentums are two dimensional vectors. It has been named the Levi-Civita transformation, after the Italian mathematician who first used this method \citep{levi-civita:1920}. The Hamiltonian is now
\begin{equation}
H(\mathbf{r},\mathbf{p},t) = \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} \ .
\end{equation}
The transformation to the new coordinates is given via complex numbers. The new coordinates are $Q_1, Q_2$ so that $\mathbf{Q} = Q_1 + iQ_2$ and $r = x + iy = \mathbf{Q}^2 = (Q_1 + iQ_2)^2$. From this we get $x = Q_1^2 - Q_2^2$ and $y = 2Q_1 Q_2$. By again choosing $g = r = \mathbf{Q}^2$ as in the previous section, the Poincaré transform gives us 
\begin{align}
\Gamma &= r \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} - E \right) \nonumber \\ 
&= \mathbf{Q}^2 \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{\mathbf{Q}^2} - E \right) \nonumber \\
&= \frac{2^2 \mathbf{Q}^2 \cdot \mathbf{p}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \nonumber \\
&= \frac{\mathbf{P}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \ ,
\end{align}
where the new momenta are $\mathbf{P} = 2 \mathbf{Q} \cdot \mathbf{p}$. Similarly to the one-dimensional case, this is the Hamiltonian of a simple harmonic oscillator, except in this case our harmonic oscillator is two-dimensional. The singularity has thus once again been removed.

The transformation $r = \mathbf{Q}^2$ can also be expressed in vector notation as
\begin{equation}
\mathbf{r} = \mathcal{L}(\mathbf{Q}) \mathbf{Q} \ ,
\end{equation}
where $\mathcal{L}(\mathbf{Q})$ is the Levi-Civita matrix
\begin{equation}
\mathcal{L}(\mathbf{Q}) =
\begin{pmatrix}
Q_1 & -Q_2 \\
Q_2 & Q_1
\end{pmatrix} \ .
\end{equation}
This sort of matrix has properties that are useful when formulating transformations also in higher dimensional cases.

\subsection{Kustaanheimo-Stiefel method} \label{sect:KSmethod}
While the two-body problem is limited to two dimensions in theory, in practice there are usually perturbations which perturb the orbit into the third dimension, thus a three-dimensional generalization is desirable. A problem with this arises from the fact that it is not possible to do the generalization directly in three dimensions. The Levi-Civita matrix is a good starting point, and it has several useful properties. All of its elements are linear homogeneous functions of $Q_i$, and the mapping defined by the matrix is conformal, i.e. angle-preserving. A matrix with such properties cannot exist in three dimensions, and the next suitable step from a two-dimensional matrix is a four-dimensional matrix \citep{diplomarbeit}. The Kustaanheimo-Stiefel (KS) method involves doing the transformation into three-dimensional space through four dimensions.

The old and new coordinates and momenta are defined as
\begin{equation}
\begin{aligned}
\mathbf{r} &= (r_1, \ r_2, \ r_3, \ r_4)^\mathrm{T} \\
\mathbf{p} &= (p_1, \ p_2, \ p_3, \ p_4)^\mathrm{T} \\
\mathbf{Q} &= (Q_1, \ Q_2, \ Q_3, \ Q_4)^\mathrm{T} \\
\mathbf{P} &= (P_1, \ P_2, \ P_3, \ P_4)^\mathrm{T} \ ,
\end{aligned}
\end{equation}
where $T$ simply means the transpose of the matrix, and the KS matrix is defined as 
\begin{equation}
\mathcal{L}(\mathbf{Q}) =
\begin{pmatrix}
Q_1 & -Q_2 & -Q_3 & Q_4 \\
Q_2 & Q_1 & -Q_4 & -Q_3 \\
Q_3 & Q_4 & Q_1 & Q_2 \\
Q_4 & -Q_3 & Q_2 & -Q_1
\end{pmatrix} \ .
\end{equation}
The transformation is given by the following relations \citep{ad}: 
\begin{equation}
\mathbf{r} = \mathcal{L} \mathbf{Q} \label{equ:KStransform}
\end{equation}
and
%\begin{equation}
%\mathbf{P} = 2 \mathcal{L}^\mathrm{T} \mathbf{p} \ \label{equ:KStransform2} .
%\end{equation}
%Multiplying equation \eqref{equ:KStransform2} with $\mathcal{L}$ yields
%\begin{equation}
%\mathcal{L} \mathbf{P} = 2 \mathcal{L} \mathcal{L}^\mathrm{T} \mathbf{p} = 2 \mathbf{Q}^2 \mathbf{p} \label{equ:KStransform3} \ ,
%\end{equation}
%since the KS matrix shares the property of the Levi-Civita matrix that $\mathcal{L} \mathcal{L}^\mathrm{T} = \mathcal{L}^\mathrm{T} \mathcal{L} = \mathbf{Q}^2 \mathbf{I}$, where $\mathbf{I}$ is the identity matrix. From equation \eqref{equ:KStransform3} we get that 
%\begin{equation}
%\mathbf{p} = \frac{\mathcal{L} \mathbf{P}}{2 \mathbf{Q}^2} \ ,
%\end{equation}
%and from squaring this we get
\begin{equation}
\mathbf{p}^2 = \frac{\mathbf{P}^2}{4 \mathbf{Q}^2} \ .
\end{equation}
By again using the time transform function $g = r = \left| \mathbf{r} \right| = \mathbf{Q}^2$ we get
\begin{align}
\Gamma = r \left( \frac{\mathbf{p}^2}{2 \mu} - \frac{G \mu M}{r} - E \right) &= \mathbf{Q}^2 \left( \frac{\mathbf{P}^2}{8 \mathbf{Q}^2 \mu} - \frac{G \mu M}{\mathbf{Q}^2} - E \right) \nonumber \\ 
&= \frac{\mathbf{P}^2}{8 \mu} - E \mathbf{Q}^2 - G \mu M \ ,
\end{align}
which once again is the Hamiltonian of a harmonic oscillator, this time in four dimensions. The coordinate singularity has again disappeared similarly to the previous cases.
However there is still an extra dimension, so we need to find a mapping to convert between the three dimensional physical space and the four dimensional space.

Calculating $\mathbf{r}$ from equation \eqref{equ:KStransform} we get
\begin{align} \label{equ:KSsolve}
\mathbf{r} = 
\begin{pmatrix}
r_1 \\
r_2 \\
r_3 \\
r_4
\end{pmatrix}
= \mathcal{L} \mathbf{Q} &=
\begin{pmatrix}
Q_1 & -Q_2 & -Q_3 & Q_4 \\
Q_2 & Q_1 & -Q_4 & -Q_3 \\
Q_3 & Q_4 & Q_1 & Q_2 \\
Q_4 & -Q_3 & Q_2 & -Q_1
\end{pmatrix} \ 
\begin{pmatrix}
Q_1 \\
Q_2 \\
Q_3 \\
Q_4
\end{pmatrix}
\nonumber \\
&= 
\begin{pmatrix}
Q_1^2 - Q_2^2 - Q_3^2 + Q_4^2 \\
2(Q_1 Q_2 - Q_3 Q_4) \\
2(Q_1 Q_3 + Q_2 Q_4) \\
0
\end{pmatrix} \ .
\end{align}
Having the fourth component of $\mathbf{r}$ set to zero makes sense since in physical space we have motion only in three dimensions.
We can still freely choose one of the extra components of $\mathbf{Q}$, so there are many different ways to change between the physical variables and the KS transformed variables. One way is by setting $Q_4 = 0$ and solving the equation \eqref{equ:KSsolve} for the $Q_i$, which gives us the following relations
%\begin{align}
%Q_1 &= \left( \frac{1}{2} (Q_1^2 + Q_2^2 + Q_3^2 + Q_1^2 - Q_2^2 - Q_3^2) \right)^{1/2} = \left( \frac{1}{2} ( \left| \mathbf{r} \right| + \left| r_1 \right| ) \right)^{1/2} \nonumber \\
%Q_2 &= \frac{1}{2} \frac{2 Q_1 Q_2}{Q_1} = \frac{1}{2} \frac{r_2}{Q_1} \\
%Q_3 &= \frac{1}{2} \frac{2 Q_1 Q_3}{Q_1} = \frac{1}{2} \frac{r_3}{Q_1} \nonumber \ 
%\end{align}
\begin{align}
Q_1 &= \left( \frac{1}{2} ( \left| \mathbf{r} \right| + \left| r_1 \right| ) \right)^{1/2} \nonumber \\
Q_2 &= \frac{1}{2} \frac{r_2}{Q_1} \\
Q_3 &= \frac{1}{2} \frac{r_3}{Q_1} \nonumber \ 
\end{align}
and the mapping
\begin{equation}
\mathbf{Q} = 
\begin{cases}
\left( Q_1, \ Q_2, \ Q_3, \ 0 \right)^\mathrm{T} \ , \ \mathrm{if} \ r_1 \geq 0 \\
\left( Q_1, \ Q_2, \ 0, \ Q_3 \right)^\mathrm{T} \ , \ \mathrm{if} \ r_1 < 0 \ .
\end{cases}
\end{equation}
With these mappings we can convert the coordinates between the different dimensional spaces \citep{diplomarbeit, ad}.

Regularized computational methods are very precise, but they are typically more computationally expensive and it is difficult to regularize many particles at the same time, so there would be benefits for an accurate method that does not need this kind of regularization. Fortunately there exists another method which involves only time transformation but no coordinate transformation, named algorithmic regularization (AR) \citep{diplomarbeit}. These kinds of regularization methods do not actually remove the Newtonian collision singularities from the equations of motion, but the equations can be evaluated in a way that they behave regularly regardless. Algorithmic regularization will be discussed in more detail in chapter \ref{chap:ar-chain}.


%\noindent (TODO: Syvällisemmin KS:ää? Lisää alakohtia? Jonkunlainen loppukaneetti.)

\section{Post-Newtonian dynamics} \label{sect:pndynam}

\subsection{Approximating general relativity}

While Newtonian dynamics is a perfectly valid approximation in everyday life, it stops giving correct results for very massive compact objects or objects moving at a significant fraction of the speed of light. In these situations we need to use general relativity (GR), which describes gravity as a geometric property of spacetime. This theory is described by the Einstein field equations, which are a set of 10 coupled, non-linear partial differential equations that describe the fundamental interaction of gravitation as a result of spacetime being curved by mass and energy. The field equations are usually expressed as
\begin{equation}
R_{\mu \nu} - \frac{1}{2}R g_{\mu \nu} + \Lambda g_{\mu \nu} = \frac{8 \pi G}{c^4} T_{\mu \nu} \ ,
\end{equation}
where $R_{\mu \nu}$ is the Ricci curvature tensor and $R$ is the scalar curvature which both are used to describe how the geometry of curved space differs from the geometry of flat Euclidean space. They are functions of the variable $g_{\mu \nu}$ which is the metric tensor which describes the geometric and causal structure of spacetime in our universe. The parameter $\Lambda$ is the cosmological constant which describes the energy density of the vacuum, and the variable $T_{\mu \nu}$ is the stress-energy tensor which describes mass and energy as the source of gravity in the universe. Finally $G$ and $c$ are the gravitational constant and the speed of light respectively.

These equations are very hard to solve exactly due to their non-linear nature, and doing fully general relativistic simulations is computationally very expensive, so more wieldy methods are required for large-scale simulations. This is where post-Newtonian (PN) expansions come in. Post-Newtonian expansions in general relativity are used for finding an approximate solution of the Einstein field equations in the weak field approximation.
In other words, the post-Newtonian theory is an approximate version of general relativity that applies when the gravitational field is relatively weak, and the motion of the matter is relatively slow, $v/c \sim 0.01$. The theory successfully describes the gravitational field of our solar system for example, but it can also be applied to situations involving compact bodies, such as neutron stars or black holes.
%
%The post-Newtonian theory is derived from the Landau-Lifshitz formulation of the Einstein field equations. The equations can be written as 
%\begin{equation}
%\partial_{\mu \nu} H^{\alpha \mu \beta \nu} = \frac{16 \pi G}{c^4}(-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL}) \ ,
%\end{equation}
%where $H^{\alpha \mu \beta \nu} \equiv \mathfrak{g}^{\alpha \beta} \mathfrak{g}^{\mu \nu} - \mathfrak{g}^{\alpha \mu} \mathfrak{g}^{\beta \mu}$ is a tensor density which possesses the same symmetries as the Riemann tensor. In the Landau-Lifshitz formulation the main variables are not the components of the metric tensor $g_{\alpha \beta}$, but those of the gothic inverse metric $\mathfrak{g}^{\alpha \beta} \equiv \sqrt{-g} g^{\alpha \beta}$, where $g^{\alpha \beta}$ is the inverse metric, and $g$ the metric determinant. $T^{\alpha \beta}$ is the energy-momentum tensor of the matter source term, and the Landau-Lifshitz pseudotensor $(-g) t^{\alpha \beta}_{LL} \sim \partial \mathfrak{g} \cdot \partial \mathfrak{g}$ can be interpreted as an energy-momentum (pseudo)tensor for the gravitational field.
%
%The antisymmetry of $H^{\alpha \mu \beta \nu}$ implies the conservation equation
%\begin{equation}
%\partial_\beta \left[ (-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL}) \right] = 0 \ ,
%\end{equation}
%which is formally equivalent to $\nabla_\beta T^{\alpha \beta} = 0$, where $\nabla_\beta$ is the covariant derivative operator. The conservation equation allows for the formulation of global conservation laws, for example for energy, linear momentum, and angular momentum. We then introduce the gravitational potentials $h^{\alpha \beta} = \eta^{\alpha \beta} - \mathfrak{g}^{\alpha \beta}$, where $\eta^{\alpha \beta} = diag(-,+,+,+)$ is the Minkowski metric expressed in Lorentzian coordinates, and impose the harmonic coordinate gauge condition $\partial_\beta h^{\alpha \beta} = 0$.
%
%The field equations become a wave equation in flat spacetime
%\begin{equation} \label{equ:waveeq}
%\square h^{\alpha \beta} = -\frac{16 \pi G}{c^4} \tau^{\alpha \beta} \ ,
%\end{equation}
%where $\square = -\frac{1}{c^2} \frac{\partial^2}{\partial t^2} + \frac{\partial^2}{\partial x^2} + \frac{\partial^2}{\partial y^2} + \frac{\partial^2}{\partial z^2}$ is the flat spacetime d'Alembert operator, and $\tau^{\alpha \beta} = (-g)(T^{\alpha \beta} + t^{\alpha \beta}_{LL} + t^{\alpha \beta}_H)$ is defined as the effective energy-momentum pseudotensor, composed of a matter contribution, the Landau-Lifshitz contribution, and the harmonic gauge contribution $t^{\alpha \beta}_H \sim \partial h \cdot \partial h + h \partial^2 h$. The conversation equation now reads $\partial_\beta \tau^{\alpha \beta} = 0$.
%
%So far no approximations have been made, and the wave equation combined with the harmonic gauge condition and conservation equation is an exact formulation of the Einstein field equations. The wave equation determines the potential $h^{\alpha \beta}$ for a given distribution of matter. The behaviour of the matter is determined by the conservation equation/gauge condition.
%
%The wave equation can be integrated without enforcing the conservation equation, and this is known as the relaxed Einstein field equations. The integration is achieved by iteration. Assuming that $h^{\alpha \beta}_n$ is known, $h^{\alpha \beta}_{n+1}$ is obtained by solving
%\begin{equation}
%\square h^{\alpha \beta}_{n+1} = -\frac{16 \pi G}{c^4} \tau^{\alpha \beta}[h_n] \ .
%\end{equation}
%The iterations are started with $h^{\alpha \beta}_0 = 0$, and stopped when the desired accuracy is obtained. In principle, the truncation is the only source of approximation. The procedure produces a formal expansion of $h^{\alpha \beta}$ in powers of G. This is known as the post-Minkowskian expansion of the gravitational field. After iterations have been done up to the desired accuracy, we can again impose the gauge condition to get a proper metric.
%
%The difference between post-Minkowskian and post-Newtonian expansion is that in post-Newtonian approximation we assume a slow-motion condition, i.e. all speeds within the matter distribution (such as the speed of sound within a body, or the speed of the body as a whole) are small compared with the speed of light. In astrophysical situations the assumption of slow speeds is accurate in the vast majority of cases, since the virial theorem implies that $U \sim v^2$ for any gravitationally bound system; weak fields are naturally accompanied by slow motion.
The approximations are expanded in terms of a small parameter $\varepsilon \sim v/c$, which expresses orders of deviation from Newton's laws of gravity. A correction of order $(v/c)^n$ to a Newtonian expression is said to be of $(n/2)\mathrm{PN}$ order. So for example the correction 3.5PN would contain terms up to the order $(v/c)^7$ \citep{will:2006}.

The post-Newtonian corrections are known for a general N-body system only up to the first order, 1.0PN. These corrections are described by the Einstein-Infeld-Hoffman (EIH) equations \citep{einstein:1938}, and the equations of motion are given by
\begin{align}
\mathbf{a}_{\mathrm{Newton},i} + \mathbf{a}_{\mathrm{1.0PN}, i} &= \sum_{j \neq i} \frac{G m_j \mathbf{n}_{ji}}{r_{ij}^2} + \frac{1}{c^2} \sum_{j \neq i} \left[ \vphantom{\sum_{k \neq j} \frac{G m_k}{r_{jk}}} v_i^2 + 2 v_j^2 - 4(\mathbf{v}_i \cdot \mathbf{v}_j) \right. \nonumber \\
&- \left. \frac{3}{2}(\mathbf{n}_{ij} \cdot \mathbf{v}_j)^2 - 4 \sum_{k \neq i} \frac{G m_k}{r_{ik}} - \sum_{k \neq j} \frac{G m_k}{r_{jk}} + \frac{1}{2} (\mathbf{r}_{ji} \cdot \mathbf{a}_j) \right] \nonumber \\
&+ \frac{1}{c^2} \sum_{j \neq i} \frac{G m_j}{r_{ij}^2} \left[ \mathbf{n}_{ji} \cdot (4 \mathbf{v}_i - 3 \mathbf{v}_j) \right] (\mathbf{v}_i - \mathbf{v}_j) \nonumber \\
&+ \frac{7}{2c^2} \sum_{j \neq i} \frac{G m_j \mathbf{a}_j}{r_{ij}} + \mathcal{O} \left( \frac{v}{c} \right)^4 \ . \label{equ:PN1Nbody}
\end{align}
Here $\mathbf{n}_{ij} = \mathbf{r}_{ij}/r_{ij}$ is the unit vector pointing from $i$ to $j$, and the big O notation indicates terms of the order $(v/c)^4$ and higher. One can see that the acceleration of a particular body depends on the accelerations of all the other bodies in the system, a new relativistic feature which does not appear in purely Newtonian systems. And since the accelerations that are being calculated also appear on the right hand side of the equation, the equations must be solved iteratively if exact solutions are required. However using the usual Newtonian accelerations on the right hand side of the equation in place of the PN corrected accelerations works as a good first approximation.

\subsection{Gravitational waves}

While the EIH equations (Eq. \eqref{equ:PN1Nbody}) give accurate results for example in our solar system, it is important to get more accurate approximations also for more extreme systems. General equations of PN corrections for a three-body system are known up to order 2.0PN \citep{gravity}. However, if we consider only a two-body system, PN corrections are known up to order of 3.5PN due to the relative simplicity of the situation. An extremely interesting effect arises from these later terms. The first three integer terms 1.0PN, 2.0PN, and 3.0PN are conservative, and we can find expressions for the total energy and angular momentum of the system. However the 2.5PN and 3.5PN corrections contain dissipative terms, and when these are taken into account the total energy and angular momentum of the system is not conserved. This is called the radiation reaction effect, and these losses are due to the emission of gravitational waves (GW) \citep{gravwaves}. After 3.5PN corrections, even the integer terms start contributing to the radiation reaction. Gravitational waves are generated by accelerating masses, and can be considered wave like disturbances in the curvature of spacetime. This effect causes the orbit in the two-body problem to shrink and circularize as energy is radiated away from the system. Gravitational waves play a major role for example in the final phases of a black hole binary merger.

The 2.5PN term contains the most dominant radiation reaction terms, and its effects on the system can be calculated by formulas derived by \cite{peters-mathews:1963}, which give the amount of energy and angular momentum the system loses, averaged over a single orbital period:
\begin{align}
\left< \frac{\mathrm{d} E}{\mathrm{d} t} \right> &= - \frac{32}{5} \frac{G^4 \mu^2 M^3}{c^5 a^5} \frac{1 + \frac{73}{24}e^2 + \frac{37}{96}e^4}{(1-e^2)^{7/2}} \\
\left< \frac{\mathrm{d} L_z}{\mathrm{d} t} \right> &= - \frac{32}{5} \frac{G^{7/2} \mu^2 M^{3/2}}{c^5 a^{7/2}} \frac{1 + \frac{7}{8}e^2}{(1-e^2)^2} \ .
\end{align}
Here $a$ and $e$ are simply the Keplerian semimajor axis and orbital eccentricity of the system, and $\mu$ and $M$ are the reduced and total mass of the two bodies. It should be noted that these formulas consider only the effects of the 2.5PN term, and not any of the other terms. \cite{peters:1964} also derived equations for the changes in the semimajor axis and eccentricity of the orbit as a result of these GW losses:
\begin{align}
\left< \frac{\mathrm{d} a}{\mathrm{d} t} \right> &= - \frac{64}{5}\frac{G^3 \mu M^2}{c^5 a^3} \frac{1 + \frac{73}{24}e^2 + \frac{37}{96}e^4}{(1-e^2)^{7/2}} \label{equ:GWsemimajor} \\
\left< \frac{\mathrm{d} e}{\mathrm{d} t} \right> &= - \frac{304}{15} e \frac{G^3 \mu M^2}{c^5 a^4} \frac{1 + \frac{121}{304}e^2}{(1-e^2)^{5/2}} \ . \label{equ:GWeccentricity}
\end{align}
Since the terms are negative, the emission of gravitational waves shrinks and circularizes the orbit. One can also see that when the eccentricity is near 1 or as the semimajor axis approaches 0, these effects strongly increase. This makes sense, as in both of these cases the orbiting body passes close to the other massive body in deep plunging orbits, entering very deep into the gravitational well and having high orbital velocity.

These correction terms which are higher than 1.0PN cause important effects and allow us to examine more extreme systems without using full GR. Thus it is important to include them in large-scale simulations. One method is to use what is known as the two-body formulation, where we calculate the PN corrections for each particle pair separately, where at least one of the particles is a black hole. PN corrections are not expected to be of major significance for interactions between stellar, gas, or dark matter particles. Another method is the cross-term formulation \citep{will:2014}, which is an approximation of the EIH equations (Eq. \eqref{equ:PN1Nbody}) for the case when a single large mass dominates the system. This is computationally expensive, so it only works in situations where there are at most hundreds of particles.

\subsection{The first application of PN corrections} \label{sect:PNapplication}

The first use of a PN expansion (to the first order) was made by Einstein himself in calculating the perihelion precession of the orbit of Mercury.
Mercury's perihelion precesses (rotates) around the Sun. This is mainly due to perturbations caused by the presence of the other planets. Another much less significant factor is the oblateness of the Sun. In the 1800s it was noticed that the orbit deviates from the precession predicted by these Newtonian effects by about 43 arc seconds per one tropical century.
Several different reasons for this deviation were proposed, and perhaps the most prevalent of these was Vulcan. Vulcan was believed to be a small hypothetical planet that was supposed to orbit the Sun inside the orbit of Mercury. The perturbations caused by this planet could have explained the anomalous precession of Mercury. 
%The existence of the planet was hypothesized by the French mathematician Urbain Le Verrier, who also predicted the existence and position of Neptune with only mathematics, so his theory was thought plausible. While many people claimed to have observed the planet, its supposed orbit was so close to the sun that this was extremely challenging to verify.
There were attempts to confirm the existence of the planet, but in 1915 the hypothetical planet was quite firmly put to rest when Einstein explained the excess precession through his theory of relativity. General relativity gives the equation for the advance of the perihelion of a planet in our solar system in radians per a single orbit:
\begin{equation}
\langle \dot{\omega} \rangle = \frac{6 \pi G M}{c^2 a (1-e^2)} \ . \label{equ:pericenterShift}
\end{equation}
Here $M$ is the total mass of the system, $a$ is the semimajor axis of the orbit of the planet, and $e$ is the orbital eccentricity \citep{will:tegp}.

The reason why the same extra precession that affects all of the planets had been observed only in Mercury is that the magnitude of the differences from simple Newtonian theory diminishes rapidly as one gets farther from the Sun. As can be seen from equation \eqref{equ:pericenterShift}, the rate of the precession increases as eccentricity approaches 1 and as the semimajor axis gets shorter. While Mercury's orbit is more eccentric than for example the orbits of Venus and Earth, this difference in eccentricity is not a major cause in the different precession rates since the value for eccentricity is squared in equation \eqref{equ:pericenterShift}, diminishing its effect. Mercury being close to the Sun is the main reason for the noticeable perihelion shift.
This was the first case of solving the general relativistic two-body problem, which is the most common use of the PN expansion nowadays.
%Hyvää matskua: gravity postminkowskian implementation sivu 10, gravity postnewtonian fundamental heti alku, common reader 3.2
%In the case of a two-body system, the post-Newtonian corrections give rise to a perturbed Keplerian orbit. The only secular effect on the orbit is the pericenter advance.
%PN1 terms are the ones responsible for the advance of the pericenter of an eccentric orbit, given by $\dot{\omega} = 6 \pi f_\mathrm{b} m/a (1-e^2)$, where $a$ and $e$ are the semimajor axis and eccentricity of the orbit, and $f_\mathrm{b}$ is the orbital frequency, given by Kepler's third law $2 \pi f_\mathrm{b} = (m/a^3)^{1/2}$ \citep{will:2006}. 
%
%The intrinsic angular momentum (spin) of a body is also a source of gravity that affects the metric and body motions. The spin causes precession in the ascending node. %TODO lisää kaavaa, gravity kalvoista

There are some systems that cannot be properly described by a post-Newtonian approximation because of their extreme conditions. Some examples of such systems include the final phases of a compact object merger, the cores of supernovae, and the structure of rapidly rotating neutron stars. These must be analysed using different methods, for example the full solution of Einstein's equations using numerical methods \citep{will:2006}. 

\section{Black hole mergers}

Galaxy formation is believed to be a hierarchical or ``bottom-up'' process, where smaller systems interact and merge multiple times, forming ever larger systems \citep{bt-galdyn}. This hierarchical model will also logically result in the central supermassive black holes that most galaxies host interacting with other black holes during the merger events. The black holes will first form binaries and then eventually merge with each other.

There are three main phases that the black holes go through before their eventual merging. At first, when the galaxies start merging, the black holes start to sink into the center of the combined galaxy system due to dynamical friction. They will eventually form a gravitationally bound binary system. The semimajor axis of the bound system depends on the masses of the black holes. For black holes with masses of $\sim 10^9 \, M_\odot$, the semimajor axis will be $\sim 100$ pc \citep{rantala:2018}. Next the semimajor axis of the binary will shrink, also known as hardening, due to stars interacting with the binary. In this process stars are ejected, carrying away angular momentum and energy from the system. Finally, once the binary is close enough together, it will start to noticeably lose energy due to the emission of gravitational waves, until their eventual coalescence. This will start happening at a semimajor axis of $\sim 0.1$ pc for the aforementioned masses \citep{rantala:2018}. These different regimes will be discussed in the next sections.

\subsection{Dynamical friction}

When galaxies merge, the interactions between their stars cause the stars to transfer momentum and kinetic energy from their relative orbital motion to random motion in the galaxy, which is known as violent relaxation. This leads to the orbital decay of the stars and causes the stellar matter to sink into the system. This process, known as dynamical friction, applies to large clusters of stars or even a single massive particle, like a black hole, moving through a larger host system \citep{bt-galdyn}. It is important to note that dynamical friction is not caused by actual collisions, only by gravitational interactions between the particles.

An intuitive way of understanding the process is to think of a body of mass $M$ travelling through a population of particles of mass $m_a$, where $m_a \ll M$. We call the mass $M$ the subject body and the smaller particles of mass $m_a$ field stars. We assume that the field stars are members of a large host with a total mass much greater than $M$, so that it can be approximated as a nearly infinite and homogeneous system. Thus when the subject body moves through the sea of field particles, it attracts the smaller particles through gravity, and leaves behind a so called gravitational wake. This higher concentration of mass behind the particle compared to the mass in front of the particle causes a drag force that slows down the subject body. This is illustrated in figure \ref{fig:dynfric}.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.33]{../images/dynfric.pdf}
\caption{A simplified image showing the basic mechanism behind dynamical friction. The subject body $M$ attracts the field particles towards itself as it travels, which creates a concentration of mass behind it. This creates a dynamical friction force $F_\mathrm{df}$ which slows down the subject body.}
\label{fig:dynfric}
\end{figure}

\cite{chandrasekhar:1943} derived a formula for the resulting force on the subject body by this effect. This force is expressed as
\begin{equation}
\mathbf{F}_{\mathrm{df}} = M \frac{\mathrm{d} \mathbf{v}_M}{\mathrm{d} t} = -16 \pi^2 G^2 M^2 m_a \mathrm{ln} \Lambda \left[ \int_0^{v_M} v_m^2 f(v_m) \mathrm{d}v_m \right] \frac{\mathbf{v}_M}{v_M^3} \label{equ:dynamicalfriction}
\end{equation}
and is called Chandrasekhar's dynamical friction formula. Here $f(v_m)$ is the distribution function of the field particles. A distribution function gives the number of particles per phase space volume having approximately the specified velocity near a certain position. Chandrasekhar's formula assumes that the field stars have an isotropic velocity distribution, and thus position does not affect the distribution function. The function is normalized so that integrating it over all the velocities gives the number density of particles near the subject body. The dynamical drag force always opposes the motion of the subject body, similarly to ordinary frictional drag.

The logarithm $\mathrm{ln} \Lambda$ is known as the Coulomb logarithm, and if we approximate that the particles are point masses, it is defined as
\begin{equation}
\mathrm{ln} \Lambda \approx \mathrm{ln} \left( \frac{b_{\mathrm{max}}}{b_{90}} \right) \ . \label{equ:coulomblog}
\end{equation}
Here $b$ means the impact parameter of a two-body encounter, defined as the shortest distance between the two particles if they passed each other without any interaction. The parameter $b_{\mathrm{max}}$ is the maximum impact parameter considered, and $b_{90}$ is the impact parameter at which the direction of the orbit of the passing particle will be deflected by $90^{\circ}$. This latter parameter can be expressed as
\begin{equation}
b_{90} = \frac{G(M+m_a)}{v^2_{\mathrm{typ}}} \ ,
\end{equation}
where $v_{\mathrm{typ}}$ is the typical encounter speed between the particles. Since we assumed that $M \gg m_a$, we can approximate that $M+m_a \approx M$. This gives the Coulomb logarithm in the form of
\begin{equation}
\mathrm{ln} \Lambda \approx \mathrm{ln} \left( \frac{b_{\mathrm{max}} v^2_{\mathrm{typ}}}{GM} \right) \ .
\end{equation}
This logarithm basically describes the cut-off of the friction effect at the maximum impact parameter \citep{galform}.

If the subject mass moves slowly, so that we can assume that $v_M$ is small, the field star distribution function can be approximately replaced with $f(0)$. In this case the integral is easy to evaluate, as we can take the function out of the integral and we are left with $\int_0^{v_M} v_m^2 \mathrm{d} v_m = v_M^3/3$. From this we get the following form for equation \eqref{equ:dynamicalfriction}: 
\begin{equation}
\mathbf{F}_{\mathrm{df}} = -\frac{16 \pi^2}{3} G^2 M^2 m_a \, \mathrm{ln} \Lambda \, f(0) \mathbf{v}_M \ . \label{equ:dynfricslow}
\end{equation}
Very interestingly, if instead the subject body moves at a high velocity, so that $v_M \gg v_m$, this behaviour changes. We can assume that the subject body moves at a higher velocity than all the surrounding field particles, so we are effectively integrating over all the velocities of the field particles. If we move $4\pi$ inside the integral, we have $\int_0^{v_M} 4 \pi v_m^2 f(v_m) \mathrm{d}v_m$, where $4 \pi v_m^2$ is a shell in velocity space. Since $v_M$ is large we are integrating over the whole velocity volume, and as it was mentioned earlier the distribution function is normalized to give the number density $n$ of particles near the subject body. So from the equation \eqref{equ:dynamicalfriction} we get
\begin{equation}
\mathbf{F}_{\mathrm{df}} = -4 \pi G^2 M^2 m_a n \, \mathrm{ln} \Lambda \, \frac{\mathbf{v}_M}{v_M^3} \ . \label{equ:dynfricfast}
\end{equation}
Here the mass density $m_a n$ can be replaced with $\rho(< v_M)$, the overall density of the field particles moving slower than the subject body \citep{bt-galdyn}.

From equations \eqref{equ:dynfricslow} and \eqref{equ:dynfricfast} we can see that unlike with hydrodynamic friction, where the drag always increases as the velocity increases, the situation here is more complicated. At sufficiently slow velocities dynamical friction is relative to the velocity of the subject body, $F_{\mathrm{df}} \propto v_M$. This is similar to ordinary drag where the force increases with velocity. But at larger velocities the frictional force begins to fall off, $F_{\mathrm{df}} \propto v_M^{-2}$. And as the mass density of the field stars can be replaced with the overall density, the force is independent of the individual masses of the field particles, so the formula is also valid for a field with distributed particle masses.

Chandrasekhar's formula makes several unrealistic assumptions when applied to a real astrophysical system. It assumes that all of the particles are point masses, it does not take into account the self-gravity of the wake left by the subject body, and it assumes a homogeneous and infinite distribution of the field particles, meaning that the choice of $b_\mathrm{max}$ and $v_\mathrm{typ}$ are a bit arbitrary in real scenarios. Despite these assumptions, Chandrasekhar's formula gives good approximations for the drag effect on a supermassive black hole moving through a field of stars. Taking these effects into account leads to an orbital decay time that is roughly twice as long compared to the estimate from Chandrasekhar's formula \citep{weinberg:1989}.

The point mass approximation can be corrected by using the half-mass radius $r_h$, which is the radius of the subject body that contains half of its total mass, in place of $b_{90}$ in equation \eqref{equ:coulomblog} \citep{bt-galdyn}. If the closest approach of a field star to the center of the subject body is $\lesssim r_h$, the orbit of the star will be less affected and thus its contribution to the drag force will be smaller than if the subject body was a point mass. Conversely the total drag force is largely unaffected if $r_h \lesssim b_{90}$. Thus using the half-mass radius gives a good estimate for the dynamical friction in case the subject body is not considered to be a point mass.

The fact that the exact values for the parameters in the Coulomb logarithm are hard to determine does not cause as much error as one might imagine. The ratio between $b_\mathrm{max}$ and $b_{90}$ is typically quite large. However we take the logarithm, so the end result is much smaller and not very sensitive to changes of order unity in the parameters. 

Finally, if we want take self-gravitation of the system into account as well, we need to use a more rigorous method such as linear response theory. Instead of considering only independent two-body interactions, the subject body is considered to be a moving external potential which causes a response density in the host system. In turn, the gravitational effect of this response density causes a drag effect on the original potential. This drag force is dynamical friction \citep{galform}. 


\subsection{Three-body scattering}

As the black holes sink close to the center of the newly merged galaxy system, they will eventually reach a point where they form a gravitationally bound binary. The distance where this happens is dependent on the masses of the black holes, but for black holes of masses $\sim 10^9 \, M_\odot$, the semimajor axis of the system is $\sim 100$ pc \citep{rantala:2018}, and the black hole orbital velocities exceed the local stellar velocity dispersion. At this point the dynamical friction formula is no longer valid, and instead the evolution of the binary is mainly determined by its interactions with the surrounding stellar population. The binary hardens, i.e. its separation shrinks, by losing kinetic energy to stars that pass close to it through complex three-body interactions, similar to the gravitational slingshot mechanisms used to accelerate spacecraft in our solar system. This hardening happens at a nearly constant rate of
\begin{equation}
\frac{\mathrm{d}}{\mathrm{d} t} \left( \frac{1}{a} \right) \propto \frac{G \rho}{\sigma}  \ ,
\end{equation}
if we assume a constant stellar density $\rho$ and velocity dispersion $\sigma$ \citep{quinlan:1996}. Here $a$ is the semimajor axis of the bound system. These stars can be ejected from the system at high velocities, spreading the central stars to wider orbits. The velocity of the ejected bodies is typically comparable to the circular orbital velocity of the binary
\begin{equation}
v_\star \sim V_{\mathrm{bin}} = \sqrt{\frac{2 G M_\bullet}{a}} \ ,
\end{equation}
where $a$ is the semimajor axis of the binary and $M_\bullet$ is the mass of the binary. The range of ejection velocities is broad, and some stars can even be ejected at high enough velocities to escape the potential of the galaxy, over $1000$ $\mathrm{km/s}$ for a $\sim 10^9 M_\odot$ binary \citep{rantala:2018}. These are so-called high velocity stars (HVS).

This analysis fails however if the stellar population surrounding the binary is not actually constant. For a star to be able to get close enough to interact strongly with the binary, it has to have angular momentum of $L \lesssim \sqrt{G(M_1 + M_2)a}$, where $M_1$ and $M_2$ are the masses of the black holes, and $a$ is the semimajor axis \citep{bt-galdyn}. The region of stars with low enough angular momentum is called the loss cone. As the stars that interact with the binary are ejected out of the system, the loss cone is slowly drained of stars. If the loss cone is completely emptied, the binary cannot harden any more, and thus its evolution is halted. This is the so called final-parsec problem, and there must be some mechanisms to refill the loss cone if black hole mergers do happen.

One clear candidate for this is two-body relaxations between stars which cause them to end up inside the loss cone \citep{milosavljevic:2003}. In non-spherical galaxies torques could drive gas to the central region of the galaxy, which would also help the hardening process \citep{mayer:2007}.
Thus the final parsec problem is more problematic in spherical galaxies where the mass inside the loss cone is the smallest due to stars not being able to be on as varied orbits as in non-spherical galaxies, and there are no torques to drive gas inwards. In triaxial and axisymmetric galaxies however the asymmetric perturbations are able to cause stars to end up on centrophilic orbits that would refill the loss cone. If the host of the binary were to undergo another merger while the black holes are in the process of merging, the perturbations caused by this could break the  symmetry of galactic potentials, and efficiently refill the loss cone.

N-body simulations of non-spherical systems suggest that the final parsec problem is not as severe as one might think. The problems has been found to not hamper the hardening of binaries in triaxial \citep{berczik:2006} and axisymmetric galaxies \citep{khan:2013}.
The simulations however are not completely accurate. Approximating a galaxy with $N \gtrsim 10^9$ stars by a model consisting of only around $N \lesssim 10^6$ particles leads to enhanced two-body relaxation timescales. Thus the loss cone is much less depleted than in real galaxies \citep{milosavljevic:2003}. This problem could be alleviated with simulations capable of handling a much larger amount of particles. Simulations with $N \sim 10^7 - 10^8$ particles could reproduce a more realistic, diffusive behaviour of a galaxy.

\subsection{Gravitational waves}

If the black hole binary overcomes the final-parsec problem and reaches a sufficiently small distance, it will lose enough energy through the emission of gravitational waves to eventually coalesce in less than the Hubble time \citep{milosavljevic:2003}. This separation is $\sim 0.1$ pc for a binary with a mass of $\sim 10^9 \, M_\odot$ \citep{rantala:2018}. Hubble time is the time it would take for the universe to expand to its current state if it expanded at a constant Hubble rate. The rate at which the semimajor axis shrinks due to the radiation reaction effect can be estimated with the Peters and Mathews equation \eqref{equ:GWsemimajor}. The evolution of the semimajor axis $\dot{a} \propto -a^{-3}$, and if we assume a constant eccentricity we can approximate the coalescence timescale with \citep{bt-galdyn}
\begin{equation}
t_c \sim -\frac{a}{4\dot{a}} \ .
\end{equation}

The longest evolutionary phase for a black hole binary is when the radius of the bound system is below that at which it becomes hard, but still above the radius at which gravitational waves become prominent \citep{begelman:1980}. This is known as the bottleneck radius, and it marks the area where binary black holes are most likely observed. The decay time at this bottleneck radius is uncertain, and depends on both the population of the loss cone and how much gas affects the evolution of the binary \citep{bt-galdyn}. If the coalescence time is very long, over 10 Gyr, we would expect to be able to observe these binaries at the centers of most galaxies. If the timescale is shorter, we could observe the coalescence events happening.

Current observational evidence suggests that long-lived supermassive black hole binaries might be quite rare. Though there are many implied candidates based on periodic variability, no concrete observations of a supermassive black hole binary have been made. If both of the supermassive black holes of a binary are quasars and they are very close to each other, it is difficult to tell them apart from a singular active galactic nuclei \citep{roedig:2014}. In the future observations made by the LIGO and LISA gravitational wave detectors might give concrete proof of the merging and thus existence of supermassive black hole binaries, and we might be able to even make interferometric direct observations of a binary black hole.

\chapter{AR-CHAIN} \label{chap:ar-chain}
%TODO kerronko introssa siitä et miks simulaatiota tehään, modaa sen perusteella mitä tässä sanotaan, kuinka paljon taustaa. 
%Selitä et gadget on muuten hyvä paitsi että, ja miks sit ketju on jebin

One of the most widely used software packages for galaxy simulations is the smoothed particle hydrodynamics simulation code GADGET-3 \citep{springel:2005}. It can be used for effectively simulating the global dynamics of galaxies in galaxy mergers and cosmological simulations. Because the number of particles that is feasible to simulate on these scales is orders of magnitude smaller than the number of real particles, e.g. stars, there is a need to account for the fact that these simulated particles are not real individual objects but rather represent large clumps of the real particles. Because of this limitation these objects need to be ``softened'': the Newtonian potential of a point particle, $-GM/r$, is replaced with some non-point potential, for example the Plummer potential
\begin{equation}
\phi = -\frac{GM}{(r^2 + \epsilon^2)^{1/2}} \ ,
\end{equation}
where $\epsilon$ is the chosen softening length. The force associated with this potential is
\begin{equation}
\mathbf{F(r)} = -\frac{GMm \mathbf{r}}{(r^2 - \epsilon^2)^{3/2}} \ .
\end{equation}
This softening essentially makes the particles non-point masses \citep{zhang:2019}. One limitation of GADGET-3 is that due to the use of softening, it cannot resolve very small-scale effects below the softening scale.

To this end \cite{rantala:2017} developed a new code, which is an extension of GADGET-3 called KETJU. The code implements algorithmically regularized regions around supermassive black holes to allow for solving the detailed dynamics of these systems, while still being able to simulate the rest of the galaxy with GADGET-3. Algorithmic regularization means that the difficult close encounters and collisions of point masses in a dynamical simulation are treated via a time transformation, while not actually removing the collisional singularity from the equations of motion.

KETJU utilizes the algorithmic regularization chain, or AR-CHAIN, method \citep{mikkola:2002, mikkola:2006,mikkola:2008b}, which combines algorithmic regularization with an older Kustaanheimo-Stiefel chain method \citep{mikkola:1993}. Since close encounters of point masses are not a problem due to algorithmic regularization, no softening is used in AR-CHAIN.

The AR-CHAIN method is comprised of three important parts: algorithmic regularization, the use of a chained coordinate system to reduce roundoff errors, and the Gragg-Bulirsch-Stoer extrapolation method for attaining a very high numerical integration accuracy. These three parts will next be discussed separately in more detail. Lastly there will be an overview of the actual implementation of this method.


%Koodi flowchartti jotenki

%Kuva leapfrogista
%Mukaileva kuva antin BS esimerkistä?


\section{Algorithmic regularization}

The main goal of the algorithmic regularization method is to be able to deal with close encounters of particles in dynamical systems without having to resort to more computationally expensive regularization methods, such as the Kustaanheimo-Stiefel regularization. This can be achieved via two slightly different time transformations. 

The first approach is called the Logarithmic Hamiltonian (LogH) method, which was discovered by \cite{mikkola:1999} and \cite{preto:1999}. To guarantee simulation accuracy, it is desirable to be able to use a symplectic integration method, i.e. a method which constrains the allowed motions in phase space strongly enough so that the usual tendency of numerical orbit integrations to drift in invariants, such as energy or angular momentum, is absent. In practice a symplectic integrator conserves the energy of the system. The leapfrog integrator is one of the most commonly used symplectic integrators.

\subsection{Leapfrog}
A leapfrog can be used for numerically integrating second order differential equations of the form $d^2x/dt^2 = F(x)$, equivalently $dx/dt = v , \ dv/dt = F(x)$. It has acquired its name from the way it updates the positions and velocities of the system at interleaved time points, so that they ``leapfrog'' over each other. This is illustrated in figure \ref{fig:leapfrog}.
\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/leapfrog.pdf}
\caption{An illustration of the leapfrog method. Positions and velocities are updated at alternating time points so they leapfrog over each other. }
\label{fig:leapfrog}
\end{figure}
The leapfrog can also be turned into a synchronized form where both the position and velocity are obtained at integer steps. In this form the integration can be performed in two different ways, depending on whether we first update the position, which is known as drifting, or the velocity, which is known as kicking. If we update the position first it is called the ``Drift-Kick-Drift'' (DKD) version, and conversely if we update the velocity first it is called the ``Kick-Drift-Kick'' (KDK) version. The equations for a DKD version are
\begin{equation}
\begin{aligned}
x_{i+1/2} &= x_{i}+v_{i} \frac{\Delta t}{2} \\
v_{i+1} &= v_{i}+F(x_{i+1/2})\Delta t \\
x_{i+1} &= x_{i+1/2}+v_{i+1}{\frac {\Delta t}{2}} \ ,
\end{aligned}
\end{equation}
where $x_i$ and $v_i$ are the values at step $i$, and $\Delta t$ is the timestep. In a dynamical system the function $F(x)$ is simply the acceleration. First the position is updated by half a timestep, then the velocity is updated based on this half step value, and finally the position is updated again so that the whole system has advanced by one timestep. In addition to its symplectic nature, the leapfrog method is also time reversible, meaning that we can accurately integrate the system backwards in time.

However the symplectic nature is only preserved when we use fixed timesteps, and using varying timesteps causes the leapfrog to no longer be symplectic \citep{bt-galdyn}. The use of a fixed timestep is not feasible in most simulations since very different densities are resolved, requiring very different timesteps to allow the simulation to be completed in a reasonable time. This can be circumvented for example with time transformations, so that the time variable changes during close encounters, and we do not actually need to change the timestep itself. Another example is to use extremely accurate extrapolation methods to keep the errors very small, so the loss of symplecticity does not matter as much.

\subsection{Logarithmic Hamiltonian}
To be able to use an explicit symplectic method such as the leapfrog, we need to have a Hamiltonian that is separable, i.e. where the different parts contain only one type of canonical coordinates.
For an N-body system such a Hamiltonian is given by
\begin{equation}
H = \sum_i^N \frac{1}{2}m_i \boldsymbol{v}_i^2 - \sum_i^N \sum_{j>i}^N \frac{G m_i m_j}{r_{ij}} = T - U \ ,
\end{equation}
where $T$ is the kinetic energy and $U$ is the so-called force function, which is the negative of the potential energy. The binding energy $B$ of the system is thus \mbox{$B = -H = U-T$}. If we apply the time transformation
\begin{equation}
\mathrm{d}s = U \mathrm{d}t \ ,
\end{equation}
where $s$ is the new fictitious time variable, we can get a new Hamiltonian through the Poincaré time transform discussed in section \ref{sect:2dregu}:
\begin{equation}
\Gamma = g (H+B) \ ,
\end{equation}
where $B$ is the binding energy and $g$ is the time transform function that is derived from $\mathrm{d}t = g \mathrm{d}s$. 
Thus the time transformed Hamiltonian is
\begin{equation}
\Gamma = \frac{T-U+B}{U} \ . \label{equ:loghGamma}
\end{equation}
This Hamiltonian is not yet separable. However, because along the correct orbit $\Gamma = 0$ since the Hamiltonian equals the energy of the system, we can use
\begin{equation}
\tilde{\Gamma} = \mathrm{log}(1+\Gamma) \label{equ:loghNewGamma}
\end{equation}
as the new Hamiltonian \citep{mikkola:1999}. The equations of motion are equivalent for this new Hamiltonian.
We can turn equation \eqref{equ:loghGamma} into the form 
\begin{equation}
1 + \Gamma = \frac{T+B}{U} \ ,
\end{equation}
take the logarithm of both sides, and combine it with \eqref{equ:loghNewGamma} which gives us 
\begin{equation}
\tilde{\Gamma} = \mathrm{log}(T+B) - \mathrm{log}(U)
\end{equation}
which is in a separable form. Thus we have managed to form a time transformed Hamiltonian that is separable, and thus usable for an explicit symplectic integrator. 

\subsection{Time Transformed Leapfrog}
The second approach is called Time Transformed Leapfrog (TTL) method, developed by \cite{mikkola:2002}. This method relies on a time transformation
\begin{equation}
\mathrm{d}s = \Omega \mathrm{d}t \ ,
\end{equation}
where $\Omega$ is an arbitrary positive function of the coordinates, i.e. $\Omega = \Omega(\boldsymbol{r})$. In order to be able to write the equations of motion in a way that allows for the construction of a leapfrog algorithm, we also introduce an auxiliary variable $\omega$ whose value is equal to $\Omega$ at the start and is supposed to have the same numerical values as $\Omega$, but its new values are obtained from the differential equation
\begin{equation}
\frac{\mathrm{d}\omega}{\mathrm{d}t} = \boldsymbol{v} \cdot \frac{\partial \Omega}{\partial \boldsymbol{r}} \ .
\end{equation}
This allows us to formulate new time transformed equations of motion with these new variables.
%The variable $\omega$ is considered to be in the same category as the velocities $\boldymbol{v}$.

These variables $\Omega$ and $\omega$ can be chosen so that their use ensures that the least massive particles in the system have a non-negligible effect on the time transformation, even if their contribution to the potential energy might be small. Based on \cite{mikkola:2008b}, one good definition for $\Omega$ in an N-body system is 
\begin{equation}
\begin{aligned}
\Omega &= \sum_i^N \sum_{j>i}^N \frac{\Omega_{ij}}{r_{ij}} \\
\Omega_{ij} &=
\begin{cases}
\tilde{m}^2, &\mathrm{if} \ m_im_j < \epsilon_{\Omega} \tilde{m} \\
0  &\mathrm{otherwise}
\end{cases} \\
\tilde{m} &= \sum_i^N \sum_{j>i}^N \frac{2m_i m_j}{N(N-1)} \ .
\end{aligned}
\end{equation}
Here $\tilde{m}$ is the mean mass product of the system, and the mean mass epsilon $\epsilon_{\Omega}$ is defined by the user, and a typical value is of the order $\sim 10^{-3}$. This way only particles with low enough mass affect the value of $\Omega$, so calculating the value for the time transform function in this manner guarantees the proper treatment for even the smaller bodies in the system, as they will not be overshadowed by the more massive particles.

The LogH and TTL methods can be combined into a single time transform function defined as 
\begin{equation}
ds = [\alpha U + \beta \Omega + \gamma] dt = [\alpha (T + B) + \beta \omega + \gamma] dt \ .
\end{equation} \label{equ:regutimetransform}
The two definitions are identical for the exact solution. Here $\alpha$, $\beta$, and $\gamma$ are user-defined parameters which determine which kind of regularization will be used. One can also easily see that if we define $\Omega = U$, the logarithmic Hamiltonian and the time transformed leapfrog methods are mathematically equivalent \citep{mikkola:2008b}. Numerically they are not the same however, due to roundoff errors in updating the value of $\Omega$. 

Choosing $(\alpha, \beta, \gamma) = (1,0,0)$ gives us the logarithmic Hamiltonian method, $(\alpha, \beta, \gamma) = (0,1,0)$ gives the time transformed leapfrog, and $(\alpha, \beta, \gamma) = (0,0,1)$ includes no time transformation at all, and thus it corresponds to the regular leapfrog method. \cite{mikkola:2006} suggest a possible choice of $(\alpha, \beta, \gamma) = (1,\mathrm{small},0)$, which corresponds to a mix of LogH and TTL. This would be desirable for systems with very large particle mass ratios, since this way the value of $\Omega$ would also affect the time transformation, so the smallest masses are not ignored when choosing the timestep. 

The time transformed equations of motion of an N-body system can now be written for both the coordinates
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha (T + B) + \beta \omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{r}_i}{\mathrm{d}s} &= t' \boldsymbol{v}_i
\end{aligned} \label{equ:tteompositions}
\end{equation}
and the velocities
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha U + \beta \Omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{v}_i}{\mathrm{d}s} &= t' \left[ \sum_{i \neq j}^N \frac{G m_j \boldsymbol{r}_{ij}}{r_{ij}^3} + \boldsymbol{f}_i \right] \\
\frac{\mathrm{d} \omega}{\mathrm{d}s} &= t' \sum_i^N \frac{\partial \Omega}{\partial \boldsymbol{r}_i} \cdot \boldsymbol{v}_i \ .
\end{aligned} \label{equ:tteomvelocities}
\end{equation}
%\frac{\mathrm{d} B}{\mathrm{d}s} &= -t' \sum_i^N m_i \boldsymbol{v}_i \cdot \boldsymbol{f}_i \\
Here $\boldsymbol{f}_i$ are any possible external perturbing accelerations that might affect the system. The time transformations are both denoted with $t'$ since as seen in equation \eqref{equ:regutimetransform} they are both identical for the exact solution. However in practice computational roundoff errors cause them to differ slightly. They are calculated in different ways so that new positions are obtained by using only velocities, and vice versa.

\cite{mikkola:2008b} concluded that using the choice of $(\alpha, \beta, \gamma) = (1,0,0)$, i.e. pure logarithmic Hamiltonian method, gives the most accurate results for the integration. Despite this the time transformed leapfrog method and the variables $\Omega$ and $\omega$ are not useless, since monitoring these variables can help identify encounters of light particles, and provides the integrator a way to forcibly lower the timestep if needed.

By using a time transformation we do not experience any catastrophic errors in the case of close-by two-body encounters since the integration with the leapfrog is exact even on collision orbits, as long as the system is not evaluated exactly during the collisions \citep{mikkola:1999}. The algorithm conserves energy down to numerical precision, and the only error is a phase error for time, which scales proportionally to $\mathcal{O}(\Delta t^3)$. This is a spectacular result for a method where the collisional singularity is not actually removed.



\section{Chain coordinate system} \label{sect:ChainSystem}

Computers use floating-point arithmetic to represent real numbers, so rounding errors are inevitable when doing calculations with such numbers. Rounding errors are especially prevalent when subtracting two very large numbers whose values are very close to each other, for example a close encounter of a pair of particles if their coordinates are measured from a distant origin. Reducing this roundoff error as much as possible is important in order to ensure that the calculations and thus the simulations are correct. Reducing this error can be achieved via introducing a new chained coordinate system, where the position of a particle is defined by its relative position to a previous particle in the chain.

This method is similar to the Kustaanheimo-Stiefel chain regularization method \citep{mikkola:1993} described in section \ref{sect:KSmethod}, where the system is not globally KS regularized. Instead we do the KS transform only for the shortest interparticle vectors in the system. The difference is that in AR-CHAIN we do not perform a KS transformation, instead we only link all the particles together in a chain. \cite{mikkola:1999} pointed out that without using this kind of chain structure, the numerical roundoff when calculating the forces is too large for satisfactory results. Thanks to this structure, the separations of particles that are near each other can be derived from the chain, so we do not need to calculate them by doing the error-prone long-range subtraction.
The chain structure can be directly used with the transformations discussed in the previous section, since the chain affects only the positions $\boldsymbol{r}$ and velocities $\boldsymbol{v}$ in equations \eqref{equ:tteompositions} and \eqref{equ:tteomvelocities}, not the time transformation itself.

Before constructing the chained system we need to move the system into a center of mass (CoM) frame. The position and velocity of the CoM of the dynamical system are defined as
\begin{equation}
\begin{aligned}
\boldsymbol{r}_{cm} &= \frac{\sum_\mathrm{i}^\mathrm{N} m_i \boldsymbol{r}_i}{\sum m_i} \\
\boldsymbol{v}_{cm} &= \frac{\sum_\mathrm{i}^\mathrm{N} m_i \boldsymbol{v}_i}{\sum m_i} \ .
\end{aligned}
\end{equation}

The construction of the chain is quite straightforward, and it starts by first calculating all the interparticle distances in the CoM frame. These are pairwise symmetric, so if we have $N$ particles, we only need to calculate $N(N-1)/2$ vectors instead of $N^2$. We pick the shortest interparticle distance, and the two particles are defined as the ``head'' and ``tail'' of the chain. The chain is extended by adding to it the particle that is closest to either the head or tail of the chain, if it is not already a member of the chain. This added particle then becomes the new head or tail of the chain. This procedure is continued until no suitable particles remain outside of the chain. In figure \ref{fig:chain} one can see an example of a chained system consisting of 11 particles.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=1]{../images/chain.pdf}
\caption{An illustration of a chained coordinate system demonstrating the 10 interparticle position vectors between 10 stars and a supermassive black hole. As the closest interparticle distance in this system is between the black hole and the nearby star, the construction of the chain starts from there and then extends from both the head and tail until complete. The dashed line shows the shortest path between the two stars in the CoM frame, which is very different compared to the path when following the chain.}
\label{fig:chain}
\end{figure}

A system of $N$ particles will contain $N-1$ chained interparticle vectors. These chain coordinates $\boldsymbol{X}_k$ can be collected into the vector $\boldsymbol{X}$ so that
\begin{equation}
\begin{aligned}
\boldsymbol{X}_k &= \boldsymbol{r}_{j_k} - \boldsymbol{r}_{i_k} \\
\boldsymbol{X} &= (\boldsymbol{X}_1, \boldsymbol{X}_2, \ldots, \boldsymbol{X}_{N-1}) \ .
\end{aligned}
\end{equation}
Similarly for velocities
\begin{equation}
\begin{aligned}
\boldsymbol{V}_k &= \boldsymbol{v}_{j_k} - \boldsymbol{v}_{i_k} \\
\boldsymbol{V} &= (\boldsymbol{V}_1, \boldsymbol{V}_2, \ldots, \boldsymbol{V}_{N-1})
\end{aligned}
\end{equation}
and accelerations
\begin{equation}
\begin{aligned}
\boldsymbol{A}_k &= \boldsymbol{a}_{j_k} - \boldsymbol{a}_{i_k} \\
\boldsymbol{A} &= (\boldsymbol{A}_1, \boldsymbol{A}_2, \ldots, \boldsymbol{A}_{N-1}) \ .
\end{aligned}
\end{equation}
Here the vector labelling starts from the head of the chain. These chain vectors are used in the force calculations to compute the separations $\boldsymbol{r}_{ij}$ for particles that are near each other in the chain. For particles that are far away from each other in the chain structure, the separation is actually more accurately calculated by using for example the CoM frame, due to the accumulating error of summing many separation vectors. The separation is calculated using the chain if the difference in the chain indexes is not greater than the arbitrarily chosen limit $N_d$. If we have chain indexes $i$ and $j$, and we assume that $j > i$, the separations in the AR-CHAIN algorithm are calculated with
\begin{equation}
\boldsymbol{r}_{ij} =
\begin{cases}
\boldsymbol{r}_j - \boldsymbol{r}_i \ &\mathrm{if} \ j-i > N_d \\
\sum_{k=i}^j \boldsymbol{X}_k \ &\mathrm{if} \ j-i \leq N_d \ .
\end{cases}
\end{equation}
According to \cite{mikkola:2008b} a good value for this limit is $N_d = 2$. This method of selecting the way the spatial separations of given particles is calculated is one of the most important features of this algorithm.

Finally we can write down the suitable leapfrog equations of motion for the chained system \citep{mikkola:2008b}. For the coordinates the equations are
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha (T + B) + \beta \omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{X}_i}{\mathrm{d}s} &= t' \boldsymbol{V}_i \ ,
\end{aligned}
\end{equation}
and for the velocities
\begin{equation}
\begin{aligned}
\frac{\mathrm{d}t}{\mathrm{d}s} &= [\alpha U + \beta \Omega + \gamma]^{-1} \equiv t' \\
\frac{\mathrm{d}\boldsymbol{V}_i}{\mathrm{d}s} &= t' [ \boldsymbol{A}_i(\{ \boldsymbol{X}_i \}) + \boldsymbol{f}_i ] \\
\frac{\mathrm{d} \omega}{\mathrm{d}s} &= t' \sum_i^N \frac{\partial \Omega}{\partial \boldsymbol{X}_i} \cdot \boldsymbol{V}_i \ .
\end{aligned} \label{equ:VelocityEomChained}
\end{equation}
The $\boldsymbol{f}_i$ means any possible external perturbing accelerations that might affect the system, for example from particles that are nearby the chain structure but are not part of the chain itself.

\section{Gragg–Bulirsch–Stoer algorithm}

The use of a leapfrog alone is usually not accurate enough in dynamical simulations, so we want to include some extrapolation method for enhancing the accuracy and providing a way for error control. Gragg-Bulirsch-Stoer (GBS) extrapolation is a powerful method for solving differential equations with relatively little computational effort \citep{gragg:1965, bulirsch:1966}. The basic idea of the GBS extrapolation algorithm is to solve a differential equation with multiple different integrator substep amounts $n$, and then extrapolate the results for $n \rightarrow \infty$, i.e. extrapolate the result for a timestep of 0. Next we will go through the different parts of the GBS algorithm and how it is used in AR-CHAIN. This mainly follows the articles by \cite{gragg:1965} and \cite{bulirsch:1966}, and also the textbook by \cite{press:2007}.

A first-order ordinary differential equation (ODE) with a known initial value can be written as 
\begin{equation}
\frac{dy}{dx} = f(y), \ \ y(x_0) = y_0 \ .
\end{equation}
Similarly a second-order ODE with an initial value can be written as
\begin{equation}
\frac{d^2 y}{dx^2} = f(y), \ \ y(x_0) = y_0, \ y'(x_0) = z_0 \ ,
\end{equation}
where $y'$ signifies a derivative with respect to x. This can also be split into a coupled pair of first order equations:
\begin{equation}
\begin{cases}
\frac{dy}{dx} = z \\
\frac{dz}{dx} = f(y)
\end{cases}
y(x_0) = y_0, \ z(x_0) = z_0 \ .
\end{equation}
The equations of motion of a classical Newtonian system are a good example of this kind of second-order ODEs. 

If we want to simulate a dynamical system forwards in time by an interval $H$, we must solve this pair of equations. The results we will get are $y(x_0 + H)$ and $z(x_0 + H)$. This long interval can also be split into $n$ substeps of length $h$:
\begin{equation}
h = \frac{H}{n} \ .
\end{equation}

The GBS algorithm uses the Richardson extrapolation method \citep{richardson:1911,richardson:1927}. The idea is to consider the final answer of a numerical calculation as being an analytic function of some adjustable parameter, like the stepsize $h$. This analytic function can then be examined by performing calculations with various values of $h$, and once enough points have been calculated try to fit the function to these points. We can then evaluate the function at $h=0$, as if we had used an infinitely small timestep \citep{press:2007}. This idea is illustrated in figure \ref{fig:gbs}.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=1.4]{../images/gbs.pdf}
\caption{A visualization of Richardson extrapolation that is used in the GBS algorithm. A large time interval $H$ is split into sequences of smaller amounts of substeps, and their results are extrapolated to estimate an infinite number of substeps.
(Adapted from \citealt{press:2007})}
\label{fig:gbs}
\end{figure}

The choice for the type of fitting function is also important. \cite{bulirsch:1966} recognized that rational functions
\begin{equation}
R(x) = \frac{p_0 + p_1 x + \ldots + p_{\mu}x^{\mu}}{q_0 + q_1 x + \ldots + p_{\nu}x^{\nu}}
\end{equation}
offer an improvement over simple polynomial functions
\begin{equation}
P(x) = p_0 + p_1 x + \ldots + p_{\mu}x^{\mu}
\end{equation}
since rational functions are able to approximate functions with poles better than polynomial functions. Rational function fits also remain good approximations even with very large values of $h$. Despite this, it has been found that in problems with smooth functions polynomial extrapolation is more efficient and accurate than rational extrapolation \citep{press:2007}. Thus polynomial functions are used in AR-CHAIN.

We want to use an integrator whose error, when expressed as a power series in $h$, contains only even powers of $h$
\begin{equation}
|y_n - y(x_0 + H)| = \sum_{k=1}^{\infty} a_k h^{2k} \ .
\end{equation}
This was discovered by \cite{gragg:1965}. The importance of an even power series is that when combining the different substep amounts, the accuracy increases by two orders at a time. Fortunately the leapfrog integrator is applicable for this, since it has this kind of error behaviour.
%TODO sano jotenki paremmin

It is important to choose an optimal sequence of the number of substeps $n$ to be used in the GBS algorithm. \cite{bulirsch:1966} originally proposed the sequence of $n_k = 2n_{k-2}$ for $k > 2$:
\begin{equation}
n = 2,4,6,8,8,12,16,24,32,64, \ldots \ .
\end{equation}
However \cite{press:2007} suggest that a Deuflhard sequence of $n_k = 2k$
\begin{equation}
n = 2,4,6,8,10,12,14,16,18,20, \ldots \label{equ:deuflhard}
\end{equation}
is more efficient and has better convergence behaviour than the original GBS sequence. 

The GBS algorithm starts by solving the differential equations with the leapfrog for the different substep amounts. After each calculation the extrapolation is performed to obtain an estimate of the results $T_k = (y_k^{\mathrm{ext}}, z_k^{\mathrm{ext}})$. The algorithm checks for convergence with the criterion
\begin{equation}
\left| \frac{T_{k+1} - T_k}{T_k} \right| \leq \eta_{\mathrm{GBS}} \ ,
\end{equation}
where $\eta_{\mathrm{GBS}}$ is the user-defined GBS error tolerance parameter. Thus the timesteps are shortened until this criterion is met. This is one of the key elements for the algorithm resulting in such high accuracy results. We also set an upper limit for the value of $n$, beyond which it is deemed non-optimal to continue increasing the substep amount, since precision can be lost with too high substep numbers \citep{press:2007}. A good choice for the maximum amount of substep divisions is 8, which corresponds to $n = 16$ when using the Deuflhard sequence \eqref{equ:deuflhard} \citep{press:2007}. If this limit is reached without sufficient convergence, we deem the large timestep $H$ to be too long and its value is reduced, and the integration then starts over with the new smaller $H$.

The leapfrog used in AR-CHAIN is a DKD variant, and the the time evolution operator with $n$ timesteps used for the GBS algorithm can be written as
\begin{equation}
\mathbf{U}_n(H) = \mathbf{D} \left( \frac{H}{2n} \right) \left[ \mathbf{K} \left( \frac{H}{n} \right) \mathbf{D} \left( \frac{H}{n} \right) \right]^{n-1} \mathbf{K} \left( \frac{H}{n} \right) \mathbf{D} \left( \frac{H}{2n} \right) \ . \label{equ:GBSLeapfrog}
\end{equation}
So for example if $n=2$, we first drift for a quarter of the total long interval $H$, then we kick and drift for half of the total time, and then finally kick for the other half and drift for the last quarter. Thus the whole system has been advanced by the time interval $H$.

\subsection{Implementing post-Newtonian corrections in AR-CHAIN}

If we include post-Newtonian corrections $\boldsymbol{g}_i(\mathbf{v})$ in the chained equations of motion \eqref{equ:VelocityEomChained}, the equation for velocity becomes
\begin{equation}
\frac{\mathrm{d}\boldsymbol{V}_i}{\mathrm{d}s} = t' [ \boldsymbol{A}_i(\{ \boldsymbol{X}_i \}) + \boldsymbol{f}_i + \boldsymbol{g}_i(\mathbf{v})] \ .
\end{equation}
We can see that in this case the accelerations on the right-hand depend on the velocities that we are using on the left-hand side, so we cannot use the standard leapfrog for this kind of equations as leapfrog depends on having separable coordinates. The equations could be solved with implicit methods over each timestep, but this kind of iterative process would be computationally costly if we want to reach high accuracy.

Thankfully more efficient explicit methods have been developed, relying on further extensions of the phase-space of the dynamical system \citep{hellstrom:2010, pihajoki:2015}. We can introduce so-called auxiliary velocities $\mathbf{w}_i$ and corresponding chained auxiliary velocities $\mathbf{W}_i$. These auxiliary variables are set equal to the physical ones at the start of each timestep. Velocities are only used in the kicking phase of the leapfrog, so we can define the auxiliary and physical kick operators $\mathbf{K}_{\mathrm{aux}}$ and $\mathbf{K}_{\mathrm{phys}}$ as
\begin{align}
\mathbf{K}_{\mathrm{aux}}(\Delta t) &=
\begin{cases}
\mathbf{V}_i \rightarrow \mathbf{V}_i \\
\mathbf{W}_i \rightarrow \mathbf{W}_i + \Delta t [ \boldsymbol{A}_i(\{ \boldsymbol{X}_i \}) + \boldsymbol{f}_i + \boldsymbol{g}_i(\mathbf{v})]
\end{cases} \\
\mathbf{K}_{\mathrm{phys}}(\Delta t) &=
\begin{cases}
\mathbf{W}_i \rightarrow \mathbf{W}_i \\
\mathbf{V}_i \rightarrow \mathbf{V}_i + \Delta t [ \boldsymbol{A}_i(\{ \boldsymbol{X}_i \}) + \boldsymbol{f}_i + \boldsymbol{g}_i(\mathbf{w})]
\end{cases}
\end{align}
During the physical kick the physical velocities are updated using the auxiliary velocities in the post-Newtonian corrections $\boldsymbol{g}_i(\mathbf{w})$, and conversely for the auxiliary kick. The complete kick operator usable in the leapfrog becomes
\begin{equation}
\mathbf{K}(\Delta t) = \mathbf{K}_{\mathrm{aux}}(\Delta t/2) \mathbf{K}_{\mathrm{phys}}(\Delta t) \mathbf{K}_{\mathrm{aux}}(\Delta t/2) \ .
\end{equation}
This can be inserted into the time evolution operator defined in equation \eqref{equ:GBSLeapfrog}, which gives us a leapfrog sequence that is usable in the GBS algorithm and works even for velocity-dependent accelerations.

\section{AR-CHAIN code overview}

Now that we have all the pieces of the AR-CHAIN algorithm we can discuss the standalone implementation used for the simulations in this thesis. A flowchart of the main steps of the program can be seen in figure \ref{fig:archain}.

\begin{figure}[h!tb]
\centering
\includegraphics[width=\textwidth]{../images/archain.pdf}
\caption{A simplified flowchart of how the standalone AR-CHAIN code progresses.}
\label{fig:archain}
\end{figure}

The AR-CHAIN code first prepares the test scenario we want to examine. In addition to creating the initial conditions, we can determine what kind of PN corrections we want to use. We can set the maximum PN order, and whether PN corrections should be taken into account only for BH-BH interactions, or also for BH-star interactions. If the star particles are set to be much more massive than physical stars actually are, this could lead to unphysical results. We can also set the desired error tolerance for the GBS algorithm.

After this the chain is initialized and built. All the particles are moved to CoM frame, and we use the chain algorithm described in section \ref{sect:ChainSystem} to construct the chain structure. All the chain members are assigned an index number, which is used for all the calculations between the members. In addition we calculate the variables of the chain structure, such as its binding energy, and the accelerations of all the member particles.

After all this preparatory work is done, we can start propagating the chain. The system will be propagated for a given time, and it will print out all the particle data after every given printing interval. The initial integration timestep for the whole system is determined from the starting parameters. Appropriate timesteps for all particle pairs in the system are calculated, and the smallest of these is chosen as the starting timestep for the whole system.

Next we get to the main loop of the code. The system is integrated until the target time is met. This starts with printing the data into a file if the output interval has elapsed. After that we check for potential mergers between all the particles in the chain. For a merger to happen at least one of the particles must be a black hole, so stars cannot collide with each other. The collisions are checked simply by looking through all the pairwise distances and merging particles that are close enough to each other. This distance criterion is 6 times the Schwarzschild radius of the total mass of the two particles. The merging preserves momentum and spin. If mergers do happen, the chain needs to be reconstructed.

The actual propagation is done via the GBS algorithm. The amount of substeps always starts at 2, and is incremented by 2 should the division not be sufficient. We start taking GBS substeps using leapfrog.
The unpropagated chain is copied, and this copy is then propagated. Since we do not at first know how many times we need to do the propagation, this way we are able to do the propagations but still have the original chain once we have sufficiently accurate propagation results. The leapfrog uses the drift-kick-drift ordering described in equation \eqref{equ:GBSLeapfrog}

After taking a step with leapfrog we do the extrapolation. The extrapolation algorithm calculates values for the particles in the system assuming infinitely small substeps, and also the error of these newly integrated values. The errors are compared to the GBS tolerance, and if the error is small enough the integration has been successful. If the error is deemed too large, we increase the number of substeps and repeat the whole process of leapfrog and extrapolation again. If the maximum number of substeps is reached and the error is still too big, then the total timestep itself is reduced, and the same procedure of taking incrementally more substeps is repeated. This is repeated until the error is small enough to satisfy the tolerance level.

After the integration is done, we check if the chain structure needs to be updated. The chain is stored in the order of indexing, so we want to keep the physically nearby particles close to each other in the chain structure as well. We must check if there are any pairwise distances in the system that are smaller than the smallest chained distance. If that is the case, the chain structure needs to be reconstructed.

Finally, after the system has been integrated for the target time, the final state of the system is outputted and the program is finished.


\chapter{AR-CHAIN results}

Two different kinds of simulations were run using the AR-CHAIN code to probe different phases of the evolution of a binary black hole. First, a simple system consisting only of a binary black hole was simulated to test the effects of the post-Newtonian corrections on the lifetime of the binary system. Second, the evolution of the binary affected by three-body interactions with stars was simulated to test how these interactions shrink the semimajor axis of the binary and alter its eccentricity.

\section{PN effects in a binary BH system}
\subsection{Binary setup}

The initial conditions of the test binary were motivated by the blazar OJ 287, which is considered to be a good candidate for a supermassive black hole binary \citep{valtonen:1996, dey:2019}. The basic setup for the test binary consisted of black holes with masses of $M_1 = 1.4 \times 10^{10} M_\odot$ and $M_2 = 1.8 \times 10^{8} M_\odot$. The initial semimajor axis of the binary was set to $a =$ 11500 AU, its eccentricity was set to $e =$ 0.6, and the primary black hole was set to have a spin of $s =$ 0.28 towards the positive z-axis of the system. The secondary black hole was set to have no spin. The inclination of the orbit of the secondary black hole was set to 45 degrees in the xz-plane. 

The simulations were run with three different PN term configurations. One run had only the 2.5PN terms included, ignoring the lower and higher order terms. Another run had all the terms up to 2.5PN included, and in the last run all the terms were included up to 3.5PN term accuracy. In addition, the effects of the spin were taken into account in all of the integrations. The simulations were always allowed to run until the eventual black hole merger.

The simulated results were compared against the theoretical lifetime of a binary system shrinking due to the emission of gravitational waves. Based on equations \eqref{equ:GWsemimajor} and \eqref{equ:GWeccentricity} that describe how the semimajor axis and eccentricity of the binary change over time due to GW emissions, \cite{peters:1964} derived an equation relating $a$ and $e$ during the decay for a general case where $e \neq 0$:
\begin{equation}
\left< \frac{\mathrm{d} a}{\mathrm{d} e} \right> = \frac{12}{19} \frac{a}{e} \frac{\left[ 1 + (73/24)e^2 + (37/96)e^4 \right] }{(1-e^2) \left[ 1 + (121/304)e^2 \right]} \ . \label{equ:GWsemiecc}
\end{equation}
This differential equation can then be integrated to obtain an expression for the semimajor axis as a function of the eccentricity:
\begin{equation}
a(e) = \frac{c_0 e^{12/19}}{(1-e^2)} \left[ 1 + \frac{121}{304} e^2 \right]^{870/2299} \ , \label{equ:GWsemifromecc}
\end{equation}
where $c_0$ is determined by the initial condition $a=a_0$ when $e=e_0$.

Combining equations \eqref{equ:GWsemifromecc} and \eqref{equ:GWeccentricity} gives us an equation for the time decay of an eccentric system exactly:
\begin{equation}
\left< \frac{\mathrm{d} e}{\mathrm{d} t} \right> = - \frac{19}{12} \frac{\beta}{c_0^4} \frac{e^{-29/19} (1-e^2)^{3/2}}{\left[ 1 + (121/304)e^2 \right]^{1181/2299}} \ , \label{equ:GWecctimedecay}
\end{equation}
where 
\begin{equation}
\beta = \frac{64}{5} \frac{G^3 \mu M^2}{c^5} \ .
\end{equation}
Equation \eqref{equ:GWecctimedecay} can then be integrated to give us an equation for the total lifetime of the system as a function of the initial semimajor axis and eccentricity:
\begin{equation}
T(a_0, e_0) = \frac{12}{19} \frac{c_0^4}{\beta} \int_0^{e_0} \frac{e^{29/19} [1 + (121/304)e^2]^{1181/2299}}{(1-e^2)^{3/2}} de \ . \label{equ:BBHlifetime}
\end{equation}

For an arbitrary semimajor axis and eccentricity equation \eqref{equ:BBHlifetime} needs to be integrated numerically.
It is of note that these equations take into account only the 2.5PN terms, so the simulations with different PN terms are expected to differ from these analytical results.

The aforementioned initial conditions of the binary were altered in three ways to test how they affect the time it takes for the black holes to merge. The other parameters were always kept fixed when changing one of the variables. Table \ref{table:GWruns} contains information on the number and run time of the simulations. First, the initial semimajor axis of the binary was varied from 8000 AU to 14000 AU in 50 AU steps, and these results are shown in figure \ref{fig:bh-merge-semi}.
Second, the initial eccentricity of the system was varied from 0.3 to 0.8 with 0.01 steps, and these results are shown in figure \ref{fig:bh-merge-ecc}.

Finally, the mass ratio of the black holes was varied from 0.0063 to 1 with 100 logarithmically equally spaced points, and these results can be seen in figure \ref{fig:bh-merge-ratio}. A logarithmic scale is used because the lifetime of the system drops rapidly as the mass ratio approaches unity. The total mass of the system $M = 1.418 \times 10^{10} M_{\odot}$ was kept constant throughout. As reference, the default mass ratio of the system is $M_2/M_1 \approx 0.013$.

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
Variable & Limits & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Number \\ of runs\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Default setup \\ run time\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Shortest \\ run time\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Longest \\ run time\end{tabular}} \\ \thickhline
$a$ & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}From 8000 AU \\ to 14000 AU\end{tabular}} & 121 & \multirowcell{3}{31 min} & 12 min & 46 min \\ \cline{1-3} \cline{5-6}
$e$ & From 0.3 to 0.8 & 51 &  & 3 min & 75 min \\ \cline{1-3} \cline{5-6}
$q$  & From 0.0063 to 1 & 100 &  & 1 sec & 33 min \\
\hline
\end{tabular}
\caption{The limits for the altered variables, number of runs done for each variable, the approximate amount of computation time the default setup took to run as reference, and the approximate amount of time the shortest and the longest of each type of run took. All of these times are for the 3.5PN term case. The runs were repeated for all 3 of the different PN term configurations. The computational time required correlates with the lifetime of the system, so the only 2.5PN and up to 2.5PN term cases were overall slightly faster. All of these simulations were run as serial code on a Linux server provided by the University of Helsinki.} \label{table:GWruns}
\end{table}


\subsection{Evolution of the binary}

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.6]{../images/bh-merge-semi.pdf}
\caption{The physical time in years it takes for a black hole binary to merge as a function of the initial semimajor axis of the system. These runs correspond to the setup on the first row of table \ref{table:GWruns}.}
\label{fig:bh-merge-semi}
\end{figure}


All of the results behave as expected. Based on equations \eqref{equ:GWsemimajor} and \eqref{equ:GWeccentricity} we know that the emission of GWs increases strongly as the semimajor axis of the binary system gets smaller and when the eccentricity is high. Thus the lifetime of the binary decreases as well, since energy is transmitted away from the system at a higher rate.
In both of these equations the value of the reduced mass depends on the mass ratio of the black holes. The larger the mass difference, the smaller the value. Thus the change in the semimajor axis is smaller and the system decays slower when the masses of the black holes differ greatly. The physical reasoning for this is that if one of the black holes is comparatively very light, the more massive black hole causes most of the distortion in the surrounding spacetime, so the situation is more akin to a test mass orbiting a black hole, instead of gravitational fields of two equally massive black holes interacting with each other very strongly.
As can be seen from the results, the slope of the lifetime of the system decreases as the semimajor axis gets smaller, eccentricity gets higher, and as the mass ratio approaches unity. This too follows from equations \eqref{equ:GWsemimajor} and \eqref{equ:GWeccentricity}. Since, for example, when the initial semimajor axis gets smaller, the difference in the initial change gets smaller as well due to the $1/a^3$ term.


The analytical results accurately match the simulated results where only the 2.5PN terms were used, showing that the simulation code does indeed accurately model a black hole binary system at these scales.
The simulations with up to 2.5PN and 3.5PN terms differ from the analytical results as expected. In the 2.5PN case the lifetime of the system is consistently much shorter than in any of the other cases, and conversely in the 3.5PN case the lifetime is consistently longer. This may at first seem counterintuitive, since the later PN terms include more dissipative terms, so one could imagine that the lifetime would decrease. However, the ways the different terms interact with each other affect the system more intricately. The corrections to the dissipation might directly decrease its efficiency instead of increasing it, if the additional terms are anti-dissipative.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.6]{../images/bh-merge-ecc.pdf}
\caption{The physical time in years it takes for a black hole binary to merge as a function of the initial eccentricity of the system. These runs correspond to the setup on the middle row of table \ref{table:GWruns}.}
\label{fig:bh-merge-ecc}
\end{figure}


While the PN terms before 2.5 do not directly affect the magnitude of the secular changes in semimajor axis and eccentricity, they do change the dynamics of the entire system \citep{merrit:nuclei}. For example, as mentioned in section \ref{sect:PNapplication}, the 1PN terms cause precession in the periapsis of the system, which also invariably leads to slightly altered orbital velocities. As all of the PN terms have factors that depend on the velocity, a small change can cause a large effect to the overall behaviour of the system. \cite{berentzen:2009} found similar results in their simulations. Using all PN terms up to 2.5 resulted in much faster coalescence compared to using only the 2.5 terms. Similarly \cite{mannerkoski:2019} noted that the analytical models for the binary decay noticeably differ from simulated experiments with 3.5PN terms.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.6]{../images/bh-merge-ratio.pdf}
\caption{The physical time in years it takes for a black hole binary to merge as a function of the mass ratio of the system. These runs correspond to the setup on the final row of table \ref{table:GWruns}.}
\label{fig:bh-merge-ratio}
\end{figure}


The 3.5PN terms are dissipative like the 2.5 terms, meaning that they directly affect the amount of energy radiated away from the system. The combined effects of the additional terms however cause the system to decay much slower than the up to 2.5PN case. On average the lifetime of the system is slightly longer than the analytical only 2.5PN case, a stark difference compared to the up to 2.5PN case. This exemplifies the fact that the PN expansion is highly non-linear, and the results are very different for different terms. For example, when compared to a Taylor expansion, where additional terms generally cause the results to be more accurate and approach the analytical solution, different PN terms can for example have different signs, causing the additional terms to affect the results in a hard to predict manner. The situation becomes ever more complex if more PN terms are taken into account. At 4PN term level the so called memory effect becomes relevant, which implies a permanent change in spacetime as a gravitational wave passes through it, e.g. test masses of a GW detector experience a permanent relative displacement after the passage of a gravitational wave \citep{blanchet:2014}. Fully calculating the effects of this effect would require the knowledge of the whole history of a binary system, since all the changes in the past affect the state of the system in the future. Thus using 4PN terms is not feasible in practice.

%\clearpage
\section{Scattering experiments on a binary BH}

Next we perform scattering experiments where individual stars are launched towards a binary black hole, and we measure the effects of the three-body interactions on the binary.
%The stars are ejected during inspiral phases that cannot be modeled analytically.
These experiments mostly follow the description laid out in \cite{sesana:2006}.

\subsection{Scattering setup}

We start by considering a binary with mass $M = M_1 + M_2$, where $M_2 \leq M_1$. The binary has a reduced mass of $\mu = M_1 M_2 / M$, mass ratio of $q = M_2/M_1$, eccentricity $e$, and semimajor axis $a$. The binary orbits in the xy-plane, and the stars that will interact with the binary have a mass of $m_{\ast}$. If the stars are light ($m_{\ast} \ll M_2$), the problem can be simplified by setting the resting center of mass of the binary at the origin of the coordinate system. For single binary-star interactions we can define an approximated dimensionless energy exchange parameter for the star $C$ and angular momentum change parameter $B$ \citep{hills:1983}:
\begin{equation}
C = \frac{M}{2m_{\ast}} \frac{\Delta E}{E} \ ,
\end{equation}
and 
\begin{equation}
B = - \frac{M}{m_{\ast}} \frac{\Delta L_z}{L_z} \ ,
\end{equation}
where $\Delta E / E$ is the increase (or decrease if negative) in the orbital binding energy $E = -GM/(2a)$, and $\Delta L_z / L_z$ is the change in the orbital specific angular momentum $L_z = \sqrt{GMa(1-e^2)}$. The conservation of total energy and angular momentum gives us an expression for the change of the eccentricity of the system
\begin{equation}
\Delta e = \frac{(1-e^2)}{2e} \frac{2 m_{\ast}}{M} (B-C) \ .
\end{equation}
Thus the parameter $C$ tells us how the energy of the binary changes during three-body interactions with stars, i.e. if C is positive the star takes energy away from the system and the binary shrinks. Similarly the parameter $B-C$ can be used to calculate how the eccentricity of the system changes due to the three-body scattering. If $B-C$ is positive, the eccentricity of the binary increases, and if it is negative the binary becomes more circular.

The approach of the star is always randomized, and for that we need several variables. These different variables and their ranges are summarized in table \ref{table:ScatterRandoms}. A significant exchange of energy ($C > 1$) occurs only when $v/V_c < (M_2/M)^{1/2}$, where $v$ is the initial speed of the star and $V_c = (GM/a)^{1/2}$ is the binary orbital speed, which is the relative speed of the black holes if the binary is circular \citep{sesana:2006}. The initial speed of the star is randomly selected in the range of $3 \times 10^{-3} (M_2/M)^{1/2} < v/V_c < 30 (M_2/M)^{1/2}$ from 80 logarithmically equally spaced points.

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|}
\hline
Variable & Range & {\renewcommand{\arraystretch}{0.9}\begin{tabular}{@{}c@{}}Linear or \\ logarithmic\end{tabular}} \\ \thickhline
$v$ & {\renewcommand{\arraystretch}{0.9}\begin{tabular}{@{}c@{}}Between $3 \times 10^{-3} (M_2/M)^{1/2} V_c$ \\ and $30 (M_2/M)^{1/2} V_c$ \end{tabular}} & Log \\ \hline
$r_p$ & {\renewcommand{\arraystretch}{0.9}\begin{tabular}{@{}c@{}}Between $2 \times 10^{-3} a$ \\ and $2 a$\end{tabular}} & Log \\ \hline
$\theta$ & Between $0$ and $\pi$ & Lin \\\hline
$\phi$ & Between $0$ and $2 \pi$ & Lin \\\hline
$\psi$ & Between $0$ and $2 \pi$ & Lin \\\hline
\end{tabular}
\caption{List of the random variables for each scattering event, the ranges they were chosen from, and whether the selection was linearly or logarithmically uniform.} \label{table:ScatterRandoms}
\end{table}

The pericenter distance $r_p$ of the incoming star is related to its impact parameter $b$ via
\begin{equation}
b^2 = r_p^2 \left( 1 + \frac{2GM}{r_p v^2} \right) \ . \label{equ:impactparam}
\end{equation}
We are interested in probing the most energetic interactions, i.e. those where the star passes close by to the binary, thus the star pericenter distance is randomly sampled from a logarithmically equal distribution in the range between $2 \times 10^{-3} a$ and $2 a$, and the impact parameter is calculated with this value and the selected velocity.

Finally, three different angles are needed to determine the direction and orientation of the orbit of the star. The angles $\theta$ and $\phi$ define the initial position of the star as seen from the center of mass of the binary system, and thus they determine the direction of the orbit. The angles are randomly selected from the ranges $[0,\pi]$ and $[0,2 \pi]$ respectively. The angle $\psi$ as seen from the star tells which way around the center of the system the star will approach, thus giving the orientation of the orbit. It is randomly selected from the range $[0,2 \pi]$.

With these five variables it is possible to fully determine the orbit of the star. The starting velocity for each star is determined at infinity, i.e. extremely far away from the binary. The star is then analytically moved to a distance of $r_i = (10^7 \mu/M)^{1/4} a$ along a hyperbolic orbit, with the center of mass of the binary used as a point mass. At this distance the quadrupole moment of the binary is still around 10 orders of magnitude smaller when compared to the total force acting on the star when it is only one semimajor axis away from the binary \citep{sesana:2006}. It is at this point that the actual integration starts. 

Not all of the simulated stars are ejected from the system, thus there needs to be a few different end conditions for the simulation to ensure that stars that end up on bound orbits are not simulated ad infinitum.
The integration is stopped once one of the following conditions is met: $(a)$ the star escapes the sphere of radius $r_i$ with positive total energy; $(b)$ the integration reaches $10^6$ time steps; or $(c)$ the physical integration time exceeds $\sim 1.5 \times 10^7$ years. Only stars that are ejected from the system within the time limits are taken into account in the results. Table \ref{table:ScatterRuns} shows that condition $(c)$ was never actually met, as all the runs hit the time step limit before the physical time of the system reached its limit. It also shows how much physical time it took for the simulation to end if the star ended up on bound orbit.

\begin{table}[htb]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
\hline
{\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Mass \\ ratio\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Number \\ of \\ scattering \\ runs\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Amount \\ of \\ bound \\ stars\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Run time \\ for \\ escaped \\ stars\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Run time \\ for \\ bound \\ stars\end{tabular}} & {\renewcommand{\arraystretch}{0.8}\begin{tabular}{@{}c@{}}Bound \\ end \\ physical \\ time\end{tabular}} \\ \thickhline
$1$ & $10^5$ & $1.5\%$ & 2.1 s & 40 s & $4.7 \times 10^5$ yr \\ \hline
$1/3$ & $10^5$ & $2.2\%$ & 2.2 s & 41 s & $4.8 \times 10^5$ yr \\ \hline
$1/9$  & $10^5$ & $3.2\%$ & 2.6 s & 46 s & $8.6 \times 10^5$ yr \\ \hline
$1/27$ & $10^5$ & $6.5\%$ & 3.7 s & 50 s & $1.7 \times 10^6$ yr \\ \hline
\end{tabular}
\caption{The number of runs, the average amount of these runs that ended on bound orbits and thus did not escape, the average run times of single scattering events for stars that did and did not escape, and the average end physical time of the system if the star did not escape (i.e. the simulation was run for $10^6$ time steps), all for the different mass ratios that were used. Each mass ratio was run with four different eccentricites ($e = 0.01$, $e = 0.3$, $e = 0.6$, and $e = 0.9$) for a total of 16 different configurations, but the change in eccentricity did not have a major impact on the amount of stars that ended up on bound orbits, or on the run time of the system. Just like the PN term simulations, all of these simulations were run as serial code on a Linux server provided by the University of Helsinki.} \label{table:ScatterRuns}
\end{table}

In addition to the randomized variables, the system has several predefined parameters. The total mass of the black holes $M$ was selected to be  $2 \times 10^7 \ M_{\odot}$, i.e. each of the black holes is $10^7 \ M_{\odot}$ if they are of equal mass. The mass of the star $m_{\ast}$ was chosen to be a realistic $1 \ M_{\odot}$. The semimajor axis of the binary was set to $0.1$ parsec, small enough that the binary can be safely said to be hard, i.e. the semimajor axis of the binary is below the limit $a_h \equiv \frac{G \mu}{4 \sigma}$, where $\sigma$ is the velocity dispersion of the surrounding stars \citep{merrit:nuclei}. 

\subsection{Scattering results}

A total of 16 different scattering experiment runs were performed on a binary with mass ratios $q =$ 1, 1/3, 1/9, and 1/27, and eccentricities  $e =$ 0.01, 0.3, 0.6, and 0.9. Each run involves $10^5$ stars. The state of the system was outputted at the start and end of each scattering event, and the values of $C$ and $B-C$ were calculated for each event. The state of the binary was reset after each event, so the overall evolution of the system was not measured. Table \ref{table:ScatterRuns} shows run times for the different scattering runs. It is of note that there was not much difference in run times for the runs where only the eccentricity was changed. Only changing the mass ratio caused a noticeable difference.

The data was plotted against a rescaled dimensionless impact parameter $x$ as defined in \cite{quinlan:1996}:
\begin{equation}
x \equiv b/b_0 \ , \ \ b_0^2 = 2GMa/v^2 \ ,
\end{equation}
where $b_0$ is approximately the impact parameter corresponding to $r_p = a$ if gravitational focusing is important, i.e. if the second term in the equation for the impact parameter \eqref{equ:impactparam} dominates over the $r_p^2$ term. The data was binned to 30 logarithmically equal bins, and the results were averaged over these bins to obtain the averaged energy exchange $\langle C \rangle$ and eccentricity change $\langle B-C \rangle$.

\subsection{Equal mass binary}

In figure \ref{fig:scatter-q1} we show $\langle C \rangle$ and $\langle B-C \rangle$ for the different eccentricities $e =$ 0.01, 0.3, 0.6, and 0.9 for an equal mass binary, i.e. $q = 1$. Figure \ref{fig:scatter-q1-stdev} has the same results but with one standard deviation error limits included. The deviations are of the similar size for all of the mass ratios and their size makes it difficult to distinguish the average values, so they are left out from the other plots for clarity. The broad distribution can be explained with the stellar population that the test stars were drawn from. The wide range of initial speeds and angles of approach mean that some of the stars were able to interact with the binary much more energetically than happens on average, causing a wide spread in the results. 
Figures \ref{fig:scatter-q0.3}, \ref{fig:scatter-q0.1}, and \ref{fig:scatter-q0.037} show the values for $\langle C \rangle$ and $\langle B-C \rangle$ for binaries with $q =$ 1/3, 1/9, and 1/27 respectively.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.62]{../images/q1.pdf}
\caption{The averaged energy exchange $<$C$>$ and eccentricity change $<$B-C$>$ as functions of the rescaled dimensionless impact parameter x $\equiv b/b_0$ for equal mass black holes (q=1).}
\label{fig:scatter-q1}
\end{figure}

As can be seen from the upper part of figure \ref{fig:scatter-q1}, for a circular equal mass binary $\langle C \rangle$ has a peak around $x \approx 0.5$, which is the distance at which each of the binary members orbit the center of mass, which then tapers off to around 0.5 at very small impact parameters. As the eccentricity gets higher the peak becomes less sharp while $\langle C \rangle$ rises at smaller values of $x$. At $e = 0.9$ $\langle C \rangle$ is almost monotonic at small impact parameters, since stars that approach close to the center of mass of the binary interact with it at their pericenter which is when their speed is at a maximum, resulting in a large energy exchange. At all eccentricities $\langle C \rangle$ goes quickly towards 0 as $x$ becomes larger than 1, since the farther away from the binary the star passes, the less strongly it interacts with the system.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.62]{../images/{q1_stdev}.pdf}
\caption{Same as figure \ref{fig:scatter-q1} but with one standard deviation limits included as coloured contours.}
\label{fig:scatter-q1-stdev}
\end{figure}

In the lower half of figure \ref{fig:scatter-q1} we see that the eccentricity change $\langle B-C \rangle$ for circular binaries is zero independent of impact parameter, meaning that a circular binary always remains circular. For larger eccentricities there is a positive peak in $\langle B-C \rangle$ near the impact parameter that corresponds to the distance at which the black holes are orbiting the center of mass. This peak increases with eccentricity. Conversely, as the impact parameter gets smaller the stars cause a slight decrease in eccentricity whose effect also grows with eccentricity. So the more eccentric the binary is to begin with, the larger effect the stars have on the eccentricity. This can be explained with the equation for the orbital specific angular momentum $L_z = \sqrt{GMa(1-e^2)} = \sqrt{GMa(1+e)(1-e)}$. Since $L_z \approx 0$ when $e \approx 1$, at high eccentricities any change will have a proportionally larger effect on the angular momentum. 

For an equal mass binary the results for $\langle C \rangle$ agree well with those of \cite{sesana:2006}. The shapes of the lines for different eccentricities are very similar. The absolute values of the averages are slightly higher in their study, but taking into account the error limits in figure \ref{fig:scatter-q1-stdev}, the differences are insignificant. Similarly, for the eccentricity change $\langle B-C \rangle$ the results agree with the results of \cite{sesana:2006}. Circular binaries do not experience any change in eccentricity, and the more eccentric the binary is, the larger of an effect the scattering events have on the binary. 

\subsection{Unequal mass binaries}

From the upper halves of figures \ref{fig:scatter-q0.3}, \ref{fig:scatter-q0.1}, and \ref{fig:scatter-q0.037} we see that for circular binaries of unequal mass, the peak in $\langle C \rangle$ that already appeared for an equal mass binary is more pronounced as $\langle C \rangle$ becomes almost constant quickly at smaller impact parameters. As the difference in the masses of the binaries gets bigger, the peak also moves towards larger $x$, as the less massive binary orbits the more massive central body at farther distance compared to an equal mass system. A stark difference to the equal mass binary is that for other mass ratios, the values of $\langle C \rangle$ at smaller values of $x$ become much more uniform for all eccentricities. In addition, the absolute values of the averages are higher than in the equal mass case. 

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.62]{../images/{q0.3}.pdf}
\caption{The averaged energy exchange $<$C$>$ and eccentricity change $<$B-C$>$ as functions of the rescaled dimensionless impact parameter x for black holes with a mass ratio q = 1/3.}
\label{fig:scatter-q0.3}
\end{figure}

We can see from the lower parts of figures \ref{fig:scatter-q0.3}, \ref{fig:scatter-q0.1}, and \ref{fig:scatter-q0.037} that just like for an equal mass binary, the eccentricity change $\langle B-C \rangle$ for circular binaries is zero independent impact parameter for all mass ratios, so even with unequal mass black holes a circular binary will remain circular.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.62]{../images/{q0.1}.pdf}
\caption{The averaged energy exchange $<$C$>$ and eccentricity change $<$B-C$>$ as functions of the rescaled dimensionless impact parameter x for black holes with a mass ratio q = 1/9.}
\label{fig:scatter-q0.1}
\end{figure}

Similarly to the equal mass case, for larger mass differences there is a peak in $\langle B-C \rangle$ at the impact parameter that matches the distance of the smaller black hole. The magnitude of the peak grows with eccentricity. However, with larger mass differences $\langle B-C \rangle$ at smaller impact parameters is not always negative. Encounters at small impact parameters can even slightly increase the eccentricity. Though with high eccentricity $\langle B-C \rangle$ tends to still be negative at small impact parameters. Aside from circular binaries, there is a slight decrease in eccentricity for all mass ratios and eccentricities, before $\langle B-C \rangle$ tends towards 0 as the impact parameter gets larger.

\begin{figure}[h!tb]
\centering
\includegraphics[scale=0.62]{../images/{q0.037}.pdf}
\caption{The averaged energy exchange $<$C$>$ and eccentricity change $<$B-C$>$ as functions of the rescaled dimensionless impact parameter x for black holes with a mass ratio q = 1/27.}
\label{fig:scatter-q0.037}
\end{figure}

As with the equal mass binary, the results for different mass ratios agree quite well with the results of \cite{sesana:2006}. The rough shapes of the plots and how the lines for different eccentricities compare to each other match their results. They also noted the effect that unequal mass binaries with larger than zero but not too high eccentricity could experience a slight gain in eccentricity from scattering events with stars that had small impact parameters. Though it should be noted that \cite{sesana:2006} only included the $\langle B-C \rangle$ results for mass ratios 1 and 1/9, so the other two mass ratios cannot be directly compared. Like with the equal mass binary, \cite{sesana:2006} found the absolute values of $\langle B-C \rangle$ to be slightly farther away from 0 than compared to figures \ref{fig:scatter-q0.3}, \ref{fig:scatter-q0.1}, and \ref{fig:scatter-q0.037}.

One noticeably different result however is that for the 1/9 mass ratio binary, \cite{sesana:2006} show a much more spread out peak near $x=1$ compared to the one visible in \ref{fig:scatter-q0.1}. This and the previously mentioned differences in the absolute values of the averages can most likely be explained with slightly different starting parameters for the system, which were not explicitly specified in \cite{sesana:2006}. Their runs also had $4 \times 10^6$ stars each, compared to the $10^5$ runs used for the figures shown here, allowing for much greater accuracy in probing the different initial star speeds and orbit orientations. The larger volume of runs also explains their much smoother and higher resolution curves.


\chapter{Conclusions}



%TODO if we had accurate data about the stellar populations near galaxy centers we could do better initial conditions for the stars, also if we had better observations of binary black holes
%jos enemmän scattering ajoja, niin paremmat keskiarvot

% STEP 5:
% Uncomment the following lines and set your .bib file and desired bibliography style
% to make a bibliography with BibTeX.
% Alternatively you can use the thebibliography environment if you want to add all
% references by hand.

\clearpage
\addcontentsline{toc}{chapter}{Bibliography} % This lines adds the bibliography to the ToC
\bibliographystyle{plainnat}
\small
\bibliography{lahteet}
\label{pagecount}

\end{document}

